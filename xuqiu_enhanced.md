# ç¼–ç¨‹é¡¹ç›®MCPï¼ˆè®°å¿†æ§åˆ¶æœºåˆ¶ï¼‰å¼€å‘æ–‡æ¡£ - å®Œæ•´å¯è½åœ°ç‰ˆæœ¬

## ğŸ¯ ä¸€ã€é¡¹ç›®æ€»è§ˆ

### 1.1 æ–‡æ¡£ç›®çš„
æä¾›ä¸€å¥—**å®Œæ•´å¯æ‰§è¡Œ**çš„MCPå¼€å‘æ–¹æ¡ˆï¼ŒåŒ…å«è¯¦ç»†çš„å¼€å‘è®¡åˆ’ã€APIè®¾è®¡ã€æ•°æ®åº“Schemaã€æµ‹è¯•æ–¹æ¡ˆå’Œéƒ¨ç½²ç­–ç•¥ï¼Œç¡®ä¿é¡¹ç›®ä»0åˆ°1å¯å¿«é€Ÿè½åœ°ã€‚

### 1.2 æ ¸å¿ƒç›®æ ‡ä¸å¯é‡åŒ–æŒ‡æ ‡
| ç›®æ ‡ç»´åº¦ | å…·ä½“æŒ‡æ ‡ | éªŒæ”¶æ ‡å‡† |
|---------|---------|----------|
| è®°å¿†èƒ½åŠ› | è·¨ä¼šè¯è®°å¿†å‡†ç¡®ç‡ | â‰¥95% |
| Tokenä¼˜åŒ– | Tokenæ¶ˆè€—é™ä½ç‡ | â‰¥90% |
| å¹»è§‰æŠ‘åˆ¶ | å¹»è§‰é”™è¯¯ç‡ | â‰¤5% |
| å“åº”æ€§èƒ½ | å•æ¬¡æŸ¥è¯¢å“åº”æ—¶é—´ | â‰¤300ms |
| å¹¶å‘èƒ½åŠ› | æ”¯æŒå¹¶å‘è¯·æ±‚æ•° | â‰¥100 QPS |
| å¯ç”¨æ€§ | ç³»ç»Ÿå¯ç”¨æ€§ | â‰¥99.9% |

### 1.3 é€‚ç”¨åœºæ™¯
- âœ… AIè¾…åŠ©ç¼–ç¨‹å·¥å…·ï¼ˆå¦‚Cursorã€Copilotæ‰©å±•ï¼‰
- âœ… æ™ºèƒ½å®¢æœç³»ç»Ÿï¼ˆå¤šè½®å¯¹è¯åœºæ™¯ï¼‰
- âœ… ä»£ç å®¡æŸ¥åŠ©æ‰‹
- âœ… è‡ªåŠ¨åŒ–æµ‹è¯•ç”Ÿæˆå·¥å…·
- âœ… æŠ€æœ¯æ–‡æ¡£é—®ç­”ç³»ç»Ÿ

### 1.4 æŠ€æœ¯æ ˆç‰ˆæœ¬é”å®š
```toml
[tool.uv.dependencies]
python = "^3.10"
mcp-sdk = "^0.5.0"
httpx = "^0.27.0"
redis = "^5.0.1"
faiss-cpu = "^1.8.0"
pymilvus = "^2.3.4"
sqlalchemy = "^2.0.23"
pyyaml = "^6.0.1"
transformers = "^4.36.0"
sentence-transformers = "^2.2.2"
pydantic = "^2.5.0"
fastapi = "^0.108.0"
uvicorn = "^0.25.0"
pytest = "^7.4.3"
pytest-asyncio = "^0.21.1"
```

---

## ğŸ—ï¸ äºŒã€ç³»ç»Ÿæ¶æ„è®¾è®¡

### 2.1 æ¶æ„ç†å¿µ
é‡‡ç”¨ã€Œ**åˆ†å±‚è®°å¿†+æ™ºèƒ½ç®¡æ§+æ ¡éªŒé—­ç¯+å¾®æœåŠ¡åŒ–**ã€æ¶æ„ï¼Œæ”¯æŒæ°´å¹³æ‰©å±•å’Œæ¨¡å—åŒ–éƒ¨ç½²ã€‚

### 2.2 æ•´ä½“æ¶æ„å›¾
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       å®¢æˆ·ç«¯å±‚ï¼ˆå¤šé¡¹ç›®æ¥å…¥ï¼‰                      â”‚
â”‚         Webé¡¹ç›® | AIå·¥å…· | åç«¯æœåŠ¡ | IDEæ’ä»¶                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      APIç½‘å…³å±‚ï¼ˆFastAPIï¼‰                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚è®¤è¯é‰´æƒ  â”‚  â”‚è¯·æ±‚è·¯ç”±  â”‚  â”‚é™æµç†”æ–­  â”‚  â”‚æ—¥å¿—è¿½è¸ª  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MCPæ ¸å¿ƒæœåŠ¡å±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚è®°å¿†ç®¡ç†æœåŠ¡  â”‚ â”‚Tokenä¼˜åŒ–æœåŠ¡ â”‚ â”‚å¹»è§‰æŠ‘åˆ¶æœåŠ¡  â”‚              â”‚
â”‚  â”‚MemoryService â”‚ â”‚TokenService  â”‚ â”‚ValidateServiceâ”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚æƒé™å®‰å…¨æœåŠ¡  â”‚ â”‚é…ç½®ç®¡ç†æœåŠ¡  â”‚ â”‚ç›‘æ§å‘Šè­¦æœåŠ¡  â”‚              â”‚
â”‚  â”‚SecurityServiceâ”‚ â”‚ConfigService â”‚ â”‚MonitorServiceâ”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         æ•°æ®å­˜å‚¨å±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚Redis Cluster â”‚ â”‚Milvus/FAISS  â”‚ â”‚PostgreSQL    â”‚              â”‚
â”‚  â”‚(çŸ­æœŸç¼“å­˜)    â”‚ â”‚(å‘é‡æ£€ç´¢)    â”‚ â”‚(ç»“æ„åŒ–å­˜å‚¨)  â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 æ ¸å¿ƒæ¨¡å—äº¤äº’æµç¨‹
```mermaid
sequenceDiagram
    participant Client as å®¢æˆ·ç«¯
    participant API as APIç½‘å…³
    participant Memory as è®°å¿†æœåŠ¡
    participant Token as TokenæœåŠ¡
    participant Validate as æ ¡éªŒæœåŠ¡
    participant Storage as å­˜å‚¨å±‚

    Client->>API: å‘èµ·è¯·æ±‚(query)
    API->>Memory: æ£€ç´¢ç›¸å…³è®°å¿†
    Memory->>Storage: å¤šç»´åº¦æŸ¥è¯¢
    Storage-->>Memory: è¿”å›Top-Kè®°å¿†
    Memory->>Token: å‹ç¼©è®°å¿†å†…å®¹
    Token-->>Memory: è¿”å›ä¼˜åŒ–åä¸Šä¸‹æ–‡
    Memory->>Validate: å¹»è§‰æ£€æµ‹
    Validate-->>API: æ ¡éªŒé€šè¿‡/å¤±è´¥
    API-->>Client: è¿”å›å“åº”
```

---

## ğŸ“¡ ä¸‰ã€APIè®¾è®¡è§„èŒƒ

### 3.1 RESTful APIè®¾è®¡

#### 3.1.1 è®°å¿†ç®¡ç†æ¥å£

**ã€POSTã€‘å­˜å‚¨è®°å¿†**
```http
POST /api/v1/memory/store
Content-Type: application/json
Authorization: Bearer {token}

{
  "project_id": "proj_001",
  "content": "é¡¹ç›®ä½¿ç”¨Django 4.2æ¡†æ¶ï¼Œæ ¸å¿ƒæ¥å£ä¸º/api/user",
  "memory_level": "mid",  // short/mid/long
  "metadata": {
    "category": "framework",
    "tags": ["django", "api"],
    "confidence": 0.95
  }
}

Response 200:
{
  "code": 0,
  "message": "å­˜å‚¨æˆåŠŸ",
  "data": {
    "memory_id": "mem_20250118_001",
    "stored_at": "2025-01-18T10:30:00Z"
  }
}
```

**ã€GETã€‘æ£€ç´¢è®°å¿†**
```http
GET /api/v1/memory/retrieve?project_id=proj_001&query=å¦‚ä½•è°ƒç”¨ç”¨æˆ·æ¥å£&top_k=5
Authorization: Bearer {token}

Response 200:
{
  "code": 0,
  "message": "æ£€ç´¢æˆåŠŸ",
  "data": {
    "memories": [
      {
        "memory_id": "mem_20250118_001",
        "content": "é¡¹ç›®ä½¿ç”¨Django 4.2æ¡†æ¶...",
        "relevance_score": 0.92,
        "created_at": "2025-01-18T10:30:00Z"
      }
    ],
    "total_token_saved": 3200
  }
}
```

**ã€PUTã€‘æ›´æ–°è®°å¿†**
```http
PUT /api/v1/memory/{memory_id}
Content-Type: application/json

{
  "content": "æ›´æ–°åçš„å†…å®¹",
  "metadata": {
    "updated_reason": "æ¥å£ç‰ˆæœ¬å‡çº§"
  }
}
```

**ã€DELETEã€‘åˆ é™¤è®°å¿†**
```http
DELETE /api/v1/memory/{memory_id}?project_id=proj_001
```

#### 3.1.2 Tokenä¼˜åŒ–æ¥å£

**ã€POSTã€‘å†…å®¹å‹ç¼©**
```http
POST /api/v1/token/compress
Content-Type: application/json

{
  "content": "é•¿æ–‡æœ¬æˆ–ä»£ç å†…å®¹...",
  "content_type": "code",  // code/text
  "compression_ratio": 0.2
}

Response 200:
{
  "code": 0,
  "data": {
    "original_tokens": 2048,
    "compressed_tokens": 410,
    "compression_rate": 0.80,
    "compressed_content": "æ ¸å¿ƒæ‘˜è¦å†…å®¹..."
  }
}
```

#### 3.1.3 å¹»è§‰æ£€æµ‹æ¥å£

**ã€POSTã€‘å†…å®¹æ ¡éªŒ**
```http
POST /api/v1/validate/hallucination
Content-Type: application/json

{
  "project_id": "proj_001",
  "output": "å¾…æ£€æµ‹çš„æ¨¡å‹è¾“å‡ºå†…å®¹",
  "threshold": 0.65
}

Response 200:
{
  "code": 0,
  "data": {
    "is_hallucination": false,
    "confidence": 0.88,
    "matched_memories": ["mem_001", "mem_003"]
  }
}
```

### 3.2 é”™è¯¯ç è®¾è®¡

| é”™è¯¯ç  | è¯´æ˜ | HTTPçŠ¶æ€ç  |
|-------|------|-----------|
| 0 | æˆåŠŸ | 200 |
| 1001 | å‚æ•°é”™è¯¯ | 400 |
| 1002 | æœªæˆæƒ | 401 |
| 1003 | æ— æƒé™ | 403 |
| 2001 | è®°å¿†æœªæ‰¾åˆ° | 404 |
| 2002 | è®°å¿†å­˜å‚¨å¤±è´¥ | 500 |
| 3001 | Tokenè¶…é™ | 413 |
| 4001 | å¹»è§‰æ£€æµ‹å¤±è´¥ | 422 |
| 5001 | ç³»ç»Ÿé”™è¯¯ | 500 |

---

## ğŸ—„ï¸ å››ã€æ•°æ®åº“è®¾è®¡

### 4.1 PostgreSQLç»“æ„åŒ–å­˜å‚¨Schema

```sql
-- é¡¹ç›®è¡¨
CREATE TABLE projects (
    project_id VARCHAR(64) PRIMARY KEY,
    project_name VARCHAR(255) NOT NULL,
    description TEXT,
    owner_id VARCHAR(64) NOT NULL,
    status SMALLINT DEFAULT 1,  -- 1:active, 0:inactive
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
CREATE INDEX idx_projects_owner ON projects(owner_id);

-- é•¿æœŸè®°å¿†è¡¨ï¼ˆæ ¸å¿ƒäº‹å®ï¼‰
CREATE TABLE long_memories (
    memory_id VARCHAR(64) PRIMARY KEY,
    project_id VARCHAR(64) NOT NULL,
    content TEXT NOT NULL,
    category VARCHAR(50),  -- framework/api/rule/config
    confidence DECIMAL(3,2) DEFAULT 0.80,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (project_id) REFERENCES projects(project_id)
);
CREATE INDEX idx_long_mem_project ON long_memories(project_id);
CREATE INDEX idx_long_mem_category ON long_memories(category);

-- ç”¨æˆ·æƒé™è¡¨
CREATE TABLE user_permissions (
    user_id VARCHAR(64),
    project_id VARCHAR(64),
    role VARCHAR(20),  -- admin/developer/viewer
    granted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY (user_id, project_id),
    FOREIGN KEY (project_id) REFERENCES projects(project_id)
);

-- æ“ä½œå®¡è®¡æ—¥å¿—è¡¨
CREATE TABLE audit_logs (
    log_id BIGSERIAL PRIMARY KEY,
    user_id VARCHAR(64),
    project_id VARCHAR(64),
    action VARCHAR(50),  -- store/retrieve/update/delete
    resource_type VARCHAR(50),  -- memory/config
    resource_id VARCHAR(64),
    details JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
CREATE INDEX idx_audit_user_time ON audit_logs(user_id, created_at);
```

### 4.2 Redisæ•°æ®ç»“æ„è®¾è®¡

```python
# çŸ­æœŸä¼šè¯è®°å¿†ï¼ˆæœ‰åºé›†åˆï¼‰
# Key: project:{project_id}:short_mem
# Score: relevance_score
# Value: JSONæ ¼å¼çš„è®°å¿†æ•°æ®
ZADD project:proj_001:short_mem 0.95 '{"content":"...", "timestamp":1705561800}'

# è®°å¿†æ£€ç´¢ç¼“å­˜ï¼ˆ7å¤©è¿‡æœŸï¼‰
# Key: cache:retrieve:{hash(query)}
# Value: JSONæ ¼å¼çš„æ£€ç´¢ç»“æœ
SETEX cache:retrieve:abc123 604800 '{"memories":[...], "cached_at":1705561800}'

# ç”¨æˆ·ä¼šè¯ä¿¡æ¯ï¼ˆ24å°æ—¶è¿‡æœŸï¼‰
# Key: session:{user_id}
# Value: JSONæ ¼å¼çš„ä¼šè¯æ•°æ®
SETEX session:user_001 86400 '{"project_id":"proj_001", "context_window":10}'

# Tokenæ¶ˆè€—ç»Ÿè®¡ï¼ˆæŒ‰å¤©ç»Ÿè®¡ï¼‰
# Key: stats:token:{project_id}:{date}
# Value: æ€»æ¶ˆè€—Tokenæ•°
INCR stats:token:proj_001:20250118
```

### 4.3 Milvuså‘é‡æ•°æ®åº“Collectionè®¾è®¡

```python
# ä¸­æœŸé¡¹ç›®è®°å¿†Collection
schema = {
    "collection_name": "mid_term_memories",
    "description": "ä¸­æœŸé¡¹ç›®è®°å¿†å‘é‡å­˜å‚¨",
    "fields": [
        {"name": "memory_id", "type": "VarChar", "max_length": 64, "is_primary": True},
        {"name": "project_id", "type": "VarChar", "max_length": 64},
        {"name": "embedding", "type": "FloatVector", "dim": 768},  # sentence-transformersè¾“å‡ºç»´åº¦
        {"name": "content", "type": "VarChar", "max_length": 2000},
        {"name": "category", "type": "VarChar", "max_length": 50},
        {"name": "created_at", "type": "Int64"}  # Unixæ—¶é—´æˆ³
    ],
    "index": {
        "field": "embedding",
        "metric_type": "COSINE",  # ä½™å¼¦ç›¸ä¼¼åº¦
        "index_type": "HNSW",
        "params": {"M": 16, "efConstruction": 200}
    }
}

# æœç´¢å‚æ•°
search_params = {
    "metric_type": "COSINE",
    "params": {"ef": 64},  # æ£€ç´¢æ—¶çš„æœç´¢æ·±åº¦
    "expr": f"project_id == '{project_id}'"  # è¿‡æ»¤è¡¨è¾¾å¼
}
```

---

## ğŸ”§ äº”ã€æ ¸å¿ƒæ¨¡å—å®ç°æ–¹æ¡ˆ

### 5.1 è®°å¿†ç®¡ç†æœåŠ¡ï¼ˆå®Œæ•´å®ç°ï¼‰

```python
# src/mcp_core/memory/service.py
from typing import List, Dict, Optional
from datetime import datetime, timedelta
import redis
import json
from sqlalchemy.orm import Session
from sentence_transformers import SentenceTransformer

class MemoryService:
    def __init__(self, config: Dict):
        self.redis_client = redis.Redis.from_url(config["redis_url"])
        self.db_session = Session(bind=config["db_engine"])
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')
        self.vector_db = self._init_vector_db(config)

    def store_memory(
        self,
        project_id: str,
        content: str,
        memory_level: str = "mid",
        metadata: Optional[Dict] = None
    ) -> Dict:
        """å­˜å‚¨è®°å¿†"""
        # 1. ç”Ÿæˆè®°å¿†IDå’Œæ—¶é—´æˆ³
        memory_id = f"mem_{datetime.now().strftime('%Y%m%d%H%M%S')}_{hash(content)[:8]}"
        timestamp = int(datetime.now().timestamp())

        # 2. æå–æ ¸å¿ƒä¿¡æ¯ï¼ˆå»é™¤å†—ä½™ï¼‰
        core_info = self._extract_core_info(content, metadata)

        # 3. è®¡ç®—ç›¸å…³æ€§è¯„åˆ†
        relevance_score = self._calculate_relevance(project_id, core_info)

        # 4. æŒ‰å±‚çº§å­˜å‚¨
        if memory_level == "short":
            # RedisçŸ­æœŸå­˜å‚¨ï¼ˆ24å°æ—¶ï¼‰
            memory_data = {
                "memory_id": memory_id,
                "content": core_info,
                "metadata": metadata or {},
                "timestamp": timestamp
            }
            self.redis_client.zadd(
                f"project:{project_id}:short_mem",
                {json.dumps(memory_data): relevance_score}
            )
            self.redis_client.expire(f"project:{project_id}:short_mem", 86400)

        elif memory_level == "mid":
            # å‘é‡æ•°æ®åº“å­˜å‚¨ï¼ˆ30å¤©è‡ªåŠ¨æ¸…ç†ï¼‰
            embedding = self.embedder.encode(core_info).tolist()
            self.vector_db.insert(
                collection_name="mid_term_memories",
                data=[{
                    "memory_id": memory_id,
                    "project_id": project_id,
                    "embedding": embedding,
                    "content": core_info[:2000],
                    "category": metadata.get("category", "general"),
                    "created_at": timestamp
                }]
            )

        elif memory_level == "long":
            # PostgreSQLæ°¸ä¹…å­˜å‚¨
            from .models import LongMemory
            long_mem = LongMemory(
                memory_id=memory_id,
                project_id=project_id,
                content=core_info,
                category=metadata.get("category"),
                confidence=metadata.get("confidence", 0.80),
                metadata=metadata
            )
            self.db_session.add(long_mem)
            self.db_session.commit()

        return {
            "memory_id": memory_id,
            "stored_at": datetime.fromtimestamp(timestamp).isoformat()
        }

    def retrieve_memory(
        self,
        project_id: str,
        query: str,
        top_k: int = 5,
        memory_levels: List[str] = ["short", "mid", "long"]
    ) -> Dict:
        """æ£€ç´¢è®°å¿†"""
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = f"cache:retrieve:{hash(project_id + query)}"
        cached = self.redis_client.get(cache_key)
        if cached:
            return json.loads(cached)

        # 2. å¤šå±‚çº§å¹¶è¡Œæ£€ç´¢
        all_memories = []

        if "short" in memory_levels:
            # ä»Redisæ£€ç´¢çŸ­æœŸè®°å¿†
            short_mems = self.redis_client.zrevrange(
                f"project:{project_id}:short_mem",
                0, top_k - 1,
                withscores=True
            )
            all_memories.extend([
                {
                    **json.loads(mem[0]),
                    "relevance_score": float(mem[1]),
                    "source": "short_term"
                }
                for mem in short_mems
            ])

        if "mid" in memory_levels:
            # ä»å‘é‡æ•°æ®åº“æ£€ç´¢ä¸­æœŸè®°å¿†
            query_embedding = self.embedder.encode(query).tolist()
            results = self.vector_db.search(
                collection_name="mid_term_memories",
                data=[query_embedding],
                limit=top_k,
                filter=f"project_id == '{project_id}'"
            )
            all_memories.extend([
                {
                    "memory_id": hit.entity.get("memory_id"),
                    "content": hit.entity.get("content"),
                    "relevance_score": hit.score,
                    "source": "mid_term",
                    "timestamp": hit.entity.get("created_at")
                }
                for hit in results[0]
            ])

        if "long" in memory_levels:
            # ä»PostgreSQLæ£€ç´¢é•¿æœŸè®°å¿†
            from .models import LongMemory
            long_mems = self.db_session.query(LongMemory).filter(
                LongMemory.project_id == project_id
            ).order_by(LongMemory.confidence.desc()).limit(top_k).all()

            all_memories.extend([
                {
                    "memory_id": mem.memory_id,
                    "content": mem.content,
                    "relevance_score": float(mem.confidence),
                    "source": "long_term",
                    "category": mem.category
                }
                for mem in long_mems
            ])

        # 3. å»é‡å¹¶æŒ‰ç›¸å…³æ€§æ’åº
        unique_memories = self._deduplicate_memories(all_memories)
        sorted_memories = sorted(
            unique_memories,
            key=lambda x: x["relevance_score"],
            reverse=True
        )[:top_k]

        # 4. è®¡ç®—TokenèŠ‚çœé‡
        original_tokens = sum(len(m["content"]) // 4 for m in sorted_memories)  # ç²—ç•¥ä¼°ç®—
        compressed_tokens = original_tokens // 5  # å‹ç¼©å

        result = {
            "memories": sorted_memories,
            "total_token_saved": original_tokens - compressed_tokens
        }

        # 5. ç¼“å­˜ç»“æœï¼ˆ7å¤©ï¼‰
        self.redis_client.setex(cache_key, 604800, json.dumps(result))

        return result

    def _extract_core_info(self, content: str, metadata: Optional[Dict]) -> str:
        """æå–æ ¸å¿ƒä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        # å®é™…é¡¹ç›®ä¸­åº”ä½¿ç”¨CodeBERTæˆ–TextRank
        # è¿™é‡Œç®€åŒ–ä¸ºç§»é™¤å¤šä½™ç©ºç™½å’Œæ³¨é‡Š
        import re
        cleaned = re.sub(r'\s+', ' ', content).strip()
        return cleaned[:1000]  # é™åˆ¶æœ€å¤§é•¿åº¦

    def _calculate_relevance(self, project_id: str, content: str) -> float:
        """è®¡ç®—ç›¸å…³æ€§è¯„åˆ†"""
        # ç®€åŒ–å®ç°ï¼šåŸºäºå†…å®¹é•¿åº¦å’Œå…³é”®è¯å¯†åº¦
        score = min(len(content) / 500, 1.0) * 0.8 + 0.2
        return score

    def _deduplicate_memories(self, memories: List[Dict]) -> List[Dict]:
        """å»é‡è®°å¿†"""
        seen = set()
        unique = []
        for mem in memories:
            content_hash = hash(mem["content"])
            if content_hash not in seen:
                seen.add(content_hash)
                unique.append(mem)
        return unique
```

### 5.2 Tokenä¼˜åŒ–æœåŠ¡

```python
# src/mcp_core/token_optimize/service.py
from transformers import AutoTokenizer, AutoModel
import torch

class TokenOptimizeService:
    def __init__(self, config: Dict):
        self.tokenizer = AutoTokenizer.from_pretrained("microsoft/codebert-base")
        self.model = AutoModel.from_pretrained("microsoft/codebert-base")

    def compress_content(
        self,
        content: str,
        content_type: str = "text",
        compression_ratio: float = 0.2
    ) -> Dict:
        """å‹ç¼©å†…å®¹"""
        # 1. è®¡ç®—åŸå§‹Tokenæ•°
        original_tokens = len(self.tokenizer.encode(content))

        # 2. æ ¹æ®å†…å®¹ç±»å‹é€‰æ‹©å‹ç¼©ç­–ç•¥
        if content_type == "code":
            compressed = self._compress_code(content, compression_ratio)
        else:
            compressed = self._compress_text(content, compression_ratio)

        # 3. è®¡ç®—å‹ç¼©åTokenæ•°
        compressed_tokens = len(self.tokenizer.encode(compressed))

        return {
            "original_tokens": original_tokens,
            "compressed_tokens": compressed_tokens,
            "compression_rate": 1 - (compressed_tokens / original_tokens),
            "compressed_content": compressed
        }

    def _compress_code(self, code: str, ratio: float) -> str:
        """ä»£ç å‹ç¼©ï¼ˆæå–æ ¸å¿ƒé€»è¾‘ï¼‰"""
        # ä½¿ç”¨CodeBERTæå–è¯­ä¹‰
        inputs = self.tokenizer(code, return_tensors="pt", truncation=True, max_length=512)
        with torch.no_grad():
            outputs = self.model(**inputs)

        # ç®€åŒ–å®ç°ï¼šæå–å‡½æ•°ç­¾åå’Œå…³é”®é€»è¾‘
        import ast
        try:
            tree = ast.parse(code)
            core_elements = []
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                    core_elements.append(ast.get_source_segment(code, node))
            return "\n".join(core_elements[:int(len(core_elements) * ratio)])
        except:
            return code[:int(len(code) * ratio)]

    def _compress_text(self, text: str, ratio: float) -> str:
        """æ–‡æœ¬å‹ç¼©ï¼ˆæ‘˜è¦æå–ï¼‰"""
        from summa.summarizer import summarize
        try:
            return summarize(text, ratio=ratio)
        except:
            return text[:int(len(text) * ratio)]
```

### 5.3 å¹»è§‰æŠ‘åˆ¶æœåŠ¡

```python
# src/mcp_core/anti_hallucination/service.py
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class HallucinationValidationService:
    def __init__(self, config: Dict, memory_service: MemoryService):
        self.memory_service = memory_service
        self.threshold = config.get("similarity_threshold", 0.65)
        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')

    def detect_hallucination(
        self,
        project_id: str,
        output: str,
        threshold: Optional[float] = None
    ) -> Dict:
        """æ£€æµ‹å¹»è§‰"""
        threshold = threshold or self.threshold

        # 1. ç”Ÿæˆè¾“å‡ºåµŒå…¥
        output_embedding = self.embedder.encode(output)

        # 2. æ£€ç´¢ç›¸å…³è®°å¿†
        memories = self.memory_service.retrieve_memory(
            project_id=project_id,
            query=output,
            top_k=3,
            memory_levels=["mid", "long"]
        )

        if not memories["memories"]:
            return {
                "is_hallucination": True,
                "confidence": 0.0,
                "reason": "æ— ç›¸å…³è®°å¿†æ”¯æ’‘"
            }

        # 3. è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for mem in memories["memories"]:
            mem_embedding = self.embedder.encode(mem["content"])
            sim = cosine_similarity(
                output_embedding.reshape(1, -1),
                mem_embedding.reshape(1, -1)
            )[0][0]
            similarities.append(sim)

        avg_similarity = np.mean(similarities)

        # 4. åŠ¨æ€è°ƒæ•´é˜ˆå€¼ï¼ˆå¤æ‚ä»»åŠ¡é™ä½10%ï¼‰
        if self._is_complex_task(output):
            threshold *= 0.9

        return {
            "is_hallucination": avg_similarity < threshold,
            "confidence": float(avg_similarity),
            "matched_memories": [m["memory_id"] for m in memories["memories"]],
            "threshold_used": threshold
        }

    def _is_complex_task(self, output: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¤æ‚ä»»åŠ¡"""
        # ç®€åŒ–å®ç°ï¼šæ ¹æ®è¾“å‡ºé•¿åº¦å’Œä»£ç å—æ•°é‡
        code_blocks = output.count("```")
        return len(output) > 500 or code_blocks > 2
```

---

## ğŸ“… å…­ã€å¼€å‘è·¯çº¿å›¾ä¸é‡Œç¨‹ç¢‘

### 6.1 è¿­ä»£è®¡åˆ’ï¼ˆ8å‘¨å®Œæ•´äº¤ä»˜ï¼‰

| é˜¶æ®µ | æ—¶é—´ | å…³é”®ä»»åŠ¡ | äº¤ä»˜ç‰© | ä¼˜å…ˆçº§ |
|-----|------|---------|--------|--------|
| **Phase 1: åŸºç¡€æ¶æ„** | Week 1-2 | â€¢ é¡¹ç›®åˆå§‹åŒ–<br>â€¢ æ•°æ®åº“Schemaè®¾è®¡<br>â€¢ APIæ¡†æ¶æ­å»º<br>â€¢ Dockerç¯å¢ƒé…ç½® | â€¢ é¡¹ç›®éª¨æ¶<br>â€¢ æ•°æ®åº“è„šæœ¬<br>â€¢ APIæ–‡æ¡£v1.0 | P0 |
| **Phase 2: æ ¸å¿ƒåŠŸèƒ½** | Week 3-4 | â€¢ è®°å¿†ç®¡ç†æ¨¡å—å¼€å‘<br>â€¢ Tokenä¼˜åŒ–æœåŠ¡<br>â€¢ Redis/Milvusé›†æˆ | â€¢ è®°å¿†å­˜å‚¨/æ£€ç´¢API<br>â€¢ Tokenå‹ç¼©åŠŸèƒ½ | P0 |
| **Phase 3: å¢å¼ºèƒ½åŠ›** | Week 5-6 | â€¢ å¹»è§‰æŠ‘åˆ¶æ¨¡å—<br>â€¢ æƒé™å®‰å…¨ç³»ç»Ÿ<br>â€¢ ç›‘æ§å‘Šè­¦é›†æˆ | â€¢ æ ¡éªŒæœåŠ¡<br>â€¢ æƒé™ä¸­é—´ä»¶<br>â€¢ PrometheusæŒ‡æ ‡ | P1 |
| **Phase 4: æµ‹è¯•ä¼˜åŒ–** | Week 7 | â€¢ å•å…ƒæµ‹è¯•è¦†ç›–<br>â€¢ æ€§èƒ½å‹æµ‹<br>â€¢ å¹»è§‰ç‡è¯„æµ‹ | â€¢ æµ‹è¯•æŠ¥å‘Š<br>â€¢ æ€§èƒ½åŸºå‡† | P0 |
| **Phase 5: éƒ¨ç½²ä¸Šçº¿** | Week 8 | â€¢ ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²<br>â€¢ æ–‡æ¡£å®Œå–„<br>â€¢ ç”¨æˆ·åŸ¹è®­ | â€¢ éƒ¨ç½²æ–‡æ¡£<br>â€¢ ä½¿ç”¨æ‰‹å†Œ | P1 |

### 6.2 è¯¦ç»†é‡Œç¨‹ç¢‘æ£€æŸ¥ç‚¹

**Week 1-2 æ£€æŸ¥ç‚¹**
- [ ] å®Œæˆé¡¹ç›®ç›®å½•ç»“æ„æ­å»º
- [ ] PostgreSQLæ•°æ®åº“è¡¨åˆ›å»ºå®Œæˆ
- [ ] Redisè¿æ¥æµ‹è¯•é€šè¿‡
- [ ] FastAPIåŸºç¡€è·¯ç”±è¿è¡Œæ­£å¸¸
- [ ] Docker Composeæœ¬åœ°ç¯å¢ƒå¯åŠ¨æˆåŠŸ

**Week 3-4 æ£€æŸ¥ç‚¹**
- [ ] è®°å¿†å­˜å‚¨APIæµ‹è¯•é€šè¿‡ï¼ˆè¦†ç›–ä¸‰çº§è®°å¿†ï¼‰
- [ ] è®°å¿†æ£€ç´¢å“åº”æ—¶é—´<300ms
- [ ] Tokenå‹ç¼©ç‡è¾¾åˆ°80%ä»¥ä¸Š
- [ ] Milvuså‘é‡æ£€ç´¢å‡†ç¡®ç‡>90%

**Week 5-6 æ£€æŸ¥ç‚¹**
- [ ] å¹»è§‰æ£€æµ‹å‡†ç¡®ç‡>95%
- [ ] æƒé™ç³»ç»Ÿé€šè¿‡å®‰å…¨å®¡è®¡
- [ ] Prometheusç›‘æ§æŒ‡æ ‡é‡‡é›†æ­£å¸¸
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡>70%

**Week 7 æ£€æŸ¥ç‚¹**
- [ ] æ€§èƒ½æµ‹è¯•è¾¾åˆ°100 QPS
- [ ] å†…å­˜æ³„æ¼æ£€æµ‹é€šè¿‡
- [ ] MME-RealWorldè¯„æµ‹å¹»è§‰ç‡<5%

**Week 8 æ£€æŸ¥ç‚¹**
- [ ] ç”Ÿäº§ç¯å¢ƒç¨³å®šè¿è¡Œ24å°æ—¶
- [ ] APIæ–‡æ¡£å®Œæ•´ä¸”å¯æ‰§è¡Œ
- [ ] è‡³å°‘1ä¸ªçœŸå®é¡¹ç›®é›†æˆæˆåŠŸ

---

## ğŸ§ª ä¸ƒã€æµ‹è¯•ç­–ç•¥ä¸éªŒæ”¶æ ‡å‡†

### 7.1 æµ‹è¯•é‡‘å­—å¡”

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  E2Eæµ‹è¯•    â”‚  10%ï¼ˆå…³é”®ä¸šåŠ¡æµç¨‹ï¼‰
        â”‚  (Playwright)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚   é›†æˆæµ‹è¯•        â”‚  30%ï¼ˆæ¨¡å—é—´äº¤äº’ï¼‰
       â”‚   (pytest)       â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚      å•å…ƒæµ‹è¯•             â”‚  60%ï¼ˆå‡½æ•°çº§åˆ«ï¼‰
    â”‚      (pytest)            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 å…³é”®æµ‹è¯•ç”¨ä¾‹

#### 7.2.1 è®°å¿†ç®¡ç†æµ‹è¯•
```python
# tests/test_memory_service.py
import pytest
from src.mcp_core.memory.service import MemoryService

@pytest.fixture
def memory_service(config):
    return MemoryService(config)

def test_store_short_memory(memory_service):
    """æµ‹è¯•çŸ­æœŸè®°å¿†å­˜å‚¨"""
    result = memory_service.store_memory(
        project_id="test_proj",
        content="æµ‹è¯•å†…å®¹ï¼šDjangoé¡¹ç›®é…ç½®",
        memory_level="short"
    )
    assert "memory_id" in result
    assert result["memory_id"].startswith("mem_")

def test_retrieve_memory_performance(memory_service):
    """æµ‹è¯•æ£€ç´¢æ€§èƒ½ï¼ˆ<300msï¼‰"""
    import time
    start = time.time()
    result = memory_service.retrieve_memory(
        project_id="test_proj",
        query="Djangoé…ç½®",
        top_k=5
    )
    elapsed = (time.time() - start) * 1000
    assert elapsed < 300, f"æ£€ç´¢è€—æ—¶{elapsed}msï¼Œè¶…è¿‡300ms"
    assert len(result["memories"]) > 0

def test_memory_deduplication(memory_service):
    """æµ‹è¯•è®°å¿†å»é‡"""
    # å­˜å‚¨ä¸¤æ¬¡ç›¸åŒå†…å®¹
    memory_service.store_memory("proj", "é‡å¤å†…å®¹", "mid")
    memory_service.store_memory("proj", "é‡å¤å†…å®¹", "mid")

    result = memory_service.retrieve_memory("proj", "é‡å¤å†…å®¹", top_k=10)
    contents = [m["content"] for m in result["memories"]]
    assert len(contents) == len(set(contents)), "å­˜åœ¨é‡å¤è®°å¿†"
```

#### 7.2.2 Tokenä¼˜åŒ–æµ‹è¯•
```python
# tests/test_token_optimize.py
def test_compression_rate(token_service):
    """æµ‹è¯•å‹ç¼©ç‡è¾¾æ ‡ï¼ˆâ‰¥80%ï¼‰"""
    long_text = "..." * 1000  # æ„é€ é•¿æ–‡æœ¬
    result = token_service.compress_content(long_text, "text", 0.2)

    assert result["compression_rate"] >= 0.80, \
        f"å‹ç¼©ç‡{result['compression_rate']}æœªè¾¾åˆ°80%"

def test_code_compression_accuracy(token_service):
    """æµ‹è¯•ä»£ç å‹ç¼©ä¿ç•™æ ¸å¿ƒé€»è¾‘"""
    code = """
    def calculate_total(items):
        total = 0
        for item in items:
            total += item.price
        return total
    """
    result = token_service.compress_content(code, "code", 0.3)

    # éªŒè¯å‡½æ•°ç­¾åè¢«ä¿ç•™
    assert "def calculate_total" in result["compressed_content"]
```

#### 7.2.3 å¹»è§‰æ£€æµ‹æµ‹è¯•
```python
# tests/test_anti_hallucination.py
def test_hallucination_detection_accuracy(validation_service):
    """æµ‹è¯•å¹»è§‰æ£€æµ‹å‡†ç¡®ç‡ï¼ˆâ‰¥95%ï¼‰"""
    # å‡†å¤‡æµ‹è¯•æ•°æ®é›†ï¼ˆ100æ¡æ­£å¸¸è¾“å‡º+100æ¡å¹»è§‰è¾“å‡ºï¼‰
    normal_outputs = load_test_data("normal_outputs.json")
    hallucination_outputs = load_test_data("hallucination_outputs.json")

    correct_detections = 0
    total_tests = 200

    for output in normal_outputs:
        result = validation_service.detect_hallucination("test_proj", output)
        if not result["is_hallucination"]:
            correct_detections += 1

    for output in hallucination_outputs:
        result = validation_service.detect_hallucination("test_proj", output)
        if result["is_hallucination"]:
            correct_detections += 1

    accuracy = correct_detections / total_tests
    assert accuracy >= 0.95, f"æ£€æµ‹å‡†ç¡®ç‡{accuracy}ä½äº95%"
```

### 7.3 æ€§èƒ½åŸºå‡†æµ‹è¯•

```python
# tests/performance/test_load.py
from locust import HttpUser, task, between

class MCPLoadTest(HttpUser):
    wait_time = between(1, 3)

    @task(3)
    def retrieve_memory(self):
        """æ£€ç´¢è®°å¿†ï¼ˆæƒé‡3ï¼‰"""
        self.client.get(
            "/api/v1/memory/retrieve",
            params={"project_id": "proj_001", "query": "æµ‹è¯•æŸ¥è¯¢", "top_k": 5},
            headers={"Authorization": "Bearer test_token"}
        )

    @task(1)
    def store_memory(self):
        """å­˜å‚¨è®°å¿†ï¼ˆæƒé‡1ï¼‰"""
        self.client.post(
            "/api/v1/memory/store",
            json={
                "project_id": "proj_001",
                "content": "æ€§èƒ½æµ‹è¯•æ•°æ®",
                "memory_level": "mid"
            },
            headers={"Authorization": "Bearer test_token"}
        )

# è¿è¡Œå‘½ä»¤: locust -f tests/performance/test_load.py --users 100 --spawn-rate 10
```

**éªŒæ”¶æ ‡å‡†**:
- 100å¹¶å‘ç”¨æˆ·ä¸‹ï¼ŒP95å“åº”æ—¶é—´<500ms
- é”™è¯¯ç‡<1%
- CPUä½¿ç”¨ç‡<70%
- å†…å­˜ä½¿ç”¨ç¨³å®šï¼ˆæ— æ³„æ¼ï¼‰

---

## ğŸš€ å…«ã€éƒ¨ç½²ä¸è¿ç»´æ–¹æ¡ˆ

### 8.1 Docker Composeæœ¬åœ°éƒ¨ç½²

```yaml
# docker-compose.yml
version: '3.8'

services:
  # MCPæ ¸å¿ƒæœåŠ¡
  mcp-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://mcp_user:mcp_pass@postgres:5432/mcp_db
      - REDIS_URL=redis://redis:6379/0
      - MILVUS_HOST=milvus-standalone
      - MILVUS_PORT=19530
    depends_on:
      - postgres
      - redis
      - milvus-standalone
    volumes:
      - ./logs:/app/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # PostgreSQLæ•°æ®åº“
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=mcp_db
      - POSTGRES_USER=mcp_user
      - POSTGRES_PASSWORD=mcp_pass
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init.sql
    ports:
      - "5432:5432"

  # Redisç¼“å­˜
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes

  # Milvuså‘é‡æ•°æ®åº“
  milvus-standalone:
    image: milvusdb/milvus:v2.3.4
    environment:
      - ETCD_ENDPOINTS=etcd:2379
      - MINIO_ADDRESS=minio:9000
    ports:
      - "19530:19530"
    depends_on:
      - etcd
      - minio

  # Milvusä¾èµ–ï¼šetcd
  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
    volumes:
      - etcd_data:/etcd

  # Milvusä¾èµ–ï¼šMinIO
  minio:
    image: minio/minio:RELEASE.2023-03-20T20-16-18Z
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    volumes:
      - minio_data:/minio_data
    command: minio server /minio_data

  # Prometheusç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus

  # Grafanaå¯è§†åŒ–
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana_dashboards:/etc/grafana/provisioning/dashboards

volumes:
  postgres_data:
  redis_data:
  etcd_data:
  minio_data:
  prometheus_data:
  grafana_data:
```

**å¯åŠ¨å‘½ä»¤**:
```bash
# æ„å»ºå¹¶å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d --build

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f mcp-api

# åœæ­¢æœåŠ¡
docker-compose down
```

### 8.2 ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼ˆKubernetesï¼‰

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mcp-api
  labels:
    app: mcp
spec:
  replicas: 3  # é«˜å¯ç”¨éƒ¨ç½²
  selector:
    matchLabels:
      app: mcp-api
  template:
    metadata:
      labels:
        app: mcp-api
    spec:
      containers:
      - name: mcp-api
        image: your-registry/mcp-api:latest
        ports:
        - containerPort: 8000
        env:
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: mcp-secrets
              key: database-url
        - name: REDIS_URL
          value: "redis://redis-service:6379/0"
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 10
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: mcp-api-service
spec:
  type: LoadBalancer
  selector:
    app: mcp-api
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8000
```

### 8.3 ç›‘æ§æŒ‡æ ‡ä¸å‘Šè­¦

```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'mcp-api'
    static_configs:
      - targets: ['mcp-api:8000']
    metrics_path: '/metrics'

# å‘Šè­¦è§„åˆ™
rule_files:
  - 'alerts.yml'

# monitoring/alerts.yml
groups:
- name: mcp_alerts
  rules:
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "MCP APIé”™è¯¯ç‡è¿‡é«˜"
      description: "è¿‡å»5åˆ†é’Ÿé”™è¯¯ç‡è¶…è¿‡5%"

  - alert: SlowResponse
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.5
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "MCP APIå“åº”ç¼“æ…¢"
      description: "P95å“åº”æ—¶é—´è¶…è¿‡500ms"

  - alert: HighMemoryUsage
    expr: container_memory_usage_bytes{container="mcp-api"} / container_spec_memory_limit_bytes{container="mcp-api"} > 0.85
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
      description: "å†…å­˜ä½¿ç”¨è¶…è¿‡85%"
```

### 8.4 å…³é”®ç›‘æ§æŒ‡æ ‡

| æŒ‡æ ‡ç±»åˆ« | æŒ‡æ ‡åç§° | æè¿° | å‘Šè­¦é˜ˆå€¼ |
|---------|---------|------|---------|
| ä¸šåŠ¡æŒ‡æ ‡ | memory_store_total | è®°å¿†å­˜å‚¨æ€»æ•° | - |
| ä¸šåŠ¡æŒ‡æ ‡ | memory_retrieve_latency_seconds | æ£€ç´¢å»¶è¿Ÿ | P95>500ms |
| ä¸šåŠ¡æŒ‡æ ‡ | token_saved_total | ç´¯è®¡èŠ‚çœTokenæ•° | - |
| ä¸šåŠ¡æŒ‡æ ‡ | hallucination_detected_total | æ£€æµ‹åˆ°çš„å¹»è§‰æ¬¡æ•° | - |
| ç³»ç»ŸæŒ‡æ ‡ | http_requests_total | HTTPè¯·æ±‚æ€»æ•° | é”™è¯¯ç‡>5% |
| ç³»ç»ŸæŒ‡æ ‡ | redis_connected_clients | Redisè¿æ¥æ•° | >1000 |
| ç³»ç»ŸæŒ‡æ ‡ | postgres_connections | æ•°æ®åº“è¿æ¥æ•° | >80% |
| èµ„æºæŒ‡æ ‡ | container_cpu_usage_percent | CPUä½¿ç”¨ç‡ | >80% |
| èµ„æºæŒ‡æ ‡ | container_memory_usage_percent | å†…å­˜ä½¿ç”¨ç‡ | >85% |

---

## ğŸ“– ä¹ã€é¡¹ç›®é…ç½®æ–‡ä»¶å®Œæ•´ç¤ºä¾‹

### 9.1 pyproject.toml
```toml
[project]
name = "mcp-project"
version = "1.0.0"
description = "MCPè®°å¿†æ§åˆ¶æœºåˆ¶ - å®Œæ•´å®ç°"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "MIT"}
authors = [
    {name = "Your Name", email = "your.email@example.com"}
]

dependencies = [
    "mcp-sdk>=0.5.0",
    "fastapi>=0.108.0",
    "uvicorn[standard]>=0.25.0",
    "pydantic>=2.5.0",
    "sqlalchemy>=2.0.23",
    "psycopg2-binary>=2.9.9",
    "redis>=5.0.1",
    "pymilvus>=2.3.4",
    "faiss-cpu>=1.8.0",
    "sentence-transformers>=2.2.2",
    "transformers>=4.36.0",
    "torch>=2.1.0",
    "numpy>=1.24.0",
    "pyyaml>=6.0.1",
    "httpx>=0.27.0",
    "prometheus-client>=0.19.0",
    "python-jose[cryptography]>=3.3.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=7.4.3",
    "pytest-asyncio>=0.21.1",
    "pytest-cov>=4.1.0",
    "locust>=2.20.0",
    "black>=23.12.0",
    "ruff>=0.1.9",
    "mypy>=1.7.1",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
addopts = "-v --cov=src --cov-report=html --cov-report=term"

[tool.black]
line-length = 100
target-version = ['py310']

[tool.ruff]
line-length = 100
select = ["E", "F", "W", "I", "N"]
ignore = ["E501"]
```

### 9.2 config.yamlï¼ˆå¢å¼ºç‰ˆï¼‰
```yaml
# é¡¹ç›®åŸºç¡€é…ç½®
project:
  name: "mcp-core"
  version: "1.0.0"
  environment: "development"  # development/testing/production

# æ—¥å¿—é…ç½®
logging:
  level: "INFO"
  format: "json"  # json/text
  output: "file"  # file/stdout/both
  file_path: "./logs/mcp.log"
  max_bytes: 10485760  # 10MB
  backup_count: 5

# æ•°æ®åº“é…ç½®
database:
  url: "postgresql://mcp_user:mcp_pass@localhost:5432/mcp_db"
  pool_size: 20
  max_overflow: 10
  pool_timeout: 30
  echo: false  # SQLæ—¥å¿—

# Redisé…ç½®
redis:
  url: "redis://localhost:6379/0"
  max_connections: 50
  socket_timeout: 5
  socket_connect_timeout: 5

# å‘é‡æ•°æ®åº“é…ç½®
vector_db:
  type: "milvus"  # milvus/faiss
  milvus:
    host: "localhost"
    port: 19530
    index_type: "HNSW"
    metric_type: "COSINE"
  faiss:
    index_path: "./data/faiss_index"
    dimension: 768

# è®°å¿†ç®¡ç†é…ç½®
memory:
  short_term:
    ttl: 86400  # 24å°æ—¶ï¼ˆç§’ï¼‰
    max_window: 20  # æœ€å¤§çª—å£å¤§å°
    min_window: 5
  mid_term:
    ttl: 2592000  # 30å¤©ï¼ˆç§’ï¼‰
    auto_archive: true
  long_term:
    min_confidence: 0.80

# Tokenä¼˜åŒ–é…ç½®
token_optimization:
  compression_ratio: 0.2
  cache_ttl: 604800  # 7å¤©ï¼ˆç§’ï¼‰
  code_model: "microsoft/codebert-base"
  text_model: "sentence-transformers/all-MiniLM-L6-v2"

# å¹»è§‰æŠ‘åˆ¶é…ç½®
anti_hallucination:
  similarity_threshold: 0.65
  complex_task_threshold_multiplier: 0.9
  max_retries: 3
  enable_fact_check: true

# å®‰å…¨é…ç½®
security:
  encryption_algorithm: "AES-256-GCM"
  jwt_secret: "your-secret-key-change-in-production"
  jwt_expiration: 86400  # 24å°æ—¶
  ssl_enabled: false  # ç”Ÿäº§ç¯å¢ƒæ”¹ä¸ºtrue

# APIé…ç½®
api:
  host: "0.0.0.0"
  port: 8000
  workers: 4
  timeout: 60
  cors_origins: ["*"]  # ç”Ÿäº§ç¯å¢ƒé™åˆ¶å…·ä½“åŸŸå
  rate_limit:
    enabled: true
    requests_per_minute: 100

# ç›‘æ§é…ç½®
monitoring:
  prometheus:
    enabled: true
    port: 9090
  metrics_prefix: "mcp"
  health_check_interval: 30

# ç¯å¢ƒç‰¹å®šé…ç½®ï¼ˆå¯é€‰ï¼‰
environments:
  development:
    logging:
      level: "DEBUG"
    database:
      echo: true

  production:
    logging:
      level: "WARNING"
    security:
      ssl_enabled: true
    api:
      cors_origins: ["https://your-domain.com"]
```

---

## ğŸ” åã€å®‰å…¨æœ€ä½³å®è·µ

### 10.1 æ•æ„Ÿä¿¡æ¯ç®¡ç†
```bash
# .env.exampleï¼ˆä¸åŒ…å«çœŸå®å¯†é’¥ï¼‰
DATABASE_URL=postgresql://user:password@localhost:5432/db
REDIS_URL=redis://localhost:6379/0
JWT_SECRET=your-jwt-secret
MILVUS_HOST=localhost
MILVUS_PORT=19530

# ç”Ÿäº§ç¯å¢ƒä½¿ç”¨å¯†é’¥ç®¡ç†æœåŠ¡ï¼ˆå¦‚AWS Secrets Managerï¼‰
```

### 10.2 APIé‰´æƒä¸­é—´ä»¶
```python
# src/mcp_core/security/auth.py
from fastapi import HTTPException, Security
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from jose import JWTError, jwt
from datetime import datetime, timedelta

security = HTTPBearer()

def create_access_token(data: dict, expires_delta: timedelta = None):
    """åˆ›å»ºJWT Token"""
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(hours=24))
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, JWT_SECRET, algorithm="HS256")
    return encoded_jwt

async def verify_token(credentials: HTTPAuthorizationCredentials = Security(security)):
    """éªŒè¯JWT Token"""
    try:
        payload = jwt.decode(credentials.credentials, JWT_SECRET, algorithms=["HS256"])
        user_id: str = payload.get("sub")
        if user_id is None:
            raise HTTPException(status_code=401, detail="æ— æ•ˆçš„Token")
        return user_id
    except JWTError:
        raise HTTPException(status_code=401, detail="TokenéªŒè¯å¤±è´¥")
```

### 10.3 è¾“å…¥éªŒè¯ï¼ˆé˜²æ³¨å…¥ï¼‰
```python
from pydantic import BaseModel, Field, validator

class MemoryStoreRequest(BaseModel):
    project_id: str = Field(..., min_length=1, max_length=64, regex="^[a-zA-Z0-9_-]+$")
    content: str = Field(..., min_length=1, max_length=10000)
    memory_level: str = Field(..., regex="^(short|mid|long)$")

    @validator('content')
    def sanitize_content(cls, v):
        # ç§»é™¤æ½œåœ¨çš„SQLæ³¨å…¥å­—ç¬¦
        dangerous_chars = ["';", "--", "/*", "*/", "xp_", "sp_"]
        for char in dangerous_chars:
            if char in v.lower():
                raise ValueError(f"å†…å®¹åŒ…å«éæ³•å­—ç¬¦: {char}")
        return v
```

---

## ğŸ“š åä¸€ã€æœ€ä½³å®è·µä¸æ³¨æ„äº‹é¡¹

### 11.1 å¼€å‘è§„èŒƒ
1. **ä»£ç é£æ ¼**: ä½¿ç”¨Blackæ ¼å¼åŒ–ï¼ŒRuffæ£€æŸ¥ï¼Œå¼ºåˆ¶100å­—ç¬¦è¡Œå®½
2. **ç±»å‹æ³¨è§£**: æ‰€æœ‰å‡½æ•°å¿…é¡»æ·»åŠ ç±»å‹æç¤ºï¼ˆä½¿ç”¨mypyæ£€æŸ¥ï¼‰
3. **æ–‡æ¡£å­—ç¬¦ä¸²**: ä½¿ç”¨Googleé£æ ¼çš„docstring
4. **é”™è¯¯å¤„ç†**: ä½¿ç”¨è‡ªå®šä¹‰å¼‚å¸¸ç±»ï¼Œé¿å…è£¸except
5. **æ—¥å¿—è®°å½•**: å…³é”®æ“ä½œå¿…é¡»è®°å½•INFOçº§åˆ«æ—¥å¿—ï¼Œé”™è¯¯è®°å½•ERRORçº§åˆ«

### 11.2 æ€§èƒ½ä¼˜åŒ–å»ºè®®
1. **æ‰¹é‡æ“ä½œ**: å‘é‡æ£€ç´¢ä½¿ç”¨æ‰¹é‡æŸ¥è¯¢ï¼Œå‡å°‘ç½‘ç»œå¾€è¿”
2. **è¿æ¥æ± **: æ•°æ®åº“å’ŒRedisä½¿ç”¨è¿æ¥æ± ï¼Œé¿å…é¢‘ç¹åˆ›å»ºè¿æ¥
3. **å¼‚æ­¥IO**: ä½¿ç”¨FastAPIçš„å¼‚æ­¥ç‰¹æ€§å¤„ç†å¹¶å‘è¯·æ±‚
4. **ç¼“å­˜ç­–ç•¥**: çƒ­ç‚¹æ•°æ®ä½¿ç”¨Redisç¼“å­˜ï¼Œè®¾ç½®åˆç†çš„TTL
5. **ç´¢å¼•ä¼˜åŒ–**: PostgreSQLè¡¨æ·»åŠ å¿…è¦çš„ç´¢å¼•ï¼ˆå‚è€ƒSchemaè®¾è®¡ï¼‰

### 11.3 å¸¸è§é™·é˜±
âŒ **é”™è¯¯åšæ³•**: åœ¨çŸ­æœŸè®°å¿†ä¸­å­˜å‚¨å¤§é‡é‡å¤ä¿¡æ¯
âœ… **æ­£ç¡®åšæ³•**: ä½¿ç”¨å»é‡æœºåˆ¶ï¼Œåªå­˜å‚¨å·®å¼‚ä¿¡æ¯

âŒ **é”™è¯¯åšæ³•**: æ‰€æœ‰è®°å¿†éƒ½å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
âœ… **æ­£ç¡®åšæ³•**: æ ¹æ®è®¿é—®é¢‘ç‡å’Œé‡è¦æ€§åˆ†çº§å­˜å‚¨

âŒ **é”™è¯¯åšæ³•**: ç¡¬ç¼–ç ç›¸ä¼¼åº¦é˜ˆå€¼
âœ… **æ­£ç¡®åšæ³•**: æ ¹æ®ä»»åŠ¡å¤æ‚åº¦åŠ¨æ€è°ƒæ•´é˜ˆå€¼

### 11.4 æ•…éšœæ’æŸ¥æ¸…å•
```bash
# 1. æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€
curl http://localhost:8000/health

# 2. æ£€æŸ¥æ•°æ®åº“è¿æ¥
psql -h localhost -U mcp_user -d mcp_db -c "SELECT 1"

# 3. æ£€æŸ¥Redisè¿æ¥
redis-cli ping

# 4. æ£€æŸ¥Milvusè¿æ¥
curl http://localhost:19530/healthz

# 5. æŸ¥çœ‹æœ€è¿‘é”™è¯¯æ—¥å¿—
tail -n 100 logs/mcp.log | grep ERROR

# 6. æ£€æŸ¥å†…å­˜ä½¿ç”¨
docker stats mcp-api

# 7. æŸ¥çœ‹æ…¢æŸ¥è¯¢
grep "duration > 500ms" logs/mcp.log
```

---

## ğŸ“ åäºŒã€æ‰©å±•æ–¹å‘ä¸æœªæ¥è§„åˆ’

### 12.1 çŸ­æœŸæ‰©å±•ï¼ˆ3ä¸ªæœˆå†…ï¼‰
- [ ] **å¤šç§Ÿæˆ·æ”¯æŒ**: æ·»åŠ ç§Ÿæˆ·éš”ç¦»æœºåˆ¶ï¼Œæ”¯æŒSaaSæ¨¡å¼
- [ ] **GraphQL API**: æä¾›GraphQLæ¥å£ï¼Œä¼˜åŒ–å‰ç«¯æŸ¥è¯¢æ•ˆç‡
- [ ] **Webhooké€šçŸ¥**: è®°å¿†æ›´æ–°æ—¶è§¦å‘Webhookï¼Œé›†æˆç¬¬ä¸‰æ–¹æœåŠ¡
- [ ] **æ‰¹é‡å¯¼å…¥å·¥å…·**: æ”¯æŒä»æ–‡æ¡£/ä»£ç åº“æ‰¹é‡å¯¼å…¥è®°å¿†

### 12.2 ä¸­æœŸæ‰©å±•ï¼ˆ6ä¸ªæœˆå†…ï¼‰
- [ ] **å¤šæ¨¡æ€è®°å¿†**: æ”¯æŒå›¾ç‰‡ã€éŸ³é¢‘ã€è§†é¢‘çš„è®°å¿†å­˜å‚¨ä¸æ£€ç´¢
- [ ] **æ™ºèƒ½æ‘˜è¦**: åŸºäºLLMçš„è‡ªåŠ¨æ‘˜è¦ç”Ÿæˆï¼ˆæ›¿ä»£è§„åˆ™å‹ç¼©ï¼‰
- [ ] **è®°å¿†æ¨è**: ä¸»åŠ¨æ¨é€ç›¸å…³è®°å¿†ï¼Œæå‡å¼€å‘æ•ˆç‡
- [ ] **A/Bæµ‹è¯•æ¡†æ¶**: æ”¯æŒä¸åŒè®°å¿†ç­–ç•¥çš„æ•ˆæœå¯¹æ¯”

### 12.3 é•¿æœŸè§„åˆ’ï¼ˆ1å¹´å†…ï¼‰
- [ ] **è”é‚¦å­¦ä¹ **: æ”¯æŒå¤šé¡¹ç›®é—´çš„éšç§ä¿æŠ¤è®°å¿†å…±äº«
- [ ] **è‡ªé€‚åº”ä¼˜åŒ–**: æ ¹æ®ç”¨æˆ·åé¦ˆè‡ªåŠ¨è°ƒæ•´è®°å¿†æƒé‡å’Œå‹ç¼©ç­–ç•¥
- [ ] **è¾¹ç¼˜éƒ¨ç½²**: æ”¯æŒå®¢æˆ·ç«¯æœ¬åœ°éƒ¨ç½²ï¼Œé™ä½å»¶è¿Ÿ
- [ ] **å¤šæ™ºèƒ½ä½“åä½œ**: æ„å»ºè®°å¿†å…±äº«ç½‘ç»œï¼Œå®ç°å›¢é˜ŸååŒç¼–ç¨‹

---

## ğŸ“ åä¸‰ã€æ”¯æŒä¸åé¦ˆ

### æŠ€æœ¯æ”¯æŒ
- ğŸ“§ é‚®ç®±: support@mcp-project.com
- ğŸ’¬ Discord: https://discord.gg/mcp-community
- ğŸ“– æ–‡æ¡£: https://docs.mcp-project.com
- ğŸ› é—®é¢˜åé¦ˆ: https://github.com/your-org/mcp-project/issues

### è´¡çŒ®æŒ‡å—
æ¬¢è¿æäº¤PRï¼è¯·éµå¾ªä»¥ä¸‹æµç¨‹ï¼š
1. Forkæœ¬ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/amazing-feature`)
3. æäº¤ä»£ç  (`git commit -m 'Add amazing feature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/amazing-feature`)
5. åˆ›å»ºPull Request

---

## ğŸ“„ é™„å½•

### A. æœ¯è¯­è¡¨
- **MCP**: è®°å¿†æ§åˆ¶æœºåˆ¶ï¼ˆMemory Control Protocolï¼‰
- **å‘é‡åµŒå…¥**: å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡çš„è¡¨ç¤ºå½¢å¼
- **å¹»è§‰**: LLMç”Ÿæˆçš„ä¸äº‹å®ä¸ç¬¦çš„å†…å®¹
- **Token**: æ–‡æœ¬çš„æœ€å°å•ä½ï¼Œé€šå¸¸ä¸ºè¯æˆ–å­è¯
- **P95å»¶è¿Ÿ**: 95%çš„è¯·æ±‚å“åº”æ—¶é—´ä½äºæ­¤å€¼

### B. å‚è€ƒèµ„æ–™
1. [MCPå®˜æ–¹æ–‡æ¡£](https://modelcontextprotocol.io)
2. [MIRIXè®ºæ–‡](https://arxiv.org/abs/2401.14604)
3. [MemInsightè®ºæ–‡](https://arxiv.org/abs/2408.16819)
4. [Milvuså‘é‡æ•°æ®åº“](https://milvus.io/docs)
5. [FastAPIæœ€ä½³å®è·µ](https://fastapi.tiangolo.com/tutorial/)

### C. å¿«é€Ÿå¯åŠ¨å‘½ä»¤
```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/your-org/mcp-project.git
cd mcp-project

# å®‰è£…ä¾èµ–
uv venv && source .venv/bin/activate
uv sync

# åˆå§‹åŒ–æ•°æ®åº“
python scripts/init_database.py

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
uvicorn src.mcp_core.main:app --reload --port 8000

# è¿è¡Œæµ‹è¯•
pytest tests/ -v --cov

# æ„å»ºDockeré•œåƒ
docker build -t mcp-api:latest .

# å¯åŠ¨å®Œæ•´ç¯å¢ƒ
docker-compose up -d
```

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0.0
**æœ€åæ›´æ–°**: 2025-01-18
**ç»´æŠ¤è€…**: MCP Development Team
