"""
æ··åˆè®°å¿†å­˜å‚¨ç³»ç»Ÿ - SQLite + MySQL
å®ç°æœ¬åœ°é«˜æ€§èƒ½ä¸å›¢é˜Ÿå…±äº«çš„å®Œç¾ç»“åˆ
"""

import os
import json
import sqlite3
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime, timedelta
from pathlib import Path
import hashlib

from sqlalchemy import create_engine, Column, String, Integer, Text, DateTime, Boolean, Float
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from sqlalchemy.pool import QueuePool

from src.mcp_core.common.logger import get_logger
from src.mcp_core.common.config import get_settings

logger = get_logger(__name__)

Base = declarative_base()

# ============================================
# æ•°æ®æ¨¡å‹
# ============================================

class MemorySnapshot(Base):
    """MySQLä¸­çš„è®°å¿†å¿«ç…§è¡¨"""
    __tablename__ = 'memory_snapshots'

    id = Column(String(50), primary_key=True)
    project_id = Column(String(50), index=True)
    project_path = Column(Text)
    timestamp = Column(DateTime, default=datetime.now)
    trigger_type = Column(String(50))
    node_count = Column(Integer)
    edge_count = Column(Integer)
    complexity = Column(Float)
    graph_data = Column(Text)  # JSON
    meta_data = Column(Text)    # JSON (renamed from metadata to avoid SQLAlchemy conflict)
    insights = Column(Text)    # JSON
    hash = Column(String(100), index=True)
    created_by = Column(String(100))
    team_id = Column(String(50), index=True)
    is_public = Column(Boolean, default=False)

class MemoryPattern(Base):
    """è¯†åˆ«çš„æ¨¡å¼è¡¨"""
    __tablename__ = 'memory_patterns'

    id = Column(Integer, primary_key=True, autoincrement=True)
    pattern_type = Column(String(50))
    pattern_data = Column(Text)
    frequency = Column(Integer, default=1)
    projects = Column(Text)  # JSON array of project IDs
    first_seen = Column(DateTime, default=datetime.now)
    last_seen = Column(DateTime, default=datetime.now)
    confidence = Column(Float)

class SharedInsight(Base):
    """å›¢é˜Ÿå…±äº«çš„æ´å¯Ÿ"""
    __tablename__ = 'shared_insights'

    id = Column(Integer, primary_key=True, autoincrement=True)
    project_id = Column(String(50), index=True)
    insight_type = Column(String(50))
    content = Column(Text)
    created_at = Column(DateTime, default=datetime.now)
    created_by = Column(String(100))
    upvotes = Column(Integer, default=0)
    tags = Column(Text)  # JSON array

# ============================================
# æ··åˆå­˜å‚¨ç®¡ç†å™¨
# ============================================

class HybridStorageManager:
    """æ··åˆå­˜å‚¨ç®¡ç†å™¨ - æ™ºèƒ½é€‰æ‹©å­˜å‚¨ç­–ç•¥"""

    def __init__(self, project_path: str):
        self.project_path = project_path
        self.project_id = self._generate_project_id(project_path)

        # åˆå§‹åŒ–å­˜å‚¨
        self.local_storage = LocalSQLiteStorage(project_path)
        self.central_storage = CentralMySQLStorage()

        # åŒæ­¥é…ç½®
        self.sync_enabled = True
        self.sync_interval = 300  # 5åˆ†é’Ÿ
        self.last_sync = None

        # ç¼“å­˜é…ç½®
        self.cache_ttl = 3600  # 1å°æ—¶
        self.cache = {}

        # å¯åŠ¨åå°ä»»åŠ¡
        self._start_background_tasks()

    def _generate_project_id(self, path: str) -> str:
        """ç”Ÿæˆé¡¹ç›®å”¯ä¸€ID"""
        return hashlib.md5(path.encode()).hexdigest()[:12]

    def _start_background_tasks(self):
        """å¯åŠ¨åå°ä»»åŠ¡"""
        if self.sync_enabled:
            asyncio.create_task(self._auto_sync_task())
            asyncio.create_task(self._cleanup_task())

    async def _auto_sync_task(self):
        """è‡ªåŠ¨åŒæ­¥ä»»åŠ¡"""
        while self.sync_enabled:
            await asyncio.sleep(self.sync_interval)
            try:
                await self.sync_to_central()
            except Exception as e:
                logger.error(f"è‡ªåŠ¨åŒæ­¥å¤±è´¥: {e}")

    async def _cleanup_task(self):
        """æ¸…ç†ä»»åŠ¡"""
        while True:
            await asyncio.sleep(3600)  # æ¯å°æ—¶æ¸…ç†
            try:
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                self._cleanup_cache()
                # æ¸…ç†æ—§å¿«ç…§
                await self.cleanup_old_snapshots()
            except Exception as e:
                logger.error(f"æ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")

    async def save(self, data: Dict[str, Any], options: Dict[str, Any] = None) -> str:
        """æ™ºèƒ½ä¿å­˜ - æ ¹æ®ç­–ç•¥é€‰æ‹©å­˜å‚¨"""
        options = options or {}

        # ç”Ÿæˆå¿«ç…§ID
        snapshot_id = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:20]

        # å‡†å¤‡æ•°æ®
        snapshot_data = {
            'id': snapshot_id,
            'project_id': self.project_id,
            'project_path': self.project_path,
            'timestamp': datetime.now().isoformat(),
            'data': data,
            'metadata': options.get('metadata', {}),
            'hash': self._calculate_hash(data)
        }

        # 1. æ€»æ˜¯å…ˆä¿å­˜åˆ°æœ¬åœ°ï¼ˆå¿«é€Ÿï¼‰
        self.local_storage.save(snapshot_id, snapshot_data)
        logger.info(f"å¿«ç…§å·²ä¿å­˜åˆ°æœ¬åœ°: {snapshot_id}")

        # 2. æ ¹æ®ç­–ç•¥å†³å®šæ˜¯å¦åŒæ­¥åˆ°ä¸­å¤®
        if self._should_sync_to_central(options):
            # å¼‚æ­¥åŒæ­¥åˆ°MySQL
            asyncio.create_task(self._async_save_to_central(snapshot_data))

        # 3. æ›´æ–°ç¼“å­˜
        self._update_cache(snapshot_id, snapshot_data)

        return snapshot_id

    def _should_sync_to_central(self, options: Dict) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åŒæ­¥åˆ°ä¸­å¤®"""
        # ç­–ç•¥1: æ˜ç¡®æŒ‡å®š
        if 'sync' in options:
            return options['sync']

        # ç­–ç•¥2: é‡è¦æ€§åˆ¤æ–­
        if options.get('importance', 'normal') == 'high':
            return True

        # ç­–ç•¥3: å›¢é˜Ÿæ¨¡å¼
        if options.get('team_mode', False):
            return True

        # ç­–ç•¥4: é‡Œç¨‹ç¢‘äº‹ä»¶
        if options.get('trigger') in ['milestone', 'release', 'commit']:
            return True

        # é»˜è®¤ï¼šå®šæœŸåŒæ­¥
        return self.sync_enabled

    async def _async_save_to_central(self, snapshot_data: Dict):
        """å¼‚æ­¥ä¿å­˜åˆ°ä¸­å¤®MySQL"""
        try:
            self.central_storage.save(snapshot_data)
            # æ ‡è®°æœ¬åœ°ä¸ºå·²åŒæ­¥
            self.local_storage.mark_synced(snapshot_data['id'])
            logger.info(f"å¿«ç…§å·²åŒæ­¥åˆ°MySQL: {snapshot_data['id']}")
        except Exception as e:
            logger.error(f"åŒæ­¥åˆ°MySQLå¤±è´¥: {e}")
            # å¤±è´¥åä¼šåœ¨ä¸‹æ¬¡è‡ªåŠ¨åŒæ­¥æ—¶é‡è¯•

    async def load(self, snapshot_id: str) -> Optional[Dict]:
        """æ™ºèƒ½åŠ è½½ - å¤šçº§æŸ¥æ‰¾"""
        # 1. æ£€æŸ¥ç¼“å­˜
        cached = self._get_from_cache(snapshot_id)
        if cached:
            logger.debug(f"ä»ç¼“å­˜åŠ è½½: {snapshot_id}")
            return cached

        # 2. æŸ¥è¯¢æœ¬åœ°
        local_data = self.local_storage.load(snapshot_id)
        if local_data:
            logger.debug(f"ä»æœ¬åœ°åŠ è½½: {snapshot_id}")
            self._update_cache(snapshot_id, local_data)
            return local_data

        # 3. æŸ¥è¯¢ä¸­å¤®
        central_data = await self.central_storage.load(snapshot_id)
        if central_data:
            logger.debug(f"ä»ä¸­å¤®åŠ è½½: {snapshot_id}")
            # ç¼“å­˜åˆ°æœ¬åœ°
            self.local_storage.save(snapshot_id, central_data)
            self._update_cache(snapshot_id, central_data)
            return central_data

        return None

    async def search(self, query: str, options: Dict = None) -> List[Dict]:
        """æ™ºèƒ½æœç´¢ - å¹¶è¡Œæœç´¢å¤šä¸ªæº"""
        options = options or {}
        results = []

        # å†³å®šæœç´¢èŒƒå›´
        search_local = options.get('local', True)
        search_central = options.get('central', True)
        search_team = options.get('team', False)

        tasks = []

        if search_local:
            tasks.append(self._search_local(query))

        if search_central:
            tasks.append(self._search_central(query))

        if search_team:
            tasks.append(self._search_team_projects(query))

        # å¹¶è¡Œæ‰§è¡Œæœç´¢
        if tasks:
            search_results = await asyncio.gather(*tasks, return_exceptions=True)
            for result in search_results:
                if isinstance(result, list):
                    results.extend(result)

        # å»é‡å’Œæ’åº
        unique_results = self._deduplicate_results(results)
        return self._rank_results(unique_results, query)

    async def _search_local(self, query: str) -> List[Dict]:
        """æœ¬åœ°æœç´¢"""
        return self.local_storage.search(query)

    async def _search_central(self, query: str) -> List[Dict]:
        """ä¸­å¤®æœç´¢"""
        return await self.central_storage.search(query, self.project_id)

    async def _search_team_projects(self, query: str) -> List[Dict]:
        """æœç´¢å›¢é˜Ÿå…¶ä»–é¡¹ç›®"""
        team_id = self._get_team_id()
        if team_id:
            return await self.central_storage.search_team(query, team_id)
        return []

    async def sync_to_central(self):
        """åŒæ­¥æœ¬åœ°æœªåŒæ­¥çš„æ•°æ®åˆ°ä¸­å¤®"""
        unsynced = self.local_storage.get_unsynced()

        success_count = 0
        for snapshot in unsynced:
            try:
                await self.central_storage.save(snapshot)
                self.local_storage.mark_synced(snapshot['id'])
                success_count += 1
            except Exception as e:
                logger.error(f"åŒæ­¥å¿«ç…§ {snapshot['id']} å¤±è´¥: {e}")

        if success_count > 0:
            logger.info(f"æˆåŠŸåŒæ­¥ {success_count} ä¸ªå¿«ç…§åˆ°ä¸­å¤®å­˜å‚¨")

        self.last_sync = datetime.now()
        return success_count

    async def share_insight(self, content: str, tags: List[str] = None):
        """åˆ†äº«æ´å¯Ÿåˆ°å›¢é˜Ÿ"""
        insight = {
            'project_id': self.project_id,
            'content': content,
            'tags': tags or [],
            'created_by': self._get_current_user(),
            'created_at': datetime.now()
        }

        return await self.central_storage.save_insight(insight)

    async def get_team_insights(self, limit: int = 20) -> List[Dict]:
        """è·å–å›¢é˜Ÿæ´å¯Ÿ"""
        team_id = self._get_team_id()
        if team_id:
            return await self.central_storage.get_team_insights(team_id, limit)
        return []

    async def analyze_patterns(self) -> Dict[str, Any]:
        """åˆ†ææ¨¡å¼ - ç»“åˆæœ¬åœ°å’Œå…¨å±€"""
        # æœ¬åœ°æ¨¡å¼
        local_patterns = self.local_storage.analyze_patterns()

        # å…¨å±€æ¨¡å¼
        global_patterns = await self.central_storage.get_global_patterns()

        # å›¢é˜Ÿæ¨¡å¼
        team_patterns = await self.central_storage.get_team_patterns(self._get_team_id())

        return {
            'local': local_patterns,
            'global': global_patterns,
            'team': team_patterns,
            'recommendations': self._generate_recommendations(
                local_patterns, global_patterns, team_patterns
            )
        }

    def _generate_recommendations(self, local, global_p, team) -> List[str]:
        """åŸºäºæ¨¡å¼ç”Ÿæˆå»ºè®®"""
        recommendations = []

        # åˆ†ææœ¬åœ°ä¸å…¨å±€å·®å¼‚
        if local and global_p:
            # æ¯”è¾ƒå¤æ‚åº¦
            if local.get('avg_complexity', 0) > global_p.get('avg_complexity', 0) * 1.5:
                recommendations.append("æ‚¨çš„é¡¹ç›®å¤æ‚åº¦é«˜äºå¹³å‡æ°´å¹³ï¼Œè€ƒè™‘é‡æ„")

            # æ¯”è¾ƒä¾èµ–å¯†åº¦
            if local.get('dependency_density', 0) > global_p.get('dependency_density', 0) * 1.2:
                recommendations.append("ä¾èµ–å…³ç³»å¯†é›†ï¼Œå»ºè®®è§£è€¦æ¨¡å—")

        # åŸºäºå›¢é˜Ÿæ¨¡å¼
        if team:
            best_practices = team.get('best_practices', [])
            for practice in best_practices[:3]:
                recommendations.append(f"å›¢é˜Ÿæœ€ä½³å®è·µ: {practice}")

        return recommendations

    async def cleanup_old_snapshots(self, days: int = 30):
        """æ¸…ç†æ—§å¿«ç…§"""
        cutoff_date = datetime.now() - timedelta(days=days)

        # æ¸…ç†æœ¬åœ°
        local_deleted = self.local_storage.delete_before(cutoff_date)

        # ä¸­å¤®ä¿ç•™æ›´ä¹…ï¼ˆå¯é…ç½®ï¼‰
        if days > 90:  # åªæœ‰è¶…è¿‡90å¤©æ‰æ¸…ç†ä¸­å¤®
            central_deleted = await self.central_storage.delete_before(cutoff_date)
        else:
            central_deleted = 0

        logger.info(f"æ¸…ç†å®Œæˆ: æœ¬åœ°åˆ é™¤ {local_deleted}, ä¸­å¤®åˆ é™¤ {central_deleted}")
        return local_deleted + central_deleted

    def _calculate_hash(self, data: Any) -> str:
        """è®¡ç®—æ•°æ®å“ˆå¸Œ"""
        data_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(data_str.encode()).hexdigest()

    def _update_cache(self, key: str, value: Any):
        """æ›´æ–°ç¼“å­˜"""
        self.cache[key] = {
            'data': value,
            'timestamp': datetime.now()
        }

    def _get_from_cache(self, key: str) -> Optional[Any]:
        """ä»ç¼“å­˜è·å–"""
        if key in self.cache:
            cached = self.cache[key]
            age = (datetime.now() - cached['timestamp']).total_seconds()
            if age < self.cache_ttl:
                return cached['data']
            else:
                del self.cache[key]
        return None

    def _cleanup_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        now = datetime.now()
        expired = []

        for key, value in self.cache.items():
            age = (now - value['timestamp']).total_seconds()
            if age > self.cache_ttl:
                expired.append(key)

        for key in expired:
            del self.cache[key]

        if expired:
            logger.debug(f"æ¸…ç†äº† {len(expired)} ä¸ªè¿‡æœŸç¼“å­˜é¡¹")

    def _deduplicate_results(self, results: List[Dict]) -> List[Dict]:
        """å»é‡ç»“æœ"""
        seen = set()
        unique = []

        for result in results:
            result_id = result.get('id')
            if result_id and result_id not in seen:
                seen.add(result_id)
                unique.append(result)

        return unique

    def _rank_results(self, results: List[Dict], query: str) -> List[Dict]:
        """å¯¹ç»“æœæ’åº"""
        # ç®€å•çš„ç›¸å…³æ€§è¯„åˆ†
        for result in results:
            score = 0

            # æ—¶é—´å› ç´ 
            if 'timestamp' in result:
                age_days = (datetime.now() - datetime.fromisoformat(result['timestamp'])).days
                score -= age_days * 0.1

            # æŸ¥è¯¢åŒ¹é…åº¦
            if query.lower() in str(result).lower():
                score += 10

            # é‡è¦æ€§
            if result.get('metadata', {}).get('importance') == 'high':
                score += 5

            result['_score'] = score

        # æŒ‰åˆ†æ•°æ’åº
        return sorted(results, key=lambda x: x.get('_score', 0), reverse=True)

    def _get_team_id(self) -> Optional[str]:
        """è·å–å›¢é˜ŸID"""
        # ä»é…ç½®æˆ–ç¯å¢ƒå˜é‡è·å–
        return os.environ.get('MCP_TEAM_ID')

    def _get_current_user(self) -> str:
        """è·å–å½“å‰ç”¨æˆ·"""
        return os.environ.get('USER', 'unknown')

    def get_stats(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨ç»Ÿè®¡"""
        local_stats = self.local_storage.get_stats()
        central_stats = self.central_storage.get_stats(self.project_id)

        return {
            'storage_mode': 'hybrid',
            'local': local_stats,
            'central': central_stats,
            'cache': {
                'size': len(self.cache),
                'hit_rate': self._calculate_cache_hit_rate()
            },
            'sync': {
                'enabled': self.sync_enabled,
                'last_sync': self.last_sync.isoformat() if self.last_sync else None,
                'pending': local_stats.get('unsynced_count', 0)
            }
        }

    def _calculate_cache_hit_rate(self) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è®°å½•å‘½ä¸­æ¬¡æ•°
        return 0.75  # ç¤ºä¾‹å€¼

# ============================================
# æœ¬åœ°SQLiteå­˜å‚¨
# ============================================

class LocalSQLiteStorage:
    """æœ¬åœ°SQLiteå­˜å‚¨å®ç°"""

    def __init__(self, project_path: str):
        self.project_path = project_path
        self.db_path = Path(project_path) / ".mcp_memory" / "local.db"
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.conn = self._init_db()

    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(str(self.db_path))
        conn.row_factory = sqlite3.Row

        conn.execute("""
            CREATE TABLE IF NOT EXISTS snapshots (
                id TEXT PRIMARY KEY,
                timestamp REAL,
                data TEXT,
                metadata TEXT,
                hash TEXT,
                synced INTEGER DEFAULT 0,
                sync_time REAL
            )
        """)

        conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_timestamp
            ON snapshots(timestamp)
        """)

        conn.execute("""
            CREATE INDEX IF NOT EXISTS idx_synced
            ON snapshots(synced)
        """)

        conn.commit()
        return conn

    def save(self, snapshot_id: str, data: Dict):
        """ä¿å­˜åˆ°æœ¬åœ°"""
        self.conn.execute("""
            INSERT OR REPLACE INTO snapshots
            (id, timestamp, data, metadata, hash, synced)
            VALUES (?, ?, ?, ?, ?, ?)
        """, (
            snapshot_id,
            datetime.now().timestamp(),
            json.dumps(data),
            json.dumps(data.get('metadata', {})),
            data.get('hash', ''),
            0
        ))
        self.conn.commit()

    def load(self, snapshot_id: str) -> Optional[Dict]:
        """åŠ è½½å¿«ç…§"""
        cursor = self.conn.execute(
            "SELECT data FROM snapshots WHERE id = ?",
            (snapshot_id,)
        )
        row = cursor.fetchone()

        if row:
            return json.loads(row['data'])
        return None

    def search(self, query: str, limit: int = 20) -> List[Dict]:
        """æœç´¢å¿«ç…§"""
        cursor = self.conn.execute("""
            SELECT id, timestamp, data
            FROM snapshots
            WHERE data LIKE ?
            ORDER BY timestamp DESC
            LIMIT ?
        """, (f'%{query}%', limit))

        results = []
        for row in cursor:
            data = json.loads(row['data'])
            data['id'] = row['id']
            data['timestamp'] = datetime.fromtimestamp(row['timestamp']).isoformat()
            results.append(data)

        return results

    def get_unsynced(self) -> List[Dict]:
        """è·å–æœªåŒæ­¥çš„å¿«ç…§"""
        cursor = self.conn.execute("""
            SELECT data FROM snapshots
            WHERE synced = 0
            ORDER BY timestamp
            LIMIT 100
        """)

        return [json.loads(row['data']) for row in cursor]

    def mark_synced(self, snapshot_id: str):
        """æ ‡è®°ä¸ºå·²åŒæ­¥"""
        self.conn.execute("""
            UPDATE snapshots
            SET synced = 1, sync_time = ?
            WHERE id = ?
        """, (datetime.now().timestamp(), snapshot_id))
        self.conn.commit()

    def delete_before(self, cutoff: datetime) -> int:
        """åˆ é™¤æŒ‡å®šæ—¥æœŸå‰çš„å¿«ç…§"""
        cursor = self.conn.execute("""
            DELETE FROM snapshots
            WHERE timestamp < ? AND synced = 1
        """, (cutoff.timestamp(),))

        self.conn.commit()
        return cursor.rowcount

    def analyze_patterns(self) -> Dict:
        """åˆ†ææœ¬åœ°æ¨¡å¼"""
        cursor = self.conn.execute("""
            SELECT COUNT(*) as count,
                   AVG(LENGTH(data)) as avg_size,
                   MAX(timestamp) as latest,
                   MIN(timestamp) as earliest
            FROM snapshots
        """)

        row = cursor.fetchone()

        return {
            'total_snapshots': row['count'],
            'avg_size': row['avg_size'],
            'time_span': row['latest'] - row['earliest'] if row['latest'] else 0
        }

    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        cursor = self.conn.execute("""
            SELECT COUNT(*) as total,
                   SUM(CASE WHEN synced = 0 THEN 1 ELSE 0 END) as unsynced,
                   SUM(LENGTH(data)) as total_size
            FROM snapshots
        """)

        row = cursor.fetchone()

        return {
            'total_count': row['total'],
            'unsynced_count': row['unsynced'],
            'total_size': row['total_size'],
            'db_size': os.path.getsize(self.db_path) if self.db_path.exists() else 0
        }

# ============================================
# ä¸­å¤®MySQLå­˜å‚¨
# ============================================

class CentralMySQLStorage:
    """ä¸­å¤®MySQLå­˜å‚¨å®ç°"""

    def __init__(self):
        self.engine = self._create_engine()
        self.Session = sessionmaker(bind=self.engine)
        self._init_tables()

    def _create_engine(self):
        """åˆ›å»ºæ•°æ®åº“å¼•æ“"""
        settings = get_settings()

        db_url = f"mysql+pymysql://{settings.DB_USER}:{settings.DB_PASSWORD}@{settings.DB_HOST}:{settings.DB_PORT}/{settings.DB_NAME}"

        return create_engine(
            db_url,
            poolclass=QueuePool,
            pool_size=10,
            max_overflow=20,
            pool_recycle=3600,
            echo=False
        )

    def _init_tables(self):
        """åˆå§‹åŒ–è¡¨"""
        Base.metadata.create_all(self.engine)

    def save(self, data: Dict):
        """ä¿å­˜åˆ°MySQL"""
        session = self.Session()
        try:
            snapshot = MemorySnapshot(
                id=data['id'],
                project_id=data['project_id'],
                project_path=data['project_path'],
                timestamp=datetime.fromisoformat(data['timestamp']),
                node_count=data.get('node_count', 0),
                edge_count=data.get('edge_count', 0),
                graph_data=json.dumps(data.get('data', {})),
                meta_data=json.dumps(data.get('metadata', {})),
                hash=data.get('hash', ''),
                created_by=os.environ.get('USER', 'unknown')
            )

            session.merge(snapshot)  # ä½¿ç”¨mergeé¿å…é‡å¤
            session.commit()

        finally:
            session.close()

    async def load(self, snapshot_id: str) -> Optional[Dict]:
        """ä»MySQLåŠ è½½"""
        session = self.Session()
        try:
            snapshot = session.query(MemorySnapshot).filter_by(id=snapshot_id).first()

            if snapshot:
                return {
                    'id': snapshot.id,
                    'project_id': snapshot.project_id,
                    'project_path': snapshot.project_path,
                    'timestamp': snapshot.timestamp.isoformat(),
                    'data': json.loads(snapshot.graph_data),
                    'metadata': json.loads(snapshot.meta_data),
                    'hash': snapshot.hash
                }

        finally:
            session.close()

        return None

    async def search(self, query: str, project_id: str, limit: int = 20) -> List[Dict]:
        """æœç´¢å¿«ç…§"""
        session = self.Session()
        try:
            snapshots = session.query(MemorySnapshot).filter(
                MemorySnapshot.project_id == project_id,
                MemorySnapshot.graph_data.contains(query)
            ).order_by(
                MemorySnapshot.timestamp.desc()
            ).limit(limit).all()

            results = []
            for snapshot in snapshots:
                results.append({
                    'id': snapshot.id,
                    'timestamp': snapshot.timestamp.isoformat(),
                    'data': json.loads(snapshot.graph_data),
                    'metadata': json.loads(snapshot.meta_data)
                })

            return results

        finally:
            session.close()

    async def search_team(self, query: str, team_id: str, limit: int = 20) -> List[Dict]:
        """æœç´¢å›¢é˜Ÿé¡¹ç›®"""
        session = self.Session()
        try:
            snapshots = session.query(MemorySnapshot).filter(
                MemorySnapshot.team_id == team_id,
                MemorySnapshot.is_public == True,
                MemorySnapshot.graph_data.contains(query)
            ).order_by(
                MemorySnapshot.timestamp.desc()
            ).limit(limit).all()

            results = []
            for snapshot in snapshots:
                results.append({
                    'id': snapshot.id,
                    'project_id': snapshot.project_id,
                    'timestamp': snapshot.timestamp.isoformat(),
                    'data': json.loads(snapshot.graph_data)
                })

            return results

        finally:
            session.close()

    async def save_insight(self, insight: Dict):
        """ä¿å­˜æ´å¯Ÿ"""
        session = self.Session()
        try:
            shared_insight = SharedInsight(
                project_id=insight['project_id'],
                insight_type='user_generated',
                content=insight['content'],
                created_by=insight['created_by'],
                tags=json.dumps(insight.get('tags', []))
            )

            session.add(shared_insight)
            session.commit()

            return shared_insight.id

        finally:
            session.close()

    async def get_team_insights(self, team_id: str, limit: int = 20) -> List[Dict]:
        """è·å–å›¢é˜Ÿæ´å¯Ÿ"""
        session = self.Session()
        try:
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥é€šè¿‡team_idå…³è”
            insights = session.query(SharedInsight).order_by(
                SharedInsight.upvotes.desc(),
                SharedInsight.created_at.desc()
            ).limit(limit).all()

            results = []
            for insight in insights:
                results.append({
                    'id': insight.id,
                    'content': insight.content,
                    'created_by': insight.created_by,
                    'created_at': insight.created_at.isoformat(),
                    'upvotes': insight.upvotes,
                    'tags': json.loads(insight.tags) if insight.tags else []
                })

            return results

        finally:
            session.close()

    async def get_global_patterns(self) -> Dict:
        """è·å–å…¨å±€æ¨¡å¼"""
        session = self.Session()
        try:
            patterns = session.query(MemoryPattern).filter(
                MemoryPattern.confidence > 0.7
            ).order_by(
                MemoryPattern.frequency.desc()
            ).limit(10).all()

            return {
                'top_patterns': [
                    {
                        'type': p.pattern_type,
                        'frequency': p.frequency,
                        'confidence': p.confidence
                    }
                    for p in patterns
                ]
            }

        finally:
            session.close()

    async def get_team_patterns(self, team_id: str) -> Dict:
        """è·å–å›¢é˜Ÿæ¨¡å¼"""
        # ç®€åŒ–å®ç°
        return await self.get_global_patterns()

    async def delete_before(self, cutoff: datetime) -> int:
        """åˆ é™¤æ—§æ•°æ®"""
        session = self.Session()
        try:
            deleted = session.query(MemorySnapshot).filter(
                MemorySnapshot.timestamp < cutoff
            ).delete()

            session.commit()
            return deleted

        finally:
            session.close()

    def get_stats(self, project_id: str) -> Dict:
        """è·å–ç»Ÿè®¡"""
        session = self.Session()
        try:
            count = session.query(MemorySnapshot).filter_by(
                project_id=project_id
            ).count()

            latest = session.query(MemorySnapshot).filter_by(
                project_id=project_id
            ).order_by(
                MemorySnapshot.timestamp.desc()
            ).first()

            return {
                'total_count': count,
                'latest_snapshot': latest.timestamp.isoformat() if latest else None
            }

        finally:
            session.close()

# ============================================
# å·¥å‚å‡½æ•°
# ============================================

def create_storage(project_path: str, mode: str = 'auto') -> HybridStorageManager:
    """åˆ›å»ºå­˜å‚¨å®ä¾‹"""

    if mode == 'auto':
        # è‡ªåŠ¨é€‰æ‹©æ¨¡å¼
        if os.environ.get('MCP_TEAM_MODE') == 'true':
            mode = 'hybrid'
        else:
            mode = 'local'

    if mode == 'hybrid':
        logger.info("ä½¿ç”¨æ··åˆå­˜å‚¨æ¨¡å¼ (SQLite + MySQL)")
        return HybridStorageManager(project_path)
    elif mode == 'local':
        logger.info("ä½¿ç”¨çº¯æœ¬åœ°å­˜å‚¨æ¨¡å¼ (SQLite)")
        manager = HybridStorageManager(project_path)
        manager.sync_enabled = False
        return manager
    elif mode == 'central':
        logger.info("ä½¿ç”¨çº¯ä¸­å¤®å­˜å‚¨æ¨¡å¼ (MySQL)")
        manager = HybridStorageManager(project_path)
        # é…ç½®ä¸ºä¸»è¦ä½¿ç”¨MySQL
        return manager
    else:
        raise ValueError(f"æœªçŸ¥çš„å­˜å‚¨æ¨¡å¼: {mode}")

# ============================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================

async def demo():
    """æ¼”ç¤ºæ··åˆå­˜å‚¨ç³»ç»Ÿ"""
    print("=" * 60)
    print("ğŸ”„ æ··åˆå­˜å‚¨ç³»ç»Ÿæ¼”ç¤º (SQLite + MySQL)")
    print("=" * 60)

    # åˆ›å»ºæ··åˆå­˜å‚¨
    storage = create_storage("/Users/mac/Downloads/MCP", mode='hybrid')

    # ä¿å­˜æ•°æ®
    data = {
        'nodes': [{'id': '1', 'name': 'test'}],
        'edges': [],
        'metadata': {'version': '1.0'}
    }

    snapshot_id = await storage.save(data, {
        'trigger': 'demo',
        'importance': 'high',
        'team_mode': True
    })

    print(f"\nâœ… æ•°æ®å·²ä¿å­˜ (ID: {snapshot_id})")
    print("   - æœ¬åœ°SQLite: ç«‹å³ä¿å­˜")
    print("   - MySQL: å¼‚æ­¥åŒæ­¥ä¸­...")

    # ç­‰å¾…å¼‚æ­¥åŒæ­¥
    await asyncio.sleep(1)

    # åŠ è½½æ•°æ®
    loaded = await storage.load(snapshot_id)
    print(f"\nâœ… æ•°æ®å·²åŠ è½½: {loaded is not None}")

    # æœç´¢
    results = await storage.search("test")
    print(f"\nğŸ” æœç´¢ç»“æœ: {len(results)} æ¡")

    # è·å–ç»Ÿè®¡
    stats = storage.get_stats()
    print(f"\nğŸ“Š å­˜å‚¨ç»Ÿè®¡:")
    print(f"   - å­˜å‚¨æ¨¡å¼: {stats['storage_mode']}")
    print(f"   - æœ¬åœ°å¿«ç…§: {stats['local']['total_count']}")
    print(f"   - ä¸­å¤®å¿«ç…§: {stats['central']['total_count']}")
    print(f"   - ç¼“å­˜å¤§å°: {stats['cache']['size']}")
    print(f"   - åŒæ­¥çŠ¶æ€: {'å¯ç”¨' if stats['sync']['enabled'] else 'ç¦ç”¨'}")

    # åˆ†ææ¨¡å¼
    patterns = await storage.analyze_patterns()
    print(f"\nğŸ”® æ¨¡å¼åˆ†æ:")
    if patterns['recommendations']:
        for rec in patterns['recommendations']:
            print(f"   â€¢ {rec}")

    print("\n" + "=" * 60)
    print("âœ¨ æ··åˆå­˜å‚¨ä¼˜åŠ¿:")
    print("   1. æœ¬åœ°SQLiteæä¾›æ¯«ç§’çº§å“åº”")
    print("   2. MySQLæä¾›å›¢é˜Ÿå…±äº«å’Œæ°¸ä¹…å­˜å‚¨")
    print("   3. è‡ªåŠ¨åŒæ­¥ç¡®ä¿æ•°æ®ä¸€è‡´æ€§")
    print("   4. æ”¯æŒç¦»çº¿å·¥ä½œï¼Œåœ¨çº¿åŒæ­¥")
    print("   5. æ™ºèƒ½ç¼“å­˜å‡å°‘æ•°æ®åº“è®¿é—®")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(demo())