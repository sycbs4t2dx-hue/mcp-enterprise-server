"""
é¡¹ç›®è®°å¿†ç³»ç»Ÿ - è‡ªåŠ¨ä¿å­˜å’Œæ¢å¤é¡¹ç›®çŸ¥è¯†
é€šè¿‡å›¾è°±å¿«ç…§å®ç°é¡¹ç›®çš„"æ—¶å…‰æœº"åŠŸèƒ½
"""

import os
import json
import hashlib
import asyncio
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from pathlib import Path
from dataclasses import dataclass, field
import pickle
import sqlite3
import difflib

from src.mcp_core.services.project_graph_generator import (
    ProjectAnalyzer, GraphGenerator, GraphData
)
from src.mcp_core.common.logger import get_logger

logger = get_logger(__name__)

# ============================================
# æ•°æ®ç»“æ„
# ============================================

@dataclass
class MemorySnapshot:
    """è®°å¿†å¿«ç…§"""
    id: str
    timestamp: datetime
    graph_data: GraphData
    metadata: Dict[str, Any]
    context: Dict[str, Any] = field(default_factory=dict)
    insights: List[str] = field(default_factory=list)
    hash: str = ""

    def __post_init__(self):
        if not self.hash:
            self.hash = self.calculate_hash()

    def calculate_hash(self) -> str:
        """è®¡ç®—å›¾è°±å“ˆå¸Œå€¼"""
        data = f"{len(self.graph_data.nodes)}:{len(self.graph_data.edges)}"
        return hashlib.md5(data.encode()).hexdigest()

@dataclass
class MemoryQuery:
    """è®°å¿†æŸ¥è¯¢"""
    query_type: str  # 'similarity', 'time_range', 'pattern', 'file_history'
    parameters: Dict[str, Any]
    context: Optional[Dict[str, Any]] = None

@dataclass
class MemoryRecoveryResult:
    """è®°å¿†æ¢å¤ç»“æœ"""
    success: bool
    snapshots: List[MemorySnapshot]
    insights: List[str]
    suggestions: List[str]
    confidence: float = 0.0

# ============================================
# è®°å¿†å­˜å‚¨
# ============================================

class MemoryStorage:
    """è®°å¿†å­˜å‚¨ç³»ç»Ÿ"""

    def __init__(self, storage_path: str = "./project_memory"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(exist_ok=True)
        self.db_path = self.storage_path / "memory.db"
        self.init_database()

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS memory_snapshots (
                id TEXT PRIMARY KEY,
                timestamp REAL,
                project_path TEXT,
                hash TEXT,
                node_count INTEGER,
                edge_count INTEGER,
                metadata TEXT,
                context TEXT,
                insights TEXT,
                file_path TEXT
            )
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_timestamp ON memory_snapshots(timestamp)
        """)

        cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_hash ON memory_snapshots(hash)
        """)

        conn.commit()
        conn.close()

    async def save_snapshot(self, snapshot: MemorySnapshot, project_path: str) -> bool:
        """ä¿å­˜è®°å¿†å¿«ç…§"""
        try:
            # ä¿å­˜å›¾è°±æ•°æ®åˆ°æ–‡ä»¶
            file_name = f"snapshot_{snapshot.id}.pkl"
            file_path = self.storage_path / file_name

            with open(file_path, 'wb') as f:
                pickle.dump(snapshot, f)

            # ä¿å­˜å…ƒæ•°æ®åˆ°æ•°æ®åº“
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            cursor.execute("""
                INSERT INTO memory_snapshots
                (id, timestamp, project_path, hash, node_count, edge_count,
                 metadata, context, insights, file_path)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                snapshot.id,
                snapshot.timestamp.timestamp(),
                project_path,
                snapshot.hash,
                len(snapshot.graph_data.nodes),
                len(snapshot.graph_data.edges),
                json.dumps(snapshot.metadata),
                json.dumps(snapshot.context),
                json.dumps(snapshot.insights),
                str(file_path)
            ))

            conn.commit()
            conn.close()

            logger.info(f"ä¿å­˜è®°å¿†å¿«ç…§: {snapshot.id}")
            return True

        except Exception as e:
            logger.error(f"ä¿å­˜å¿«ç…§å¤±è´¥: {e}")
            return False

    async def load_snapshot(self, snapshot_id: str) -> Optional[MemorySnapshot]:
        """åŠ è½½è®°å¿†å¿«ç…§"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute(
            "SELECT file_path FROM memory_snapshots WHERE id = ?",
            (snapshot_id,)
        )
        result = cursor.fetchone()
        conn.close()

        if result:
            file_path = result[0]
            if os.path.exists(file_path):
                with open(file_path, 'rb') as f:
                    return pickle.load(f)

        return None

    async def search_snapshots(
        self,
        project_path: str,
        time_range: Optional[tuple] = None,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """æœç´¢å¿«ç…§"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        query = "SELECT * FROM memory_snapshots WHERE project_path = ?"
        params = [project_path]

        if time_range:
            query += " AND timestamp BETWEEN ? AND ?"
            params.extend([time_range[0].timestamp(), time_range[1].timestamp()])

        query += " ORDER BY timestamp DESC LIMIT ?"
        params.append(limit)

        cursor.execute(query, params)
        columns = [column[0] for column in cursor.description]
        results = []

        for row in cursor.fetchall():
            results.append(dict(zip(columns, row)))

        conn.close()
        return results

# ============================================
# è®°å¿†åˆ†æå™¨
# ============================================

class MemoryAnalyzer:
    """è®°å¿†åˆ†æå™¨"""

    def __init__(self):
        self.patterns = {}

    def calculate_similarity(
        self,
        graph1: GraphData,
        graph2: GraphData
    ) -> float:
        """è®¡ç®—ä¸¤ä¸ªå›¾è°±çš„ç›¸ä¼¼åº¦"""
        # 1. èŠ‚ç‚¹ç›¸ä¼¼åº¦
        nodes1 = {n.path for n in graph1.nodes}
        nodes2 = {n.path for n in graph2.nodes}
        node_similarity = len(nodes1 & nodes2) / max(len(nodes1), len(nodes2))

        # 2. è¾¹ç›¸ä¼¼åº¦
        edges1 = {(e.source, e.target) for e in graph1.edges}
        edges2 = {(e.source, e.target) for e in graph2.edges}
        edge_similarity = len(edges1 & edges2) / max(len(edges1), len(edges2), 1)

        # 3. ç»“æ„ç›¸ä¼¼åº¦
        struct_sim = self.calculate_structural_similarity(graph1, graph2)

        # åŠ æƒå¹³å‡
        return 0.4 * node_similarity + 0.3 * edge_similarity + 0.3 * struct_sim

    def calculate_structural_similarity(
        self,
        graph1: GraphData,
        graph2: GraphData
    ) -> float:
        """è®¡ç®—ç»“æ„ç›¸ä¼¼åº¦"""
        # ç®€åŒ–ç‰ˆï¼šæ¯”è¾ƒèŠ‚ç‚¹åº¦åˆ†å¸ƒ
        degree1 = self.get_degree_distribution(graph1)
        degree2 = self.get_degree_distribution(graph2)

        if not degree1 or not degree2:
            return 0.0

        # è®¡ç®—åˆ†å¸ƒç›¸ä¼¼åº¦
        all_degrees = set(degree1.keys()) | set(degree2.keys())
        similarity = 0
        for degree in all_degrees:
            d1 = degree1.get(degree, 0)
            d2 = degree2.get(degree, 0)
            similarity += 1 - abs(d1 - d2) / max(d1, d2, 1)

        return similarity / len(all_degrees)

    def get_degree_distribution(self, graph: GraphData) -> Dict[int, int]:
        """è·å–åº¦åˆ†å¸ƒ"""
        degrees = {}
        for node in graph.nodes:
            degree = sum(1 for e in graph.edges
                        if e.source == node.id or e.target == node.id)
            degrees[degree] = degrees.get(degree, 0) + 1
        return degrees

    def find_changes(
        self,
        old_graph: GraphData,
        new_graph: GraphData
    ) -> Dict[str, List]:
        """æ‰¾å‡ºä¸¤ä¸ªå›¾è°±ä¹‹é—´çš„å˜åŒ–"""
        old_nodes = {n.path: n for n in old_graph.nodes}
        new_nodes = {n.path: n for n in new_graph.nodes}

        added_nodes = set(new_nodes.keys()) - set(old_nodes.keys())
        removed_nodes = set(old_nodes.keys()) - set(new_nodes.keys())

        modified_nodes = []
        for path in set(old_nodes.keys()) & set(new_nodes.keys()):
            if old_nodes[path].size != new_nodes[path].size:
                modified_nodes.append(path)

        return {
            "added": list(added_nodes),
            "removed": list(removed_nodes),
            "modified": modified_nodes
        }

    def generate_insights(
        self,
        current_graph: GraphData,
        historical_graphs: List[GraphData]
    ) -> List[str]:
        """ç”Ÿæˆæ´å¯Ÿ"""
        insights = []

        # 1. å¢é•¿è¶‹åŠ¿
        if historical_graphs:
            old_size = len(historical_graphs[0].nodes)
            new_size = len(current_graph.nodes)
            growth = ((new_size - old_size) / old_size) * 100
            insights.append(f"é¡¹ç›®è§„æ¨¡å¢é•¿: {growth:.1f}%")

        # 2. å¤æ‚åº¦åˆ†æ
        avg_complexity = sum(n.complexity for n in current_graph.nodes) / len(current_graph.nodes)
        insights.append(f"å¹³å‡å¤æ‚åº¦: {avg_complexity:.1f}")

        # 3. ä¾èµ–å¯†åº¦
        density = len(current_graph.edges) / (len(current_graph.nodes) * (len(current_graph.nodes) - 1))
        insights.append(f"ä¾èµ–å¯†åº¦: {density:.3f}")

        return insights

# ============================================
# é¡¹ç›®è®°å¿†ç³»ç»Ÿ
# ============================================

class ProjectMemorySystem:
    """é¡¹ç›®è®°å¿†ç³»ç»Ÿ - ä¸»ç±»"""

    def __init__(self, storage_path: str = "./project_memory"):
        self.storage = MemoryStorage(storage_path)
        self.analyzer = MemoryAnalyzer()
        self.graph_analyzer = ProjectAnalyzer()
        self.graph_generator = GraphGenerator()
        self.auto_snapshot_enabled = False
        self.snapshot_interval = 3600  # 1å°æ—¶

    async def create_snapshot(
        self,
        project_path: str,
        trigger: str = "manual",
        context: Optional[Dict] = None
    ) -> MemorySnapshot:
        """åˆ›å»ºè®°å¿†å¿«ç…§"""
        logger.info(f"åˆ›å»ºè®°å¿†å¿«ç…§: {project_path}")

        # ç”Ÿæˆå›¾è°±
        graph_data = await self.graph_analyzer.analyze_project(project_path)

        # ç”ŸæˆID
        snapshot_id = datetime.now().strftime("%Y%m%d_%H%M%S")

        # æ”¶é›†ä¸Šä¸‹æ–‡
        if not context:
            context = await self.collect_context(project_path)

        # ç”Ÿæˆæ´å¯Ÿ
        historical = await self.get_recent_snapshots(project_path, limit=5)
        historical_graphs = [s.graph_data for s in historical]
        insights = self.analyzer.generate_insights(graph_data, historical_graphs)

        # åˆ›å»ºå¿«ç…§
        snapshot = MemorySnapshot(
            id=snapshot_id,
            timestamp=datetime.now(),
            graph_data=graph_data,
            metadata={
                "project_path": project_path,
                "trigger": trigger,
                "node_count": len(graph_data.nodes),
                "edge_count": len(graph_data.edges),
            },
            context=context,
            insights=insights
        )

        # ä¿å­˜å¿«ç…§
        await self.storage.save_snapshot(snapshot, project_path)

        return snapshot

    async def recover_memory(
        self,
        query: MemoryQuery,
        project_path: str
    ) -> MemoryRecoveryResult:
        """æ¢å¤è®°å¿†"""
        logger.info(f"æ¢å¤è®°å¿†: {query.query_type}")

        if query.query_type == "similarity":
            # åŸºäºç›¸ä¼¼åº¦æ¢å¤
            return await self.recover_by_similarity(query, project_path)

        elif query.query_type == "time_range":
            # åŸºäºæ—¶é—´èŒƒå›´æ¢å¤
            return await self.recover_by_time(query, project_path)

        elif query.query_type == "file_history":
            # æ¢å¤æ–‡ä»¶å†å²
            return await self.recover_file_history(query, project_path)

        elif query.query_type == "pattern":
            # åŸºäºæ¨¡å¼æ¢å¤
            return await self.recover_by_pattern(query, project_path)

        else:
            return MemoryRecoveryResult(
                success=False,
                snapshots=[],
                insights=["æœªçŸ¥çš„æŸ¥è¯¢ç±»å‹"],
                suggestions=[]
            )

    async def recover_by_similarity(
        self,
        query: MemoryQuery,
        project_path: str
    ) -> MemoryRecoveryResult:
        """åŸºäºç›¸ä¼¼åº¦æ¢å¤è®°å¿†"""
        current_graph = query.parameters.get("current_graph")
        if not current_graph:
            # ç”Ÿæˆå½“å‰å›¾è°±
            current_graph = await self.graph_analyzer.analyze_project(project_path)

        # è·å–å†å²å¿«ç…§
        snapshots = await self.get_all_snapshots(project_path)

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for snapshot in snapshots:
            sim = self.analyzer.calculate_similarity(
                current_graph,
                snapshot.graph_data
            )
            similarities.append((snapshot, sim))

        # æ’åºå¹¶è¿”å›æœ€ç›¸ä¼¼çš„
        similarities.sort(key=lambda x: x[1], reverse=True)
        similar_snapshots = [s[0] for s in similarities[:5]]

        # ç”Ÿæˆæ´å¯Ÿ
        insights = []
        if similar_snapshots:
            insights.append(
                f"æ‰¾åˆ° {len(similar_snapshots)} ä¸ªç›¸ä¼¼çš„å†å²çŠ¶æ€"
            )
            insights.append(
                f"æœ€ç›¸ä¼¼çš„æ˜¯ {similar_snapshots[0].timestamp.strftime('%Y-%m-%d %H:%M')}"
            )

        return MemoryRecoveryResult(
            success=True,
            snapshots=similar_snapshots,
            insights=insights,
            suggestions=self.generate_suggestions(similar_snapshots),
            confidence=similarities[0][1] if similarities else 0.0
        )

    async def recover_file_history(
        self,
        query: MemoryQuery,
        project_path: str
    ) -> MemoryRecoveryResult:
        """æ¢å¤æ–‡ä»¶å†å²"""
        file_path = query.parameters.get("file_path")
        if not file_path:
            return MemoryRecoveryResult(
                success=False,
                snapshots=[],
                insights=["éœ€è¦æä¾›æ–‡ä»¶è·¯å¾„"],
                suggestions=[]
            )

        # è·å–æ‰€æœ‰åŒ…å«è¯¥æ–‡ä»¶çš„å¿«ç…§
        all_snapshots = await self.get_all_snapshots(project_path)
        file_snapshots = []

        for snapshot in all_snapshots:
            for node in snapshot.graph_data.nodes:
                if node.path == file_path:
                    file_snapshots.append((snapshot, node))
                    break

        # åˆ†ææ–‡ä»¶æ¼”å˜
        insights = []
        if file_snapshots:
            insights.append(f"æ–‡ä»¶å‡ºç°åœ¨ {len(file_snapshots)} ä¸ªå†å²å¿«ç…§ä¸­")

            # åˆ†æå¤§å°å˜åŒ–
            sizes = [node.size for _, node in file_snapshots]
            if len(sizes) > 1:
                size_change = sizes[-1] - sizes[0]
                insights.append(f"æ–‡ä»¶å¤§å°å˜åŒ–: {size_change} å­—èŠ‚")

            # åˆ†æå¤æ‚åº¦å˜åŒ–
            complexities = [node.complexity for _, node in file_snapshots]
            if len(complexities) > 1:
                complexity_change = complexities[-1] - complexities[0]
                insights.append(f"å¤æ‚åº¦å˜åŒ–: {complexity_change}")

        return MemoryRecoveryResult(
            success=True,
            snapshots=[s for s, _ in file_snapshots],
            insights=insights,
            suggestions=["è€ƒè™‘é‡æ„" if len(file_snapshots) > 10 else "ä¿æŒç°çŠ¶"],
            confidence=1.0 if file_snapshots else 0.0
        )

    async def get_recent_snapshots(
        self,
        project_path: str,
        limit: int = 10
    ) -> List[MemorySnapshot]:
        """è·å–æœ€è¿‘çš„å¿«ç…§"""
        results = await self.storage.search_snapshots(project_path, limit=limit)
        snapshots = []

        for result in results:
            snapshot = await self.storage.load_snapshot(result['id'])
            if snapshot:
                snapshots.append(snapshot)

        return snapshots

    async def get_all_snapshots(self, project_path: str) -> List[MemorySnapshot]:
        """è·å–æ‰€æœ‰å¿«ç…§"""
        return await self.get_recent_snapshots(project_path, limit=1000)

    async def collect_context(self, project_path: str) -> Dict[str, Any]:
        """æ”¶é›†ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context = {
            "timestamp": datetime.now().isoformat(),
            "project_path": project_path,
        }

        # å°è¯•è·å–Gitä¿¡æ¯
        try:
            import subprocess
            result = subprocess.run(
                ["git", "rev-parse", "HEAD"],
                cwd=project_path,
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                context["git_commit"] = result.stdout.strip()

            result = subprocess.run(
                ["git", "branch", "--show-current"],
                cwd=project_path,
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                context["git_branch"] = result.stdout.strip()
        except Exception:
            pass

        return context

    def generate_suggestions(
        self,
        snapshots: List[MemorySnapshot]
    ) -> List[str]:
        """ç”Ÿæˆå»ºè®®"""
        suggestions = []

        if not snapshots:
            return ["åˆ›å»ºåˆå§‹å¿«ç…§ä»¥å¼€å§‹è®°å½•é¡¹ç›®å†å²"]

        # åŸºäºå†å²å¿«ç…§ç”Ÿæˆå»ºè®®
        latest = snapshots[0]

        if len(latest.graph_data.nodes) > 100:
            suggestions.append("é¡¹ç›®è§„æ¨¡è¾ƒå¤§ï¼Œè€ƒè™‘æ¨¡å—åŒ–é‡æ„")

        if len(latest.graph_data.edges) > len(latest.graph_data.nodes) * 2:
            suggestions.append("ä¾èµ–å…³ç³»å¤æ‚ï¼Œè€ƒè™‘è§£è€¦")

        if latest.insights:
            suggestions.extend([
                f"åŸºäºæ´å¯Ÿ: {insight}"
                for insight in latest.insights[:2]
            ])

        return suggestions

    async def enable_auto_snapshot(
        self,
        project_path: str,
        interval: int = 3600
    ):
        """å¯ç”¨è‡ªåŠ¨å¿«ç…§"""
        self.auto_snapshot_enabled = True
        self.snapshot_interval = interval

        logger.info(f"å¯ç”¨è‡ªåŠ¨å¿«ç…§: æ¯ {interval} ç§’")

        while self.auto_snapshot_enabled:
            await asyncio.sleep(interval)
            await self.create_snapshot(project_path, trigger="auto")

    def disable_auto_snapshot(self):
        """ç¦ç”¨è‡ªåŠ¨å¿«ç…§"""
        self.auto_snapshot_enabled = False
        logger.info("ç¦ç”¨è‡ªåŠ¨å¿«ç…§")

# ============================================
# è®°å¿†æ¢å¤åŠ©æ‰‹
# ============================================

class MemoryRecoveryAssistant:
    """è®°å¿†æ¢å¤åŠ©æ‰‹ - æä¾›é«˜çº§æŸ¥è¯¢åŠŸèƒ½"""

    def __init__(self, memory_system: ProjectMemorySystem):
        self.memory_system = memory_system

    async def find_when_file_added(
        self,
        project_path: str,
        file_path: str
    ) -> Optional[datetime]:
        """æŸ¥æ‰¾æ–‡ä»¶ä½•æ—¶æ·»åŠ """
        snapshots = await self.memory_system.get_all_snapshots(project_path)

        for snapshot in reversed(snapshots):  # ä»æ—§åˆ°æ–°
            for node in snapshot.graph_data.nodes:
                if node.path == file_path:
                    return snapshot.timestamp

        return None

    async def find_when_dependency_added(
        self,
        project_path: str,
        source: str,
        target: str
    ) -> Optional[datetime]:
        """æŸ¥æ‰¾ä¾èµ–ä½•æ—¶æ·»åŠ """
        snapshots = await self.memory_system.get_all_snapshots(project_path)

        for snapshot in reversed(snapshots):
            for edge in snapshot.graph_data.edges:
                if edge.source == source and edge.target == target:
                    return snapshot.timestamp

        return None

    async def explain_growth(
        self,
        project_path: str,
        time_range: Optional[tuple] = None
    ) -> Dict[str, Any]:
        """è§£é‡Šé¡¹ç›®å¢é•¿"""
        snapshots = await self.memory_system.get_all_snapshots(project_path)

        if not snapshots:
            return {"error": "æ²¡æœ‰å†å²å¿«ç…§"}

        # è¿‡æ»¤æ—¶é—´èŒƒå›´
        if time_range:
            snapshots = [
                s for s in snapshots
                if time_range[0] <= s.timestamp <= time_range[1]
            ]

        if len(snapshots) < 2:
            return {"error": "å¿«ç…§ä¸è¶³ä»¥åˆ†æå¢é•¿"}

        oldest = snapshots[-1]
        newest = snapshots[0]

        # è®¡ç®—å¢é•¿æŒ‡æ ‡
        node_growth = len(newest.graph_data.nodes) - len(oldest.graph_data.nodes)
        edge_growth = len(newest.graph_data.edges) - len(oldest.graph_data.edges)

        # æ‰¾å‡ºæ–°å¢çš„ä¸»è¦æ–‡ä»¶
        old_paths = {n.path for n in oldest.graph_data.nodes}
        new_paths = {n.path for n in newest.graph_data.nodes}
        added_files = new_paths - old_paths

        return {
            "time_period": f"{oldest.timestamp} åˆ° {newest.timestamp}",
            "node_growth": node_growth,
            "edge_growth": edge_growth,
            "added_files": list(added_files)[:10],  # å‰10ä¸ª
            "growth_rate": f"{(node_growth / len(oldest.graph_data.nodes)) * 100:.1f}%"
        }

# ============================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================

async def demo():
    """æ¼”ç¤ºè®°å¿†ç³»ç»Ÿçš„ä½¿ç”¨"""
    print("=" * 60)
    print("ğŸ§  é¡¹ç›®è®°å¿†ç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)

    # åˆ›å»ºè®°å¿†ç³»ç»Ÿ
    memory_system = ProjectMemorySystem()

    # é¡¹ç›®è·¯å¾„
    project_path = "/Users/mac/Downloads/MCP"

    print("\n1. åˆ›å»ºè®°å¿†å¿«ç…§...")
    snapshot = await memory_system.create_snapshot(
        project_path,
        trigger="demo",
        context={"reason": "æ¼”ç¤ºè®°å¿†ç³»ç»Ÿ"}
    )
    print(f"   âœ… å¿«ç…§åˆ›å»ºæˆåŠŸ: {snapshot.id}")
    print(f"   - èŠ‚ç‚¹æ•°: {len(snapshot.graph_data.nodes)}")
    print(f"   - è¾¹æ•°: {len(snapshot.graph_data.edges)}")
    print(f"   - æ´å¯Ÿ: {snapshot.insights}")

    print("\n2. æ¢å¤ç›¸ä¼¼è®°å¿†...")
    query = MemoryQuery(
        query_type="similarity",
        parameters={"current_graph": snapshot.graph_data}
    )
    result = await memory_system.recover_memory(query, project_path)
    print(f"   âœ… æ‰¾åˆ° {len(result.snapshots)} ä¸ªç›¸ä¼¼å¿«ç…§")
    print(f"   - ç½®ä¿¡åº¦: {result.confidence:.2f}")
    print(f"   - æ´å¯Ÿ: {result.insights}")
    print(f"   - å»ºè®®: {result.suggestions}")

    print("\n3. æŸ¥è¯¢æ–‡ä»¶å†å²...")
    query = MemoryQuery(
        query_type="file_history",
        parameters={"file_path": "src/mcp_core/services/ai_model_manager.py"}
    )
    result = await memory_system.recover_memory(query, project_path)
    print(f"   âœ… æ–‡ä»¶å†å²æ¢å¤")
    print(f"   - å¿«ç…§æ•°: {len(result.snapshots)}")
    print(f"   - æ´å¯Ÿ: {result.insights}")

    print("\n4. ä½¿ç”¨æ¢å¤åŠ©æ‰‹...")
    assistant = MemoryRecoveryAssistant(memory_system)
    growth = await assistant.explain_growth(project_path)
    print(f"   âœ… é¡¹ç›®å¢é•¿åˆ†æ")
    for key, value in growth.items():
        if key != "added_files":
            print(f"   - {key}: {value}")

    print("\n" + "=" * 60)
    print("ğŸ’¡ è®°å¿†ç³»ç»Ÿå¯ä»¥:")
    print("   1. è‡ªåŠ¨ä¿å­˜é¡¹ç›®çŠ¶æ€å¿«ç…§")
    print("   2. åŸºäºç›¸ä¼¼åº¦æ¢å¤å†å²")
    print("   3. è¿½è¸ªæ–‡ä»¶æ¼”å˜å†ç¨‹")
    print("   4. åˆ†æé¡¹ç›®å¢é•¿è¶‹åŠ¿")
    print("   5. æä¾›æ™ºèƒ½å»ºè®®")
    print("=" * 60)

if __name__ == "__main__":
    asyncio.run(demo())