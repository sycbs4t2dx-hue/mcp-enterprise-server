"""
åµŒå…¥å‘é‡ç”ŸæˆæœåŠ¡
ä½¿ç”¨sentence-transformersç”Ÿæˆæ–‡æœ¬åµŒå…¥
æ”¯æŒä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹ï¼Œé¿å…é‡å¤ä¸‹è½½
"""

import os
from functools import lru_cache
from pathlib import Path
from typing import List, Optional, Union

import numpy as np
import torch
from sentence_transformers import SentenceTransformer

from ..common.config import get_settings
from ..common.logger import get_logger

logger = get_logger(__name__)


class EmbeddingService:
    """åµŒå…¥å‘é‡ç”ŸæˆæœåŠ¡"""

    def __init__(self, model_name: Optional[str] = None):
        """
        åˆå§‹åŒ–åµŒå…¥æœåŠ¡

        Args:
            model_name: æ¨¡å‹åç§°,ä¸ºNoneæ—¶ä½¿ç”¨é…ç½®æ–‡ä»¶
        """
        settings = get_settings()

        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
        self.model_path = self._resolve_model_path(model_name)

        # æ£€æµ‹è®¾å¤‡
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        logger.info(
            f"åŠ è½½åµŒå…¥æ¨¡å‹",
            extra={"model": self.model_path, "device": self.device},
        )

        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡
            self._setup_environment(settings)

            # åŠ è½½æ¨¡å‹ (æ”¯æŒæœ¬åœ°è·¯å¾„æˆ–HFä»“åº“åç§°)
            self.model = SentenceTransformer(self.model_path, device=self.device)

            # è·å–åµŒå…¥ç»´åº¦
            self.dimension = self.model.get_sentence_embedding_dimension()

            logger.info(
                f"åµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ",
                extra={"model": self.model_path, "dimension": self.dimension},
            )

        except Exception as e:
            logger.error(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def _resolve_model_path(self, model_name: Optional[str] = None) -> str:
        """
        è§£ææ¨¡å‹è·¯å¾„ (ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„)

        Args:
            model_name: æ¨¡å‹åç§°

        Returns:
            æ¨¡å‹è·¯å¾„ (æœ¬åœ°è·¯å¾„æˆ–HFä»“åº“åç§°)
        """
        settings = get_settings()

        # è·å–æ¨¡å‹é…ç½®
        local_path_str = None
        model_repo = None

        if hasattr(settings, 'models') and settings.models:
            models_config = settings.models
            embedding_config = getattr(models_config, 'embedding', None)

            if embedding_config:
                # embedding_config å¯èƒ½æ˜¯dictç±»å‹
                if isinstance(embedding_config, dict):
                    local_path_str = embedding_config.get('local_path')
                    model_repo = embedding_config.get('model_name')
                else:
                    local_path_str = embedding_config.local_path
                    model_repo = embedding_config.model_name

                # 1. ä¼˜å…ˆä½¿ç”¨æœ¬åœ°è·¯å¾„ (å¦‚æœå­˜åœ¨ä¸”æœ‰æ•ˆ)
                if models_config.prefer_local and local_path_str:
                    local_path = Path(local_path_str)

                    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”åŒ…å«å¿…è¦æ–‡ä»¶
                    if local_path.exists() and self._validate_model_directory(local_path):
                        logger.info(
                            f"âœ… ä½¿ç”¨æœ¬åœ°æ¨¡å‹",
                            extra={"path": str(local_path)}
                        )
                        return str(local_path)
                    else:
                        logger.warning(
                            f"âš ï¸ æœ¬åœ°æ¨¡å‹è·¯å¾„æ— æ•ˆæˆ–ä¸å®Œæ•´: {local_path}",
                            extra={"reason": "ç›®å½•ä¸å­˜åœ¨æˆ–ç¼ºå°‘å¿…è¦æ–‡ä»¶"}
                        )

            # 2. ä½¿ç”¨HFä»“åº“åç§° (ä¼šè‡ªåŠ¨ä¸‹è½½)
            repo_name = model_name or model_repo
            logger.info(
                f"ğŸ“¥ å°†ä»Hugging FaceåŠ è½½æ¨¡å‹",
                extra={"repo": repo_name}
            )
            return repo_name
        else:
            # å…¼å®¹æ—§é…ç½®
            return model_name or settings.token_optimization.text_model

    def _validate_model_directory(self, model_dir: Path) -> bool:
        """
        éªŒè¯æ¨¡å‹ç›®å½•æ˜¯å¦åŒ…å«å¿…è¦æ–‡ä»¶

        Args:
            model_dir: æ¨¡å‹ç›®å½•

        Returns:
            æ˜¯å¦æœ‰æ•ˆ
        """
        # å¿…é¡»å­˜åœ¨çš„æ–‡ä»¶ (è‡³å°‘åŒ…å«å…¶ä¸­ä¹‹ä¸€)
        required_files = [
            "config.json",
            "pytorch_model.bin",
            "model.safetensors",
        ]

        for file_name in required_files:
            if (model_dir / file_name).exists():
                return True

        return False

    def _setup_environment(self, settings):
        """
        è®¾ç½®Hugging Faceç¯å¢ƒå˜é‡

        Args:
            settings: é…ç½®å¯¹è±¡
        """
        if hasattr(settings, 'models') and settings.models:
            models_config = settings.models
            hf_config = models_config.huggingface

            # ç¦»çº¿æ¨¡å¼
            if hf_config.offline_mode:
                os.environ["TRANSFORMERS_OFFLINE"] = "1"
                os.environ["HF_HUB_OFFLINE"] = "1"
                logger.info("ğŸ”Œ Hugging Faceç¦»çº¿æ¨¡å¼å·²å¯ç”¨")

            # ä½¿ç”¨é•œåƒç«™
            if hf_config.use_mirror and hf_config.mirror_url:
                os.environ["HF_ENDPOINT"] = hf_config.mirror_url
                logger.info(
                    f"ğŸŒ ä½¿ç”¨Hugging Faceé•œåƒ",
                    extra={"mirror": hf_config.mirror_url}
                )

            # è®¾ç½®ç¼“å­˜ç›®å½• (ç»Ÿä¸€å­˜å‚¨ä½ç½®)
            cache_dir = Path(models_config.local_model_dir)
            cache_dir.mkdir(parents=True, exist_ok=True)
            os.environ["TRANSFORMERS_CACHE"] = str(cache_dir)
            os.environ["HF_HOME"] = str(cache_dir)

    def encode_single(
        self,
        text: str,
        normalize: bool = True,
        convert_to_numpy: bool = True,
    ) -> Union[np.ndarray, List[float]]:
        """
        ç”Ÿæˆå•æ¡æ–‡æœ¬åµŒå…¥

        Args:
            text: è¾“å…¥æ–‡æœ¬
            normalize: æ˜¯å¦å½’ä¸€åŒ–(ç”¨äºä½™å¼¦ç›¸ä¼¼åº¦)
            convert_to_numpy: æ˜¯å¦è½¬ä¸ºnumpyæ•°ç»„

        Returns:
            åµŒå…¥å‘é‡
        """
        try:
            embedding = self.model.encode(
                text,
                normalize_embeddings=normalize,
                convert_to_numpy=convert_to_numpy,
                show_progress_bar=False,
            )

            logger.debug(
                f"ç”ŸæˆåµŒå…¥",
                extra={"text_length": len(text), "dimension": len(embedding)},
            )

            return embedding if convert_to_numpy else embedding.tolist()

        except Exception as e:
            logger.error(f"ç”ŸæˆåµŒå…¥å¤±è´¥: {e}", extra={"text": text[:100]})
            # è¿”å›é›¶å‘é‡ä½œä¸ºé™çº§
            return np.zeros(self.dimension) if convert_to_numpy else [0.0] * self.dimension

    def encode_batch(
        self,
        texts: List[str],
        batch_size: int = 32,
        normalize: bool = True,
        show_progress: bool = False,
    ) -> np.ndarray:
        """
        æ‰¹é‡ç”ŸæˆåµŒå…¥(æ€§èƒ½ä¼˜åŒ–)

        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            normalize: æ˜¯å¦å½’ä¸€åŒ–
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡

        Returns:
            åµŒå…¥çŸ©é˜µ (num_texts, dimension)
        """
        try:
            embeddings = self.model.encode(
                texts,
                batch_size=batch_size,
                normalize_embeddings=normalize,
                convert_to_numpy=True,
                show_progress_bar=show_progress,
            )

            logger.info(
                f"æ‰¹é‡ç”ŸæˆåµŒå…¥",
                extra={
                    "count": len(texts),
                    "batch_size": batch_size,
                    "shape": embeddings.shape,
                },
            )

            return embeddings

        except Exception as e:
            logger.error(f"æ‰¹é‡ç”ŸæˆåµŒå…¥å¤±è´¥: {e}", extra={"count": len(texts)})
            return np.zeros((len(texts), self.dimension))

    @lru_cache(maxsize=1000)
    def encode_cached(self, text: str) -> tuple:
        """
        å¸¦LRUç¼“å­˜çš„åµŒå…¥ç”Ÿæˆ(é€‚ç”¨äºå¸¸ç”¨çŸ­æ–‡æœ¬)

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            åµŒå…¥å‘é‡(tupleæ ¼å¼,æ”¯æŒhash)

        Note:
            - ç¼“å­˜æœ€å¤š1000æ¡
            - é€‚ç”¨äºå¸¸ç”¨å…³é”®è¯ã€æ ‡ç­¾ç­‰çŸ­æ–‡æœ¬
            - é•¿æ–‡æœ¬ä¸å»ºè®®ç¼“å­˜
        """
        embedding = self.encode_single(text, convert_to_numpy=True)
        return tuple(embedding.tolist())

    def calculate_similarity(
        self,
        embedding1: Union[np.ndarray, List[float]],
        embedding2: Union[np.ndarray, List[float]],
        metric: str = "cosine",
    ) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªåµŒå…¥çš„ç›¸ä¼¼åº¦

        Args:
            embedding1: åµŒå…¥1
            embedding2: åµŒå…¥2
            metric: ç›¸ä¼¼åº¦åº¦é‡(cosine/euclidean/dot)

        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•°
        """
        # è½¬ä¸ºnumpyæ•°ç»„
        emb1 = np.array(embedding1) if not isinstance(embedding1, np.ndarray) else embedding1
        emb2 = np.array(embedding2) if not isinstance(embedding2, np.ndarray) else embedding2

        if metric == "cosine":
            # ä½™å¼¦ç›¸ä¼¼åº¦
            from sklearn.metrics.pairwise import cosine_similarity

            sim = cosine_similarity(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]

        elif metric == "euclidean":
            # æ¬§æ°è·ç¦»(è½¬ä¸ºç›¸ä¼¼åº¦:1 / (1 + distance))
            from sklearn.metrics.pairwise import euclidean_distances

            dist = euclidean_distances(emb1.reshape(1, -1), emb2.reshape(1, -1))[0][0]
            sim = 1.0 / (1.0 + dist)

        elif metric == "dot":
            # ç‚¹ç§¯
            sim = np.dot(emb1, emb2)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦åº¦é‡: {metric}")

        return float(sim)

    def calculate_similarity_matrix(
        self,
        embeddings1: np.ndarray,
        embeddings2: Optional[np.ndarray] = None,
        metric: str = "cosine",
    ) -> np.ndarray:
        """
        è®¡ç®—åµŒå…¥çŸ©é˜µçš„ç›¸ä¼¼åº¦çŸ©é˜µ

        Args:
            embeddings1: åµŒå…¥çŸ©é˜µ1 (m, dimension)
            embeddings2: åµŒå…¥çŸ©é˜µ2 (n, dimension),ä¸ºNoneæ—¶ä¸embeddings1è‡ªèº«è®¡ç®—
            metric: ç›¸ä¼¼åº¦åº¦é‡

        Returns:
            ç›¸ä¼¼åº¦çŸ©é˜µ (m, n)
        """
        if embeddings2 is None:
            embeddings2 = embeddings1

        if metric == "cosine":
            from sklearn.metrics.pairwise import cosine_similarity

            sim_matrix = cosine_similarity(embeddings1, embeddings2)

        elif metric == "euclidean":
            from sklearn.metrics.pairwise import euclidean_distances

            dist_matrix = euclidean_distances(embeddings1, embeddings2)
            sim_matrix = 1.0 / (1.0 + dist_matrix)

        elif metric == "dot":
            sim_matrix = np.dot(embeddings1, embeddings2.T)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦åº¦é‡: {metric}")

        return sim_matrix

    def find_most_similar(
        self,
        query_embedding: Union[np.ndarray, List[float]],
        candidate_embeddings: np.ndarray,
        top_k: int = 5,
        metric: str = "cosine",
    ) -> List[tuple]:
        """
        æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„å€™é€‰

        Args:
            query_embedding: æŸ¥è¯¢åµŒå…¥
            candidate_embeddings: å€™é€‰åµŒå…¥çŸ©é˜µ (num_candidates, dimension)
            top_k: è¿”å›Top-K
            metric: ç›¸ä¼¼åº¦åº¦é‡

        Returns:
            [(index, similarity_score), ...]
        """
        # è½¬ä¸ºnumpyæ•°ç»„
        query = (
            np.array(query_embedding)
            if not isinstance(query_embedding, np.ndarray)
            else query_embedding
        )

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for idx, candidate in enumerate(candidate_embeddings):
            sim = self.calculate_similarity(query, candidate, metric=metric)
            similarities.append((idx, sim))

        # æ’åºå¹¶è¿”å›Top-K
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]

    def get_dimension(self) -> int:
        """è·å–åµŒå…¥ç»´åº¦"""
        return self.dimension

    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "dimension": self.dimension,
            "device": self.device,
            "max_seq_length": self.model.max_seq_length,
        }

    def clear_cache(self) -> None:
        """æ¸…é™¤LRUç¼“å­˜"""
        self.encode_cached.cache_clear()
        logger.info("åµŒå…¥ç¼“å­˜å·²æ¸…é™¤")


# å•ä¾‹æ¨¡å¼
_embedding_service_instance: Optional[EmbeddingService] = None


def get_embedding_service() -> EmbeddingService:
    """
    è·å–åµŒå…¥æœåŠ¡å•ä¾‹

    Returns:
        EmbeddingServiceå®ä¾‹
    """
    global _embedding_service_instance

    if _embedding_service_instance is None:
        _embedding_service_instance = EmbeddingService()

    return _embedding_service_instance
