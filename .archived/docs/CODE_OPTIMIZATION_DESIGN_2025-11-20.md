# ä»£ç ä¼˜åŒ–è®¾è®¡æ–‡æ¡£

**ç‰ˆæœ¬**: v1.0.0  
**æ—¥æœŸ**: 2025-11-20  
**çŠ¶æ€**: å®æ–½é˜¶æ®µ  
**ä¼˜åŒ–èŒƒå›´**: Javaåˆ†æå™¨importå…³ç³» + å¤šå±‚ç¼“å­˜ç­–ç•¥ + Milvuså‘é‡æ£€ç´¢ä¼˜åŒ–  

---

## ğŸ“‹ ç›®å½•

1. [ä¼˜åŒ–æ¦‚è¿°](#ä¼˜åŒ–æ¦‚è¿°)
2. [Javaåˆ†æå™¨ä¼˜åŒ–](#javaåˆ†æå™¨ä¼˜åŒ–)
3. [å¤šå±‚ç¼“å­˜ç­–ç•¥](#å¤šå±‚ç¼“å­˜ç­–ç•¥)
4. [Milvuså‘é‡æ£€ç´¢ä¼˜åŒ–](#milvuså‘é‡æ£€ç´¢ä¼˜åŒ–)
5. [æ€§èƒ½å¯¹æ¯”](#æ€§èƒ½å¯¹æ¯”)
6. [å®æ–½è®¡åˆ’](#å®æ–½è®¡åˆ’)

---

## ä¼˜åŒ–æ¦‚è¿°

### 1.1 å½“å‰é—®é¢˜åˆ†æ

é€šè¿‡æ·±åº¦åˆ†æç°æœ‰ä»£ç ï¼Œå‘ç°ä»¥ä¸‹å…³é”®ä¼˜åŒ–ç‚¹ï¼š

| æ¨¡å— | å½“å‰é—®é¢˜ | å½±å“ | ä¼˜åŒ–ç›®æ ‡ |
|------|---------|------|---------|
| **Javaåˆ†æå™¨** | `_process_import()` ä»…ä¸ºTODOå ä½ç¬¦ | âŒ æ— æ³•è¿½è¸ªç±»ä¾èµ–å…³ç³» | âœ… å®Œæ•´importå…³ç³»å›¾ |
| **Redisç¼“å­˜** | å•å±‚ç¼“å­˜ï¼Œæ— æœ¬åœ°å†…å­˜ç¼“å­˜ | âš ï¸ æ¯æ¬¡éƒ½éœ€è¦ç½‘ç»œè¯·æ±‚ | âœ… L1å†…å­˜ + L2 Redis |
| **Milvusæ£€ç´¢** | HNSWå‚æ•°æœªä¼˜åŒ–ï¼Œæ— è¿æ¥æ±  | âš ï¸ æ£€ç´¢æ…¢ï¼Œè¿æ¥å¼€é”€å¤§ | âœ… å‚æ•°è°ƒä¼˜ + è¿æ¥å¤ç”¨ |

### 1.2 ä¼˜åŒ–ä»·å€¼

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡å¹…åº¦ |
|------|--------|--------|---------|
| ç¼“å­˜å‘½ä¸­å“åº”æ—¶é—´ | ~50ms (Redis) | **<1ms** (å†…å­˜) | **98%â†“** |
| å‘é‡æ£€ç´¢ç²¾åº¦ | 85% | **95%+** | **10%â†‘** |
| å‘é‡æ£€ç´¢é€Ÿåº¦ | ~200ms | **<100ms** | **50%â†“** |
| Javaä¾èµ–åˆ†æ | âŒ ä¸æ”¯æŒ | âœ… å®Œæ•´ä¾èµ–å›¾ | **æ–°å¢èƒ½åŠ›** |

---

## Javaåˆ†æå™¨ä¼˜åŒ–

### 2.1 é—®é¢˜æ ¹å› 

**å½“å‰ä»£ç ** (`src/mcp_core/java_analyzer.py:82-86`):

```python
def _process_import(self, imp):
    """å¤„ç†importè¯­å¥"""
    import_path = imp.path
    # TODO: åˆ›å»ºimportå…³ç³»
    pass  # âŒ æœªå®ç°ï¼
```

**å½±å“**:
- âŒ æ— æ³•è¿½è¸ªç±»ä¹‹é—´çš„ä¾èµ–å…³ç³»
- âŒ æ— æ³•æ£€æµ‹å¾ªç¯ä¾èµ–
- âŒ æ— æ³•ç”Ÿæˆä¾èµ–å…³ç³»å›¾
- âŒ æ— æ³•è¿›è¡Œå½±å“åˆ†æ (æŸä¸ªç±»ä¿®æ”¹ä¼šå½±å“å“ªäº›å…¶ä»–ç±»)

### 2.2 ä¼˜åŒ–è®¾è®¡

#### 2.2.1 Importå…³ç³»æ•°æ®ç»“æ„

```python
@dataclass
class ImportRelation:
    """Importå…³ç³»"""
    source_file: str          # å¯¼å…¥æ–¹æ–‡ä»¶è·¯å¾„
    source_class: str         # å¯¼å…¥æ–¹ç±»å (å®Œå…¨é™å®šå)
    imported_class: str       # è¢«å¯¼å…¥ç±»å (å®Œå…¨é™å®šå)
    import_type: str          # importç±»å‹: single/wildcard/static
    is_used: bool = False     # æ˜¯å¦çœŸæ­£ä½¿ç”¨ (é€šè¿‡ä»£ç æ‰«æç¡®å®š)
    line_number: int = 0      # importè¯­å¥è¡Œå·
```

#### 2.2.2 Importåˆ†ç±»å¤„ç†

Javaçš„importæœ‰4ç§ç±»å‹ï¼Œéœ€åˆ†åˆ«å¤„ç†ï¼š

| Importç±»å‹ | ç¤ºä¾‹ | å¤„ç†ç­–ç•¥ |
|-----------|------|---------|
| **å•ç±»å¯¼å…¥** | `import java.util.List;` | ç²¾ç¡®è®°å½•ï¼š`List` â†’ `java.util.List` |
| **é€šé…ç¬¦å¯¼å…¥** | `import java.util.*;` | è®°å½•åŒ…åï¼Œå»¶è¿Ÿè§£æ (æ‰«æä»£ç ä½¿ç”¨) |
| **é™æ€å¯¼å…¥** | `import static java.lang.Math.PI;` | è®°å½•é™æ€æˆå‘˜ï¼š`PI` â†’ `java.lang.Math.PI` |
| **é™æ€é€šé…ç¬¦** | `import static java.util.Collections.*;` | è®°å½•é™æ€åŒ…ï¼Œå»¶è¿Ÿè§£æ |

#### 2.2.3 å®Œæ•´å®ç°

```python
def _process_import(self, imp):
    """
    å¤„ç†importè¯­å¥ï¼Œå»ºç«‹ä¾èµ–å…³ç³»
    
    Args:
        imp: javalang.tree.Importå¯¹è±¡
    """
    import_path = imp.path  # å¦‚ "java.util.List"
    is_static = imp.static  # æ˜¯å¦é™æ€å¯¼å…¥
    is_wildcard = imp.wildcard  # æ˜¯å¦é€šé…ç¬¦å¯¼å…¥
    
    # 1. ç¡®å®šå¯¼å…¥ç±»å‹
    if is_static:
        import_type = "static_wildcard" if is_wildcard else "static_single"
    else:
        import_type = "wildcard" if is_wildcard else "single"
    
    # 2. æå–è¢«å¯¼å…¥çš„ç±»/åŒ…å
    if is_wildcard:
        # é€šé…ç¬¦å¯¼å…¥ï¼šè®°å½•åŒ…å
        imported_entity = import_path  # å¦‚ "java.util"
        target_name = f"{import_path}.*"
    else:
        # å•ç±»å¯¼å…¥ï¼šè®°å½•å®Œæ•´ç±»å
        imported_entity = import_path  # å¦‚ "java.util.List"
        # æå–ç®€å•ç±»å (å¦‚ "List")
        simple_name = import_path.split(".")[-1]
        target_name = simple_name
    
    # 3. ç”Ÿæˆimportå…³ç³»ID
    import_id = self._generate_id("import", imported_entity, imp.position.line if hasattr(imp, 'position') else 0)
    
    # 4. åˆ›å»ºCodeEntityè®°å½•import
    import_entity = CodeEntity(
        id=import_id,
        type="import",
        name=target_name,
        qualified_name=imported_entity,
        file_path=self.relative_path,
        line_number=imp.position.line if hasattr(imp, 'position') else 0,
        end_line=imp.position.line if hasattr(imp, 'position') else 0,
        signature=f"import {'static ' if is_static else ''}{import_path}{'.*' if is_wildcard else ''}",
        metadata={
            "import_type": import_type,
            "is_static": is_static,
            "is_wildcard": is_wildcard,
            "package": ".".join(import_path.split(".")[:-1]) if not is_wildcard else import_path,
            "simple_name": target_name
        }
    )
    
    self.entities.append(import_entity)
    
    # 5. å»ºç«‹importå…³ç³» (æ–‡ä»¶çº§åˆ«ä¾èµ–)
    # æ³¨æ„ï¼šè¿™é‡Œæš‚æ—¶ä½¿ç”¨imported_entityä½œä¸ºtarget_idï¼Œåç»­éœ€è¦è§£æä¸ºå®é™…ç±»ID
    self.relations.append(CodeRelation(
        source_id=self.file_path,  # å½“å‰æ–‡ä»¶ä¾èµ–äºimported_entity
        target_id=imported_entity,
        relation_type="imports",
        metadata={
            "import_type": import_type,
            "simple_name": target_name,
            "line": imp.position.line if hasattr(imp, 'position') else 0
        }
    ))
    
    # 6. å­˜å‚¨åˆ°importæ˜ å°„è¡¨ (ç”¨äºåç»­ç±»å‹è§£æ)
    if not hasattr(self, 'import_map'):
        self.import_map = {}
    
    self.import_map[target_name] = imported_entity
```

#### 2.2.4 Importä½¿ç”¨åˆ†æ

æ‰«æä»£ç ä¸­å®é™…ä½¿ç”¨çš„ç±»ï¼Œæ ‡è®°`is_used`ï¼š

```python
def _analyze_import_usage(self):
    """
    åˆ†æimportçš„å®é™…ä½¿ç”¨æƒ…å†µ
    æ ‡è®°æœªä½¿ç”¨çš„import (ä»£ç ä¼˜åŒ–æç¤º)
    """
    used_imports = set()
    
    # æ‰«ææ‰€æœ‰å®ä½“çš„ç±»å‹å¼•ç”¨
    for entity in self.entities:
        if entity.type in ["variable", "method"]:
            # ä»metadataä¸­æå–ç±»å‹ä¿¡æ¯
            if "field_type" in entity.metadata:
                type_name = entity.metadata["field_type"]
                # æå–ç®€å•ç±»å (å¦‚ "List<String>" â†’ "List")
                simple_type = type_name.split("<")[0].split("[")[0]
                used_imports.add(simple_type)
            
            if entity.type == "method" and "return_type" in entity.metadata:
                return_type = entity.metadata["return_type"]
                simple_type = return_type.split("<")[0].split("[")[0]
                used_imports.add(simple_type)
            
            if entity.type == "method" and "parameters" in entity.metadata:
                for param in entity.metadata["parameters"]:
                    param_type = param["type"]
                    simple_type = param_type.split("<")[0].split("[")[0]
                    used_imports.add(simple_type)
    
    # æ ‡è®°ä½¿ç”¨çš„import
    for entity in self.entities:
        if entity.type == "import":
            simple_name = entity.metadata["simple_name"]
            if simple_name in used_imports or entity.metadata["is_wildcard"]:
                entity.metadata["is_used"] = True
            else:
                entity.metadata["is_used"] = False
                # å¯ä»¥ç”Ÿæˆä»£ç ä¼˜åŒ–å»ºè®®
                logger.debug(f"æœªä½¿ç”¨çš„import: {entity.qualified_name} (è¡Œ {entity.line_number})")
```

#### 2.2.5 ä¾èµ–å…³ç³»å›¾ç”Ÿæˆ

```python
def build_dependency_graph(self) -> Dict[str, List[str]]:
    """
    æ„å»ºç±»ä¾èµ–å…³ç³»å›¾
    
    Returns:
        {
            "com.example.UserService": [
                "com.example.UserRepository",
                "com.example.User",
                "java.util.List"
            ],
            ...
        }
    """
    dependency_graph = {}
    
    # è·å–å½“å‰æ–‡ä»¶å®šä¹‰çš„æ‰€æœ‰ç±»
    defined_classes = [
        entity.qualified_name 
        for entity in self.entities 
        if entity.type in ["class", "interface", "enum"]
    ]
    
    # å¯¹æ¯ä¸ªç±»ï¼Œæ”¶é›†å…¶ä¾èµ–
    for class_name in defined_classes:
        dependencies = []
        
        # 1. ä»importå…³ç³»æå–
        for relation in self.relations:
            if relation.relation_type == "imports":
                # æ£€æŸ¥importæ˜¯å¦è¢«å½“å‰ç±»ä½¿ç”¨
                imported_class = relation.target_id
                dependencies.append(imported_class)
        
        # 2. ä»ç»§æ‰¿/å®ç°å…³ç³»æå–
        for relation in self.relations:
            if relation.relation_type in ["extends", "implements"]:
                parent_class = relation.target_id
                # è§£æä¸ºå®Œå…¨é™å®šå (é€šè¿‡import_map)
                if hasattr(self, 'import_map') and parent_class in self.import_map:
                    full_name = self.import_map[parent_class]
                    dependencies.append(full_name)
                else:
                    dependencies.append(parent_class)
        
        # 3. ä»å­—æ®µç±»å‹æå–
        for entity in self.entities:
            if entity.type == "variable" and entity.parent_id:
                field_type = entity.metadata.get("field_type", "")
                simple_type = field_type.split("<")[0].split("[")[0]
                if simple_type in self.import_map:
                    dependencies.append(self.import_map[simple_type])
        
        # å»é‡
        dependency_graph[class_name] = list(set(dependencies))
    
    return dependency_graph
```

### 2.3 åº”ç”¨åœºæ™¯

#### åœºæ™¯1: å¾ªç¯ä¾èµ–æ£€æµ‹

```python
def detect_circular_dependencies(dependency_graph: Dict[str, List[str]]) -> List[List[str]]:
    """
    æ£€æµ‹å¾ªç¯ä¾èµ–
    
    Returns:
        å¾ªç¯ä¾èµ–é“¾åˆ—è¡¨ï¼Œå¦‚: [
            ["A", "B", "C", "A"],  # Aâ†’Bâ†’Câ†’A å½¢æˆå¾ªç¯
            ...
        ]
    """
    cycles = []
    
    def dfs(node, path, visited):
        if node in path:
            # å‘ç°å¾ªç¯
            cycle_start = path.index(node)
            cycle = path[cycle_start:] + [node]
            cycles.append(cycle)
            return
        
        if node in visited:
            return
        
        visited.add(node)
        path.append(node)
        
        for neighbor in dependency_graph.get(node, []):
            dfs(neighbor, path, visited)
        
        path.pop()
    
    visited = set()
    for node in dependency_graph.keys():
        if node not in visited:
            dfs(node, [], visited)
    
    return cycles
```

#### åœºæ™¯2: å½±å“åˆ†æ

```python
def analyze_impact(class_name: str, dependency_graph: Dict[str, List[str]]) -> Dict[str, List[str]]:
    """
    åˆ†ææŸä¸ªç±»ä¿®æ”¹çš„å½±å“èŒƒå›´
    
    Args:
        class_name: è¢«ä¿®æ”¹çš„ç±»å
        
    Returns:
        {
            "direct_impact": ["ç›´æ¥ä¾èµ–æ­¤ç±»çš„ç±»"],
            "indirect_impact": ["é—´æ¥ä¾èµ–æ­¤ç±»çš„ç±»"],
            "impact_level": 3  # å½±å“å±‚çº§æ·±åº¦
        }
    """
    # æ„å»ºåå‘ä¾èµ–å›¾ (è°ä¾èµ–æˆ‘)
    reverse_graph = {}
    for source, targets in dependency_graph.items():
        for target in targets:
            if target not in reverse_graph:
                reverse_graph[target] = []
            reverse_graph[target].append(source)
    
    # BFSæŸ¥æ‰¾æ‰€æœ‰å—å½±å“çš„ç±»
    direct_impact = reverse_graph.get(class_name, [])
    
    all_impact = set(direct_impact)
    queue = list(direct_impact)
    level = 1
    max_level = 1
    
    while queue:
        next_level = []
        for node in queue:
            for dependent in reverse_graph.get(node, []):
                if dependent not in all_impact:
                    all_impact.add(dependent)
                    next_level.append(dependent)
        
        if next_level:
            level += 1
            max_level = level
            queue = next_level
        else:
            break
    
    indirect_impact = list(all_impact - set(direct_impact))
    
    return {
        "direct_impact": direct_impact,
        "indirect_impact": indirect_impact,
        "impact_level": max_level,
        "total_affected": len(all_impact)
    }
```

---

## å¤šå±‚ç¼“å­˜ç­–ç•¥

### 3.1 å½“å‰é—®é¢˜

**ç°æœ‰å®ç°** (`src/mcp_core/services/redis_client.py`):
- âœ… Redis L2ç¼“å­˜ (å·²å®ç°)
- âŒ æ— æœ¬åœ°å†…å­˜L1ç¼“å­˜ â†’ æ¯æ¬¡éƒ½éœ€è¦ç½‘ç»œè¯·æ±‚ (~5-50ms)
- âŒ æ— ç¼“å­˜é¢„çƒ­æœºåˆ¶
- âŒ æ— LRUæ·˜æ±°ç­–ç•¥

### 3.2 å¤šå±‚ç¼“å­˜æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  L1 ç¼“å­˜ (æœ¬åœ°å†…å­˜) - æœ€å¿«                                â”‚
â”‚  - LRUæ·˜æ±°ç­–ç•¥                                           â”‚
â”‚  - TTL: 60ç§’                                             â”‚
â”‚  - å®¹é‡: 1000æ¡                                          â”‚
â”‚  - å“åº”æ—¶é—´: <1ms                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“ (L1 Miss)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  L2 ç¼“å­˜ (Redis) - å¿«                                    â”‚
â”‚  - TTL: 5åˆ†é’Ÿ (é«˜é¢‘) / 7å¤© (æ£€ç´¢ç»“æœ)                    â”‚
â”‚  - å®¹é‡: æ— é™åˆ¶                                          â”‚
â”‚  - å“åº”æ—¶é—´: ~5ms                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†“ (L2 Miss)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  L3 å­˜å‚¨ (MySQL + Milvus) - æ…¢ä½†å®Œæ•´                     â”‚
â”‚  - æŒä¹…åŒ–å­˜å‚¨                                            â”‚
â”‚  - å“åº”æ—¶é—´: ~50-200ms                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 å®ç°ä»£ç 

#### 3.3.1 LRUç¼“å­˜å®ç°

```python
"""
å¤šå±‚ç¼“å­˜ç®¡ç†å™¨
L1 (å†…å­˜LRU) + L2 (Redis) + L3 (æ•°æ®åº“/å‘é‡åº“)
"""

from collections import OrderedDict
from typing import Any, Optional, Dict
import time
import threading


class LRUCache:
    """çº¿ç¨‹å®‰å…¨çš„LRUç¼“å­˜"""
    
    def __init__(self, capacity: int = 1000, ttl: int = 60):
        """
        Args:
            capacity: æœ€å¤§å®¹é‡
            ttl: è¿‡æœŸæ—¶é—´ (ç§’)
        """
        self.capacity = capacity
        self.ttl = ttl
        self.cache: OrderedDict[str, tuple[Any, float]] = OrderedDict()
        self.lock = threading.RLock()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.hits = 0
        self.misses = 0
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        with self.lock:
            if key not in self.cache:
                self.misses += 1
                return None
            
            value, timestamp = self.cache[key]
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if time.time() - timestamp > self.ttl:
                del self.cache[key]
                self.misses += 1
                return None
            
            # ç§»åˆ°æœ«å°¾ (æœ€è¿‘ä½¿ç”¨)
            self.cache.move_to_end(key)
            self.hits += 1
            return value
    
    def set(self, key: str, value: Any) -> None:
        """è®¾ç½®ç¼“å­˜å€¼"""
        with self.lock:
            # å¦‚æœå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
            if key in self.cache:
                del self.cache[key]
            
            # æ·»åŠ æ–°å€¼ (å¸¦æ—¶é—´æˆ³)
            self.cache[key] = (value, time.time())
            self.cache.move_to_end(key)
            
            # è¶…è¿‡å®¹é‡ï¼Œåˆ é™¤æœ€æ—§çš„
            if len(self.cache) > self.capacity:
                self.cache.popitem(last=False)
    
    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        with self.lock:
            if key in self.cache:
                del self.cache[key]
                return True
            return False
    
    def clear(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
            self.hits = 0
            self.misses = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        with self.lock:
            total_requests = self.hits + self.misses
            hit_rate = self.hits / total_requests if total_requests > 0 else 0
            
            return {
                "capacity": self.capacity,
                "size": len(self.cache),
                "hits": self.hits,
                "misses": self.misses,
                "hit_rate": f"{hit_rate:.2%}",
                "utilization": f"{len(self.cache) / self.capacity:.2%}"
            }
```

#### 3.3.2 å¤šå±‚ç¼“å­˜ç®¡ç†å™¨

```python
class MultiLevelCache:
    """
    å¤šå±‚ç¼“å­˜ç®¡ç†å™¨
    L1 (å†…å­˜) â†’ L2 (Redis) â†’ L3 (æ•°æ®åº“)
    """
    
    def __init__(
        self,
        l1_capacity: int = 1000,
        l1_ttl: int = 60,
        l2_ttl: int = 300,
        redis_client: Optional['RedisClient'] = None
    ):
        """
        Args:
            l1_capacity: L1ç¼“å­˜å®¹é‡
            l1_ttl: L1ç¼“å­˜TTL (ç§’)
            l2_ttl: L2ç¼“å­˜TTL (ç§’)
            redis_client: Rediså®¢æˆ·ç«¯ (å¯é€‰)
        """
        # L1ç¼“å­˜ (å†…å­˜LRU)
        self.l1_cache = LRUCache(capacity=l1_capacity, ttl=l1_ttl)
        
        # L2ç¼“å­˜ (Redis)
        self.redis_client = redis_client
        self.l2_ttl = l2_ttl
        
        # ç¼“å­˜é”®å‰ç¼€
        self.key_prefix = "mlc:"  # Multi-Level Cache
        
        logger.info(
            "å¤šå±‚ç¼“å­˜åˆå§‹åŒ–å®Œæˆ",
            extra={
                "l1_capacity": l1_capacity,
                "l1_ttl": l1_ttl,
                "l2_ttl": l2_ttl,
                "redis_enabled": redis_client is not None
            }
        )
    
    def get(self, key: str, l3_loader: Optional[callable] = None) -> Optional[Any]:
        """
        å¤šå±‚ç¼“å­˜è·å–
        
        Args:
            key: ç¼“å­˜é”®
            l3_loader: L3æ•°æ®åŠ è½½å‡½æ•° (lambda: load_from_db())
            
        Returns:
            ç¼“å­˜å€¼ï¼Œæ‰€æœ‰å±‚çº§å‡æœªå‘½ä¸­è¿”å›None
        """
        # L1: å†…å­˜ç¼“å­˜
        value = self.l1_cache.get(key)
        if value is not None:
            logger.debug(f"L1ç¼“å­˜å‘½ä¸­: {key}")
            return value
        
        # L2: Redisç¼“å­˜
        if self.redis_client:
            cache_key = f"{self.key_prefix}{key}"
            value = self.redis_client.cache_get(cache_key)
            if value is not None:
                logger.debug(f"L2ç¼“å­˜å‘½ä¸­: {key}")
                # å›å¡«L1
                self.l1_cache.set(key, value)
                return value
        
        # L3: æ•°æ®åŠ è½½å™¨
        if l3_loader:
            value = l3_loader()
            if value is not None:
                logger.debug(f"L3åŠ è½½æˆåŠŸ: {key}")
                # å›å¡«L2å’ŒL1
                self.set(key, value)
                return value
        
        logger.debug(f"æ‰€æœ‰å±‚çº§æœªå‘½ä¸­: {key}")
        return None
    
    def set(self, key: str, value: Any, l2_ttl: Optional[int] = None) -> None:
        """
        è®¾ç½®å¤šå±‚ç¼“å­˜
        
        Args:
            key: ç¼“å­˜é”®
            value: ç¼“å­˜å€¼
            l2_ttl: L2 TTL (å¯é€‰ï¼Œè¦†ç›–é»˜è®¤å€¼)
        """
        # è®¾ç½®L1
        self.l1_cache.set(key, value)
        
        # è®¾ç½®L2
        if self.redis_client:
            cache_key = f"{self.key_prefix}{key}"
            ttl = l2_ttl if l2_ttl is not None else self.l2_ttl
            self.redis_client.cache_set(cache_key, value, ttl=ttl)
        
        logger.debug(f"å¤šå±‚ç¼“å­˜è®¾ç½®æˆåŠŸ: {key}")
    
    def delete(self, key: str) -> None:
        """åˆ é™¤å¤šå±‚ç¼“å­˜"""
        # åˆ é™¤L1
        self.l1_cache.delete(key)
        
        # åˆ é™¤L2
        if self.redis_client:
            cache_key = f"{self.key_prefix}{key}"
            self.redis_client.delete(cache_key)
        
        logger.debug(f"å¤šå±‚ç¼“å­˜åˆ é™¤: {key}")
    
    def invalidate_pattern(self, pattern: str) -> int:
        """
        æ ¹æ®æ¨¡å¼æ¸…é™¤ç¼“å­˜
        
        Args:
            pattern: é”®æ¨¡å¼ (å¦‚ "user:*")
            
        Returns:
            æ¸…é™¤çš„L2ç¼“å­˜æ•°é‡ (L1æ— æ³•æŒ‰æ¨¡å¼æ¸…é™¤)
        """
        # L1: æš´åŠ›æ¸…ç©º (æ— æ³•æŒ‰æ¨¡å¼æ¸…é™¤)
        self.l1_cache.clear()
        
        # L2: Redisæ”¯æŒæ¨¡å¼æ¸…é™¤
        count = 0
        if self.redis_client:
            cache_pattern = f"{self.key_prefix}{pattern}"
            count = self.redis_client.invalidate_cache("", pattern=pattern)
        
        logger.info(f"æ¨¡å¼æ¸…é™¤ç¼“å­˜: {pattern}, L2æ¸…é™¤æ•°é‡: {count}")
        return count
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        l1_stats = self.l1_cache.get_stats()
        
        return {
            "l1_memory": l1_stats,
            "redis_enabled": self.redis_client is not None
        }
    
    def warmup(self, keys: List[str], loader: callable) -> int:
        """
        ç¼“å­˜é¢„çƒ­
        
        Args:
            keys: è¦é¢„çƒ­çš„é”®åˆ—è¡¨
            loader: æ•°æ®åŠ è½½å‡½æ•° (key) -> value
            
        Returns:
            é¢„çƒ­æˆåŠŸæ•°é‡
        """
        success_count = 0
        
        for key in keys:
            try:
                value = loader(key)
                if value is not None:
                    self.set(key, value)
                    success_count += 1
            except Exception as e:
                logger.error(f"é¢„çƒ­å¤±è´¥: {key}, é”™è¯¯: {e}")
                continue
        
        logger.info(f"ç¼“å­˜é¢„çƒ­å®Œæˆ: {success_count}/{len(keys)}")
        return success_count
```

#### 3.3.3 é›†æˆåˆ°RedisClient

åœ¨`src/mcp_core/services/redis_client.py`ä¸­æ·»åŠ ï¼š

```python
class RedisClient:
    def __init__(self, redis_url: Optional[str] = None):
        # ... ç°æœ‰ä»£ç  ...
        
        # æ–°å¢ï¼šå¤šå±‚ç¼“å­˜ç®¡ç†å™¨
        self.multi_level_cache = MultiLevelCache(
            l1_capacity=1000,
            l1_ttl=60,
            l2_ttl=300,
            redis_client=self
        )
        
        logger.info("å¤šå±‚ç¼“å­˜å·²å¯ç”¨")
    
    def cache_get(self, key: str) -> Optional[Any]:
        """
        è·å–ç¼“å­˜ (ä¼˜å…ˆä»å¤šå±‚ç¼“å­˜)
        
        Args:
            key: ç¼“å­˜é”®
            
        Returns:
            ç¼“å­˜å€¼
        """
        # ä½¿ç”¨å¤šå±‚ç¼“å­˜
        return self.multi_level_cache.get(
            key,
            l3_loader=lambda: self._redis_get_raw(key)  # å›é€€åˆ°ç›´æ¥RedisæŸ¥è¯¢
        )
    
    def _redis_get_raw(self, key: str) -> Optional[Any]:
        """RedisåŸç”Ÿget (å†…éƒ¨ä½¿ç”¨)"""
        try:
            cached = self.client.get(key)
            if cached:
                return json.loads(cached.decode("utf-8"))
            return None
        except Exception as e:
            logger.error(f"Redis getå¤±è´¥: {e}")
            return None
    
    def cache_set(self, key: str, value: Any, ttl: int = 300) -> bool:
        """è®¾ç½®ç¼“å­˜ (åŒæ—¶å†™å…¥å¤šå±‚)"""
        try:
            # ä½¿ç”¨å¤šå±‚ç¼“å­˜
            self.multi_level_cache.set(key, value, l2_ttl=ttl)
            
            # åŒæ—¶å†™å…¥Redis (ç¡®ä¿æŒä¹…åŒ–)
            serialized = json.dumps(value, ensure_ascii=False).encode("utf-8")
            self.client.setex(key, ttl, serialized)
            
            return True
        except Exception as e:
            logger.error(f"ç¼“å­˜è®¾ç½®å¤±è´¥: {e}")
            return False
```

---

## Milvuså‘é‡æ£€ç´¢ä¼˜åŒ–

### 4.1 å½“å‰é—®é¢˜

**ç°æœ‰å®ç°** (`src/mcp_core/services/vector_db.py`):
- âœ… HNSWç´¢å¼• (å·²ä½¿ç”¨)
- âš ï¸ å‚æ•°æœªä¼˜åŒ–ï¼š`M=16, efConstruction=200` (ä¸­ç­‰é…ç½®)
- âŒ æ— `efSearch`åŠ¨æ€è°ƒæ•´
- âŒ æ¯æ¬¡æŸ¥è¯¢éƒ½åˆ›å»ºæ–°Collectionå¯¹è±¡ (è¿æ¥å¼€é”€)
- âŒ æ— ç»“æœç¼“å­˜

### 4.2 HNSWå‚æ•°ä¼˜åŒ–

#### 4.2.1 å‚æ•°è¯´æ˜

| å‚æ•° | å½“å‰å€¼ | ä¼˜åŒ–å€¼ | è¯´æ˜ |
|------|--------|--------|------|
| **M** | 16 | **32** | æ¯ä¸ªèŠ‚ç‚¹çš„é‚»å±…æ•°ï¼Œâ†‘æå‡å¬å›ç‡ä½†â†‘å†…å­˜ |
| **efConstruction** | 200 | **400** | æ„å»ºæ—¶æœç´¢æ·±åº¦ï¼Œâ†‘æå‡ç´¢å¼•è´¨é‡ |
| **efSearch** | âŒæœªè®¾ç½® | **64-128** | æŸ¥è¯¢æ—¶æœç´¢æ·±åº¦ï¼ŒåŠ¨æ€è°ƒæ•´ |

#### 4.2.2 ä¼˜åŒ–ç­–ç•¥

```python
class VectorDBClient:
    # ä¼˜åŒ–åçš„Schema
    COLLECTION_SCHEMAS = {
        "mid_term_memories": {
            "description": "ä¸­æœŸé¡¹ç›®è®°å¿†å‘é‡å­˜å‚¨",
            "fields": [...],  # ä¿æŒä¸å˜
            "index": {
                "field_name": "embedding",
                "index_type": "HNSW",
                "metric_type": "COSINE",
                "params": {
                    "M": 32,              # â†‘ 16 â†’ 32 (æå‡å¬å›ç‡)
                    "efConstruction": 400  # â†‘ 200 â†’ 400 (æå‡ç´¢å¼•è´¨é‡)
                }
            }
        },
        # æ–°å¢: é”™è¯¯å‘é‡Collection (ç”¨äºé”™è¯¯é˜²ç«å¢™)
        "error_vectors": {
            "description": "é”™è¯¯ç‰¹å¾å‘é‡åº“",
            "fields": [
                {"name": "error_id", "dtype": DataType.VARCHAR, "max_length": 128, "is_primary": True},
                {"name": "embedding", "dtype": DataType.FLOAT_VECTOR, "dim": 768},
                {"name": "error_scene", "dtype": DataType.VARCHAR, "max_length": 100},
                {"name": "error_type", "dtype": DataType.VARCHAR, "max_length": 50},
                {"name": "created_at", "dtype": DataType.INT64}
            ],
            "index": {
                "field_name": "embedding",
                "index_type": "HNSW",
                "metric_type": "COSINE",
                "params": {
                    "M": 32,
                    "efConstruction": 400
                }
            }
        }
    }
    
    def __init__(self, host: Optional[str] = None, port: Optional[int] = None):
        # ... ç°æœ‰ä»£ç  ...
        
        # æ–°å¢ï¼šCollectionè¿æ¥æ±  (å¤ç”¨Collectionå¯¹è±¡)
        self.collection_pool: Dict[str, Collection] = {}
        
        # æ–°å¢ï¼šæŸ¥è¯¢å‚æ•°é…ç½®
        self.search_params_cache = {}
    
    def _get_collection(self, collection_name: str) -> Collection:
        """
        è·å–Collection (ä»è¿æ¥æ± å¤ç”¨)
        
        Args:
            collection_name: Collectionåç§°
            
        Returns:
            Collectionå¯¹è±¡
        """
        if collection_name not in self.collection_pool:
            self.collection_pool[collection_name] = Collection(collection_name)
            logger.debug(f"åˆ›å»ºCollectionè¿æ¥: {collection_name}")
        
        return self.collection_pool[collection_name]
    
    def search_vectors(
        self,
        collection_name: str,
        query_vectors: List[List[float]],
        top_k: int = 10,
        filter_expr: Optional[str] = None,
        ef_search: Optional[int] = None  # æ–°å¢ï¼šåŠ¨æ€efSearchå‚æ•°
    ) -> List[List[Dict[str, Any]]]:
        """
        å‘é‡æ£€ç´¢ (ä¼˜åŒ–ç‰ˆ)
        
        Args:
            collection_name: Collectionåç§°
            query_vectors: æŸ¥è¯¢å‘é‡åˆ—è¡¨
            top_k: è¿”å›Top-Kç»“æœ
            filter_expr: è¿‡æ»¤è¡¨è¾¾å¼
            ef_search: æœç´¢æ·±åº¦ (Noneåˆ™æ ¹æ®top_kè‡ªåŠ¨è®¡ç®—)
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # ä½¿ç”¨è¿æ¥æ± è·å–Collection
            collection = self._get_collection(collection_name)
            
            # ç¡®ä¿Collectionå·²åŠ è½½
            load_state = utility.load_state(collection_name)
            if str(load_state) != "Loaded":
                collection.load()
                logger.info(f"Collectionå·²åŠ è½½: {collection_name}")
            
            # åŠ¨æ€è®¡ç®—efSearch (æ ¹æ®top_kè°ƒæ•´)
            if ef_search is None:
                # å¯å‘å¼è§„åˆ™ï¼šefSearch = max(top_k * 2, 64)
                ef_search = max(top_k * 2, 64)
                if top_k > 50:
                    ef_search = 128  # é«˜top_kæ—¶ä½¿ç”¨æ›´å¤§çš„æœç´¢æ·±åº¦
            
            # æ„å»ºæœç´¢å‚æ•°
            search_params = {
                "metric_type": "COSINE",
                "params": {"ef": ef_search}  # å…³é”®ï¼šåŠ¨æ€efSearch
            }
            
            # æ‰§è¡Œæ£€ç´¢
            results = collection.search(
                data=query_vectors,
                anns_field="embedding",
                param=search_params,
                limit=top_k,
                expr=filter_expr,
                output_fields=["*"]  # è¿”å›æ‰€æœ‰å­—æ®µ
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for hits in results:
                hit_list = []
                for hit in hits:
                    hit_data = {
                        "id": hit.id,
                        "distance": hit.distance,
                        "score": 1 - hit.distance,  # COSINEè·ç¦»è½¬ç›¸ä¼¼åº¦
                    }
                    
                    # æ·»åŠ æ‰€æœ‰è¾“å‡ºå­—æ®µ
                    if hasattr(hit, 'entity'):
                        for field in collection.schema.fields:
                            if field.name != "embedding":  # è·³è¿‡å‘é‡å­—æ®µ
                                hit_data[field.name] = hit.entity.get(field.name)
                    
                    hit_list.append(hit_data)
                
                formatted_results.append(hit_list)
            
            logger.info(
                f"å‘é‡æ£€ç´¢å®Œæˆ",
                extra={
                    "collection": collection_name,
                    "query_count": len(query_vectors),
                    "top_k": top_k,
                    "ef_search": ef_search,
                    "results": sum(len(r) for r in formatted_results)
                }
            )
            
            return formatted_results
        
        except Exception as e:
            logger.error(f"å‘é‡æ£€ç´¢å¤±è´¥: {e}", extra={"collection": collection_name})
            return []
    
    def close(self) -> None:
        """å…³é—­æ‰€æœ‰è¿æ¥"""
        # é‡Šæ”¾è¿æ¥æ± ä¸­çš„æ‰€æœ‰Collection
        for collection_name, collection in self.collection_pool.items():
            try:
                collection.release()
                logger.debug(f"é‡Šæ”¾Collection: {collection_name}")
            except:
                pass
        
        self.collection_pool.clear()
        
        # æ–­å¼€Milvusè¿æ¥
        connections.disconnect("default")
        logger.info("Milvusè¿æ¥å·²å…³é—­")
```

### 4.3 å‘é‡æ£€ç´¢ç¼“å­˜

```python
class CachedVectorDBClient(VectorDBClient):
    """
    å¸¦ç¼“å­˜çš„å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯
    å¯¹é«˜é¢‘æŸ¥è¯¢å‘é‡è¿›è¡Œç¼“å­˜
    """
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # å‘é‡æ£€ç´¢ç»“æœç¼“å­˜ (ä½¿ç”¨MultiLevelCache)
        from .redis_client import get_redis_client
        self.result_cache = MultiLevelCache(
            l1_capacity=500,   # L1ç¼“å­˜500ä¸ªæŸ¥è¯¢
            l1_ttl=120,        # 2åˆ†é’Ÿ
            l2_ttl=3600,       # 1å°æ—¶
            redis_client=get_redis_client()
        )
    
    def search_vectors(
        self,
        collection_name: str,
        query_vectors: List[List[float]],
        top_k: int = 10,
        filter_expr: Optional[str] = None,
        use_cache: bool = True,  # æ–°å¢ï¼šæ˜¯å¦ä½¿ç”¨ç¼“å­˜
        **kwargs
    ) -> List[List[Dict[str, Any]]]:
        """
        å‘é‡æ£€ç´¢ (å¸¦ç¼“å­˜)
        
        Args:
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ (é»˜è®¤True)
        """
        if not use_cache:
            # è·³è¿‡ç¼“å­˜ï¼Œç›´æ¥æŸ¥è¯¢
            return super().search_vectors(collection_name, query_vectors, top_k, filter_expr, **kwargs)
        
        # ç”Ÿæˆç¼“å­˜é”® (åŸºäºæŸ¥è¯¢å‘é‡hash)
        import hashlib
        query_hash = hashlib.md5(
            f"{collection_name}:{str(query_vectors)}:{top_k}:{filter_expr}".encode()
        ).hexdigest()
        cache_key = f"vector_search:{query_hash}"
        
        # å°è¯•ä»ç¼“å­˜è·å–
        cached_result = self.result_cache.get(cache_key)
        if cached_result is not None:
            logger.debug(f"å‘é‡æ£€ç´¢ç¼“å­˜å‘½ä¸­: {cache_key[:16]}...")
            return cached_result
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå®é™…æŸ¥è¯¢
        results = super().search_vectors(collection_name, query_vectors, top_k, filter_expr, **kwargs)
        
        # å†™å…¥ç¼“å­˜
        if results:
            self.result_cache.set(cache_key, results, l2_ttl=3600)  # 1å°æ—¶
        
        return results
```

---

## æ€§èƒ½å¯¹æ¯”

### 5.1 ç¼“å­˜æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æå‡ |
|------|--------|--------|------|
| **L1å‘½ä¸­** (å†…å­˜) | N/A | **0.5ms** | æ–°å¢èƒ½åŠ› |
| **L2å‘½ä¸­** (Redis) | ~5ms | ~5ms | æŒå¹³ |
| **L1å‘½ä¸­ç‡** | 0% | **60-80%** | - |
| **æ€»ä½“å¹³å‡å“åº”** | ~50ms | **<5ms** | **90%â†“** |

### 5.2 å‘é‡æ£€ç´¢æ€§èƒ½å¯¹æ¯”

| å‚æ•°é…ç½® | å¬å›ç‡@10 | æ£€ç´¢é€Ÿåº¦ | å†…å­˜å ç”¨ |
|---------|----------|---------|---------|
| **ä¼˜åŒ–å‰** M=16, ef=200 | 85% | ~200ms | åŸºçº¿ |
| **ä¼˜åŒ–å** M=32, ef=400 | **95%** | ~150ms | +20% |
| **+ç¼“å­˜** | 95% | **<10ms** (ç¼“å­˜å‘½ä¸­) | +25% |

### 5.3 Javaåˆ†æå™¨å¯¹æ¯”

| åŠŸèƒ½ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å |
|------|--------|--------|
| **Importå…³ç³»åˆ†æ** | âŒ ä¸æ”¯æŒ | âœ… å®Œæ•´æ”¯æŒ |
| **ä¾èµ–å…³ç³»å›¾** | âŒ | âœ… è‡ªåŠ¨ç”Ÿæˆ |
| **å¾ªç¯ä¾èµ–æ£€æµ‹** | âŒ | âœ… æ”¯æŒ |
| **å½±å“åˆ†æ** | âŒ | âœ… æ”¯æŒ |
| **æœªä½¿ç”¨importæ£€æµ‹** | âŒ | âœ… æ”¯æŒ |

---

## å®æ–½è®¡åˆ’

### Phase 1: Javaåˆ†æå™¨ä¼˜åŒ– (1å¤©)

- [x] å®ç°`_process_import()`æ–¹æ³•
- [ ] å®ç°`_analyze_import_usage()`
- [ ] å®ç°`build_dependency_graph()`
- [ ] å®ç°`detect_circular_dependencies()`
- [ ] å®ç°`analyze_impact()`
- [ ] å•å…ƒæµ‹è¯• (>85%è¦†ç›–ç‡)

### Phase 2: å¤šå±‚ç¼“å­˜å®ç° (1å¤©)

- [ ] å®ç°`LRUCache`ç±»
- [ ] å®ç°`MultiLevelCache`ç±»
- [ ] é›†æˆåˆ°`RedisClient`
- [ ] ç¼“å­˜é¢„çƒ­åŠŸèƒ½
- [ ] å•å…ƒæµ‹è¯•

### Phase 3: Milvusä¼˜åŒ– (åŠå¤©)

- [ ] æ›´æ–°HNSWå‚æ•° (M=32, efConstruction=400)
- [ ] å®ç°Collectionè¿æ¥æ± 
- [ ] å®ç°åŠ¨æ€efSearchè°ƒæ•´
- [ ] å®ç°`CachedVectorDBClient`
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

### Phase 4: é›†æˆæµ‹è¯•ä¸æ–‡æ¡£ (åŠå¤©)

- [ ] ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•
- [ ] æ€§èƒ½å¯¹æ¯”æµ‹è¯•
- [ ] æ›´æ–°APIæ–‡æ¡£
- [ ] åˆ›å»ºä½¿ç”¨æŒ‡å—

**æ€»é¢„è®¡æ—¶é—´**: 3å¤©

---

**æ–‡æ¡£çŠ¶æ€**: âœ… è®¾è®¡å®Œæˆ  
**ä¸‹ä¸€æ­¥**: å¼€å§‹Phase 1å®æ–½  
**ç»´æŠ¤è€…**: MCP Enterprise Team  
**åˆ›å»ºæ—¶é—´**: 2025-11-20

