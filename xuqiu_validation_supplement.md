# MCPé¡¹ç›®éœ€æ±‚æ–‡æ¡£ - éªŒè¯è¡¥å……æ–¹æ¡ˆ

> **æ–‡æ¡£ç›®çš„**: é’ˆå¯¹æ ¸å¿ƒæŒ‡æ ‡éªŒè¯ã€æŠ€æœ¯æ ˆå…¼å®¹æ€§ã€æ€§èƒ½è¾¹ç•Œã€å®‰å…¨å®¡è®¡ç­‰å…³é”®è½åœ°é—®é¢˜ï¼Œæä¾›å¯æ‰§è¡Œçš„éªŒè¯æ–¹æ¡ˆå’ŒåŸºå‡†æ•°æ®

---

## ğŸ“Š ä¸€ã€æ ¸å¿ƒæŒ‡æ ‡éªŒè¯æ–¹æ¡ˆ

### 1.1 åŸºå‡†æµ‹è¯•æ•°æ®é›†æ„å»º

#### 1.1.1 è®°å¿†å‡†ç¡®ç‡æµ‹è¯•æ•°æ®é›†
```python
# tests/benchmark/memory_accuracy_dataset.py
"""
è®°å¿†å‡†ç¡®ç‡åŸºå‡†æµ‹è¯•æ•°æ®é›†
åŒ…å«5ä¸ªçœŸå®ä¸šåŠ¡åœºæ™¯ï¼Œå…±100ç»„å¯¹è¯æ•°æ®
"""

BENCHMARK_SCENARIOS = {
    "scenario_1_django_project": {
        "description": "Django Webé¡¹ç›®å¼€å‘åœºæ™¯",
        "conversations": [
            {
                "session_id": "sess_001",
                "rounds": [
                    {"user": "é¡¹ç›®ä½¿ç”¨ä»€ä¹ˆæ¡†æ¶?", "expected_memory": "Django 4.2"},
                    {"user": "æ•°æ®åº“æ˜¯ä»€ä¹ˆ?", "expected_memory": "PostgreSQL 15"},
                    # ... è·¨ä¼šè¯åï¼ˆ24å°æ—¶åï¼‰
                    {"user": "æé†’æˆ‘é¡¹ç›®çš„æ¡†æ¶ç‰ˆæœ¬", "expected_recall": "Django 4.2", "memory_level": "mid"}
                ],
                "metrics": {
                    "cross_session_recall_required": True,
                    "min_accuracy": 0.95
                }
            }
        ]
    },

    "scenario_2_api_development": {
        "description": "RESTful APIå¼€å‘åœºæ™¯",
        "conversations": [
            {
                "session_id": "sess_002",
                "rounds": [
                    {"user": "ç”¨æˆ·ç™»å½•æ¥å£è·¯å¾„æ˜¯ä»€ä¹ˆ?", "expected_memory": "/api/v1/auth/login"},
                    {"user": "éœ€è¦å“ªäº›å‚æ•°?", "expected_memory": "username, password, captcha"},
                    # 30åˆ†é’ŸååŒä¸€ä¼šè¯
                    {"user": "ç™»å½•æ¥å£çš„å®Œæ•´ä¿¡æ¯", "expected_recall": "/api/v1/auth/login (POST) å‚æ•°: username, password, captcha"}
                ]
            }
        ]
    },

    "scenario_3_code_refactoring": {
        "description": "ä»£ç é‡æ„åœºæ™¯ï¼ˆæµ‹è¯•è®°å¿†æ›´æ–°èƒ½åŠ›ï¼‰",
        "conversations": [
            {
                "session_id": "sess_003",
                "rounds": [
                    {"user": "ç”¨æˆ·æœåŠ¡çš„è®¤è¯æ–¹å¼", "expected_memory": "JWT Token"},
                    # è®°å¿†æ›´æ–°
                    {"user": "è®¤è¯æ–¹å¼æ”¹ä¸ºOAuth2", "action": "update_memory", "new_value": "OAuth2"},
                    # éªŒè¯æ›´æ–°
                    {"user": "å½“å‰è®¤è¯æ–¹å¼æ˜¯ä»€ä¹ˆ?", "expected_recall": "OAuth2", "should_not_recall": "JWT Token"}
                ],
                "metrics": {
                    "conflict_resolution_required": True,
                    "update_accuracy": 1.0
                }
            }
        ]
    },

    "scenario_4_multi_project": {
        "description": "å¤šé¡¹ç›®éš”ç¦»åœºæ™¯",
        "conversations": [
            {
                "session_id": "sess_004_proj_a",
                "project_id": "proj_a",
                "rounds": [
                    {"user": "é¡¹ç›®Aä½¿ç”¨Python 3.10", "expected_memory": "Python 3.10"}
                ]
            },
            {
                "session_id": "sess_005_proj_b",
                "project_id": "proj_b",
                "rounds": [
                    {"user": "é¡¹ç›®Bä½¿ç”¨Python 3.8", "expected_memory": "Python 3.8"},
                    # éªŒè¯é¡¹ç›®éš”ç¦»
                    {"user": "é¡¹ç›®Açš„Pythonç‰ˆæœ¬", "expected_recall": "æ— ç›¸å…³è®°å¿†ï¼ˆè·¨é¡¹ç›®æŸ¥è¯¢åº”è¢«é˜»æ­¢ï¼‰"}
                ]
            }
        ]
    },

    "scenario_5_complex_context": {
        "description": "å¤æ‚ä¸Šä¸‹æ–‡åœºæ™¯ï¼ˆæµ‹è¯•é•¿æœŸè®°å¿†ï¼‰",
        "conversations": [
            {
                "session_id": "sess_006",
                "rounds": [
                    {"user": "æ•°æ®åº“åˆ†åº“åˆ†è¡¨ç­–ç•¥ï¼šç”¨æˆ·è¡¨æŒ‰user_idå–æ¨¡8ï¼Œè®¢å•è¡¨æŒ‰æ—¥æœŸåˆ†è¡¨",
                     "expected_memory": "ç”¨æˆ·è¡¨: user_id % 8; è®¢å•è¡¨: æŒ‰æ—¥æœŸåˆ†è¡¨"},
                    # 7å¤©å
                    {"user": "ç”¨æˆ·è¡¨çš„åˆ†åº“è§„åˆ™", "expected_recall": "user_idå–æ¨¡8", "memory_level": "long"}
                ]
            }
        ]
    }
}
```

#### 1.1.2 è‡ªåŠ¨åŒ–éªŒè¯è„šæœ¬
```python
# tests/benchmark/validate_memory_accuracy.py
import asyncio
from typing import Dict, List
from datetime import datetime, timedelta

class MemoryAccuracyValidator:
    """è®°å¿†å‡†ç¡®ç‡è‡ªåŠ¨éªŒè¯å™¨"""

    def __init__(self, memory_service, config):
        self.memory_service = memory_service
        self.config = config
        self.results = []

    async def run_benchmark(self, scenarios: Dict) -> Dict:
        """è¿è¡ŒåŸºå‡†æµ‹è¯•"""
        total_tests = 0
        passed_tests = 0

        for scenario_name, scenario_data in scenarios.items():
            print(f"\nâ–¶ è¿è¡Œåœºæ™¯: {scenario_data['description']}")

            for conversation in scenario_data["conversations"]:
                result = await self._test_conversation(conversation)
                total_tests += result["total"]
                passed_tests += result["passed"]
                self.results.append({
                    "scenario": scenario_name,
                    "result": result
                })

        accuracy = passed_tests / total_tests if total_tests > 0 else 0

        return {
            "overall_accuracy": accuracy,
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": total_tests - passed_tests,
            "meets_requirement": accuracy >= 0.95,
            "detailed_results": self.results
        }

    async def _test_conversation(self, conversation: Dict) -> Dict:
        """æµ‹è¯•å•ä¸ªå¯¹è¯"""
        session_id = conversation["session_id"]
        project_id = conversation.get("project_id", "default_project")
        total = 0
        passed = 0

        for round_data in conversation["rounds"]:
            total += 1

            if "expected_memory" in round_data:
                # æµ‹è¯•è®°å¿†å­˜å‚¨
                self.memory_service.store_memory(
                    project_id=project_id,
                    content=round_data["user"],
                    memory_level="mid"
                )
                passed += 1  # å­˜å‚¨æˆåŠŸ

            elif "expected_recall" in round_data:
                # æµ‹è¯•è®°å¿†æ£€ç´¢
                result = self.memory_service.retrieve_memory(
                    project_id=project_id,
                    query=round_data["user"],
                    top_k=3
                )

                # éªŒè¯å¬å›å†…å®¹
                recalled_contents = [m["content"] for m in result["memories"]]
                if any(round_data["expected_recall"] in content for content in recalled_contents):
                    passed += 1
                    print(f"  âœ“ å¬å›æˆåŠŸ: {round_data['user'][:30]}...")
                else:
                    print(f"  âœ— å¬å›å¤±è´¥: æœŸæœ› '{round_data['expected_recall']}', å®é™… {recalled_contents}")

                # éªŒè¯ä¸åº”å¬å›çš„å†…å®¹
                if "should_not_recall" in round_data:
                    if not any(round_data["should_not_recall"] in content for content in recalled_contents):
                        print(f"  âœ“ æ­£ç¡®è¿‡æ»¤: {round_data['should_not_recall']}")
                    else:
                        passed -= 1
                        print(f"  âœ— é”™è¯¯å¬å›å·²æ›´æ–°çš„æ—§è®°å¿†")

        return {"total": total, "passed": passed}


# è¿è¡Œç¤ºä¾‹
async def main():
    from src.mcp_core.memory.service import MemoryService
    from src.mcp_core.common.config import load_config

    config = load_config()
    memory_service = MemoryService(config)
    validator = MemoryAccuracyValidator(memory_service, config)

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    report = await validator.run_benchmark(BENCHMARK_SCENARIOS)

    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“Š è®°å¿†å‡†ç¡®ç‡åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
    print("="*60)
    print(f"æ€»æµ‹è¯•æ•°: {report['total_tests']}")
    print(f"é€šè¿‡æ•°: {report['passed_tests']}")
    print(f"å¤±è´¥æ•°: {report['failed_tests']}")
    print(f"å‡†ç¡®ç‡: {report['overall_accuracy']:.2%}")
    print(f"æ˜¯å¦è¾¾æ ‡(â‰¥95%): {'âœ“ æ˜¯' if report['meets_requirement'] else 'âœ— å¦'}")

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    import json
    with open("benchmark_report.json", "w") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)

if __name__ == "__main__":
    asyncio.run(main())
```

### 1.2 Tokenä¼˜åŒ–å¯¹æ¯”æµ‹è¯•

```python
# tests/benchmark/validate_token_optimization.py
"""Tokenæ¶ˆè€—å¯¹æ¯”æµ‹è¯•ï¼ˆä¿®æ”¹å‰åéªŒè¯ï¼‰"""

class TokenOptimizationValidator:

    TEST_CASES = [
        {
            "name": "ç®€å•æŸ¥è¯¢",
            "content": "å¦‚ä½•ä½¿ç”¨Djangoåˆ›å»ºä¸€ä¸ªç®€å•çš„ç”¨æˆ·æ¨¡å‹ï¼Ÿ",
            "expected_original_tokens": 512,
            "expected_compressed_tokens": 64,
            "min_compression_rate": 0.875  # 87.5%
        },
        {
            "name": "å¤æ‚ä»£ç ç‰‡æ®µ",
            "content": """
            class UserManager(models.Manager):
                def create_user(self, username, email, password):
                    user = self.model(username=username, email=email)
                    user.set_password(password)
                    user.save(using=self._db)
                    return user
            """,
            "expected_original_tokens": 1024,
            "expected_compressed_tokens": 128,
            "min_compression_rate": 0.875
        },
        {
            "name": "é•¿æ–‡æ¡£",
            "content": "..." * 2000,  # æ¨¡æ‹Ÿé•¿æ–‡æ¡£
            "expected_original_tokens": 8192,
            "expected_compressed_tokens": 1024,
            "min_compression_rate": 0.875
        }
    ]

    def __init__(self, token_service):
        self.token_service = token_service

    def validate(self, baseline_report: Dict = None) -> Dict:
        """éªŒè¯Tokenä¼˜åŒ–æ•ˆæœï¼ˆå¯å¯¹æ¯”åŸºçº¿ï¼‰"""
        results = []

        for test_case in self.TEST_CASES:
            result = self.token_service.compress_content(
                content=test_case["content"],
                content_type="code" if "class" in test_case["content"] else "text"
            )

            compression_rate = result["compression_rate"]
            meets_requirement = compression_rate >= test_case["min_compression_rate"]

            test_result = {
                "name": test_case["name"],
                "original_tokens": result["original_tokens"],
                "compressed_tokens": result["compressed_tokens"],
                "compression_rate": compression_rate,
                "meets_requirement": meets_requirement
            }

            # å¦‚æœæä¾›äº†åŸºçº¿ï¼Œè¿›è¡Œå¯¹æ¯”
            if baseline_report:
                baseline_case = next(
                    (c for c in baseline_report["results"] if c["name"] == test_case["name"]),
                    None
                )
                if baseline_case:
                    test_result["baseline_comparison"] = {
                        "baseline_rate": baseline_case["compression_rate"],
                        "current_rate": compression_rate,
                        "regression": compression_rate < baseline_case["compression_rate"]
                    }

            results.append(test_result)

        avg_compression = sum(r["compression_rate"] for r in results) / len(results)

        return {
            "average_compression_rate": avg_compression,
            "meets_90_percent_target": avg_compression >= 0.90,
            "results": results
        }
```

### 1.3 å¹»è§‰æŠ‘åˆ¶è¾¹ç¼˜æ¡ˆä¾‹æµ‹è¯•

```python
# tests/benchmark/hallucination_edge_cases.py
"""å¹»è§‰æ£€æµ‹è¾¹ç¼˜æ¡ˆä¾‹æµ‹è¯•é›†"""

EDGE_CASE_DATASET = {
    "æ¨¡ç³ŠæŸ¥è¯¢åœºæ™¯": [
        {
            "stored_memory": "é¡¹ç›®ä½¿ç”¨Django 4.2æ¡†æ¶",
            "query": "æˆ‘ä»¬ç”¨çš„æ˜¯ä»€ä¹ˆPythonæ¡†æ¶ï¼Ÿ",
            "expected_output": "Django",
            "expected_hallucination": False,
            "reason": "è¯­ä¹‰ç›¸ä¼¼ä½†è¡¨è¿°ä¸åŒ"
        },
        {
            "stored_memory": "æ•°æ®åº“é‡‡ç”¨PostgreSQL",
            "query": "MySQLé…ç½®åœ¨å“ªé‡Œï¼Ÿ",
            "expected_output": "é¡¹ç›®ä½¿ç”¨PostgreSQLï¼Œä¸æ˜¯MySQL",
            "expected_hallucination": False,  # åº”è¯¥çº æ­£ç”¨æˆ·é”™è¯¯
            "reason": "ç”¨æˆ·æ··æ·†æ•°æ®åº“ç±»å‹"
        }
    ],

    "è·¨é¢†åŸŸçŸ¥è¯†æ··æ·†": [
        {
            "stored_memory": "ç”¨æˆ·è®¤è¯ä½¿ç”¨JWT Token",
            "query": "OAuth2çš„é…ç½®åœ¨å“ªï¼Ÿ",
            "expected_hallucination": True,
            "reason": "é¡¹ç›®æœªä½¿ç”¨OAuth2ï¼Œä¸åº”ç¼–é€ é…ç½®"
        },
        {
            "stored_memory": "å‰ç«¯ä½¿ç”¨React 18",
            "query": "Vueç»„ä»¶æ€ä¹ˆå†™ï¼Ÿ",
            "expected_hallucination": True,
            "reason": "é¡¹ç›®æœªä½¿ç”¨Vueï¼Œä¸åº”ç”ŸæˆVueä»£ç "
        }
    ],

    "ç‰ˆæœ¬å·ç»†èŠ‚": [
        {
            "stored_memory": "Python 3.10.5",
            "query": "Pythonç‰ˆæœ¬æ˜¯3.9å—ï¼Ÿ",
            "expected_output": "ä¸æ˜¯ï¼Œé¡¹ç›®ä½¿ç”¨Python 3.10.5",
            "expected_hallucination": False
        }
    ],

    "ä¸å­˜åœ¨çš„åŠŸèƒ½": [
        {
            "stored_memory": "é¡¹ç›®åŒ…å«ç”¨æˆ·ç®¡ç†ã€æƒé™ç®¡ç†æ¨¡å—",
            "query": "æ”¯ä»˜æ¨¡å—çš„APIåœ¨å“ªï¼Ÿ",
            "expected_hallucination": True,
            "reason": "é¡¹ç›®ä¸­ä¸å­˜åœ¨æ”¯ä»˜æ¨¡å—"
        }
    ],

    "æ—¶é—´æ•æ„Ÿä¿¡æ¯": [
        {
            "stored_memory": "2025-01-18: éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ",
            "query": "ä»€ä¹ˆæ—¶å€™ä¸Šçº¿çš„ï¼Ÿ",
            "expected_output": "2025å¹´1æœˆ18æ—¥",
            "expected_hallucination": False
        }
    ]
}

class HallucinationEdgeCaseValidator:

    def __init__(self, validation_service, memory_service):
        self.validation_service = validation_service
        self.memory_service = memory_service

    def run_edge_case_tests(self) -> Dict:
        """è¿è¡Œè¾¹ç¼˜æ¡ˆä¾‹æµ‹è¯•"""
        results = {
            "total_cases": 0,
            "correct_detections": 0,
            "false_positives": 0,  # è¯¯åˆ¤ä¸ºå¹»è§‰
            "false_negatives": 0,  # æ¼åˆ¤å¹»è§‰
            "details": []
        }

        for category, cases in EDGE_CASE_DATASET.items():
            print(f"\nâ–¶ æµ‹è¯•ç±»åˆ«: {category}")

            for case in cases:
                results["total_cases"] += 1

                # å­˜å‚¨è®°å¿†
                project_id = f"test_edge_{results['total_cases']}"
                self.memory_service.store_memory(
                    project_id=project_id,
                    content=case["stored_memory"],
                    memory_level="mid"
                )

                # ç”Ÿæˆæ¨¡æ‹Ÿè¾“å‡ºï¼ˆå®é™…åº”è°ƒç”¨LLMï¼‰
                simulated_output = case.get("expected_output", case["query"])

                # æ£€æµ‹å¹»è§‰
                detection_result = self.validation_service.detect_hallucination(
                    project_id=project_id,
                    output=simulated_output
                )

                is_correct = detection_result["is_hallucination"] == case["expected_hallucination"]

                if is_correct:
                    results["correct_detections"] += 1
                    status = "âœ“"
                elif case["expected_hallucination"] and not detection_result["is_hallucination"]:
                    results["false_negatives"] += 1
                    status = "âœ— æ¼åˆ¤"
                else:
                    results["false_positives"] += 1
                    status = "âœ— è¯¯åˆ¤"

                print(f"  {status} {case['reason']}")

                results["details"].append({
                    "category": category,
                    "case": case,
                    "detection": detection_result,
                    "is_correct": is_correct
                })

        # è®¡ç®—æŒ‡æ ‡
        results["accuracy"] = results["correct_detections"] / results["total_cases"]
        results["precision"] = 1 - (results["false_positives"] / results["total_cases"])
        results["recall"] = 1 - (results["false_negatives"] / results["total_cases"])
        results["meets_requirement"] = results["accuracy"] >= 0.95

        return results
```

---

## ğŸ”§ äºŒã€æŠ€æœ¯æ ˆå…¼å®¹æ€§éªŒè¯æ¸…å•

### 2.1 ä¾èµ–å‡çº§éªŒè¯çŸ©é˜µ

```python
# scripts/validate_dependencies.py
"""ä¾èµ–å…¼å®¹æ€§è‡ªåŠ¨éªŒè¯è„šæœ¬"""

CRITICAL_DEPENDENCIES = {
    "pydantic": {
        "current_version": "2.5.0",
        "compatible_range": ">=2.0.0,<3.0.0",
        "breaking_changes_in_v2": [
            "Configç±»æ”¹ä¸ºmodel_config",
            "validatoræ”¹ä¸ºfield_validator",
            "__root__æ¨¡å‹åºŸå¼ƒ"
        ],
        "validation_tests": [
            "tests/test_pydantic_models.py::test_memory_request_validation",
            "tests/test_pydantic_models.py::test_config_loading"
        ]
    },

    "sentence-transformers": {
        "current_version": "2.2.2",
        "compatible_range": ">=2.0.0,<3.0.0",
        "critical_check": "embedding_dimension_stability",
        "validation_tests": [
            "tests/test_embedding_dimension.py::test_vector_dimension_unchanged"
        ]
    },

    "transformers": {
        "current_version": "4.36.0",
        "compatible_range": ">=4.30.0,<5.0.0",
        "model_compatibility": {
            "codebert": "microsoft/codebert-base",
            "required_files": ["config.json", "pytorch_model.bin"]
        }
    },

    "sqlalchemy": {
        "current_version": "2.0.23",
        "compatible_range": ">=2.0.0",
        "breaking_changes_from_v1": [
            "Session.query()æ”¹ä¸ºSession.execute(select())",
            "declarative_baseæ”¹ä¸ºDeclarativeBase"
        ],
        "migration_guide": "docs/sqlalchemy_v2_migration.md"
    }
}

def validate_dependency_compatibility():
    """éªŒè¯ä¾èµ–å…¼å®¹æ€§"""
    import importlib.metadata
    import subprocess

    report = {"passed": [], "failed": [], "warnings": []}

    for package, config in CRITICAL_DEPENDENCIES.items():
        try:
            installed_version = importlib.metadata.version(package)
            print(f"\næ£€æŸ¥ {package}: å½“å‰ç‰ˆæœ¬ {installed_version}")

            # è¿è¡Œç‰ˆæœ¬ç‰¹å®šçš„æµ‹è¯•
            if "validation_tests" in config:
                for test_path in config["validation_tests"]:
                    result = subprocess.run(
                        ["pytest", test_path, "-v"],
                        capture_output=True,
                        text=True
                    )
                    if result.returncode == 0:
                        report["passed"].append(f"{package}: {test_path}")
                        print(f"  âœ“ æµ‹è¯•é€šè¿‡: {test_path}")
                    else:
                        report["failed"].append({
                            "package": package,
                            "test": test_path,
                            "error": result.stderr
                        })
                        print(f"  âœ— æµ‹è¯•å¤±è´¥: {test_path}")

            # ç‰¹æ®Šæ£€æŸ¥
            if package == "sentence-transformers" and config.get("critical_check") == "embedding_dimension_stability":
                # éªŒè¯åµŒå…¥ç»´åº¦æœªæ”¹å˜
                from sentence_transformers import SentenceTransformer
                model = SentenceTransformer('all-MiniLM-L6-v2')
                test_embedding = model.encode("æµ‹è¯•æ–‡æœ¬")
                expected_dim = 384

                if len(test_embedding) == expected_dim:
                    print(f"  âœ“ åµŒå…¥ç»´åº¦ç¨³å®š: {expected_dim}")
                else:
                    report["failed"].append({
                        "package": package,
                        "error": f"åµŒå…¥ç»´åº¦å˜åŒ–: æœŸæœ›{expected_dim}, å®é™…{len(test_embedding)}"
                    })

        except Exception as e:
            report["failed"].append({"package": package, "error": str(e)})

    return report
```

### 2.2 å‘é‡ç»´åº¦è¿ç§»æ–¹æ¡ˆ

```python
# scripts/migrate_vector_dimension.py
"""
å½“sentence-transformerså‡çº§å¯¼è‡´åµŒå…¥ç»´åº¦å˜åŒ–æ—¶çš„è¿ç§»æ–¹æ¡ˆ
"""

def migrate_milvus_collection_if_dimension_changed():
    """æ£€æµ‹å¹¶è¿ç§»Milvus Collectionï¼ˆå¦‚æœç»´åº¦å˜åŒ–ï¼‰"""
    from pymilvus import connections, Collection, utility
    from sentence_transformers import SentenceTransformer

    # 1. æ£€æµ‹å½“å‰æ¨¡å‹çš„åµŒå…¥ç»´åº¦
    model = SentenceTransformer('all-MiniLM-L6-v2')
    current_dim = model.get_sentence_embedding_dimension()

    # 2. æ£€æŸ¥Milvusä¸­ç°æœ‰Collectionçš„ç»´åº¦
    connections.connect("default", host="localhost", port="19530")
    collection_name = "mid_term_memories"

    if utility.has_collection(collection_name):
        collection = Collection(collection_name)
        schema = collection.schema
        embedding_field = next(f for f in schema.fields if f.name == "embedding")
        stored_dim = embedding_field.params["dim"]

        if stored_dim != current_dim:
            print(f"âš ï¸  æ£€æµ‹åˆ°ç»´åº¦å˜åŒ–: {stored_dim} -> {current_dim}")
            print("å¼€å§‹è¿ç§»æµç¨‹...")

            # 3. åˆ›å»ºæ–°Collection
            new_collection_name = f"{collection_name}_v{current_dim}"
            # ... åˆ›å»ºæ–°Schema

            # 4. é‡æ–°ç”Ÿæˆæ‰€æœ‰åµŒå…¥
            old_data = collection.query(expr="id >= 0", output_fields=["*"])
            for item in old_data:
                new_embedding = model.encode(item["content"])
                # æ’å…¥åˆ°æ–°Collection

            # 5. åŸå­åˆ‡æ¢
            utility.rename_collection(collection_name, f"{collection_name}_backup")
            utility.rename_collection(new_collection_name, collection_name)

            print("âœ“ è¿ç§»å®Œæˆ")
        else:
            print(f"âœ“ ç»´åº¦ä¸€è‡´: {current_dim}")
```

---

## âš¡ ä¸‰ã€æ€§èƒ½å‹æµ‹ä¸è¾¹ç•ŒéªŒè¯

### 3.1 å¢å¼ºç‰ˆLocustå‹æµ‹è„šæœ¬

```python
# tests/performance/advanced_load_test.py
"""å¢å¼ºç‰ˆæ€§èƒ½å‹æµ‹ï¼ˆåŒ…å«è¾¹ç•Œåœºæ™¯ï¼‰"""
from locust import HttpUser, task, between, events
import random
import json

class AdvancedMCPLoadTest(HttpUser):
    wait_time = between(1, 3)

    def on_start(self):
        """åˆå§‹åŒ–æµ‹è¯•æ•°æ®"""
        self.project_ids = [f"proj_{i:03d}" for i in range(10)]
        self.long_content = "æµ‹è¯•å†…å®¹" * 1000  # 4000å­—é•¿æ–‡æœ¬

    @task(5)
    def retrieve_memory_normal(self):
        """æ­£å¸¸æ£€ç´¢ï¼ˆæƒé‡5ï¼‰"""
        project_id = random.choice(self.project_ids)
        with self.client.get(
            "/api/v1/memory/retrieve",
            params={"project_id": project_id, "query": "æµ‹è¯•æŸ¥è¯¢", "top_k": 5},
            catch_response=True
        ) as response:
            if response.elapsed.total_seconds() > 0.3:
                response.failure(f"å“åº”æ—¶é—´è¶…æ ‡: {response.elapsed.total_seconds()}s")

    @task(2)
    def store_memory_normal(self):
        """æ­£å¸¸å­˜å‚¨ï¼ˆæƒé‡2ï¼‰"""
        self.client.post(
            "/api/v1/memory/store",
            json={
                "project_id": random.choice(self.project_ids),
                "content": "æ­£å¸¸é•¿åº¦çš„æµ‹è¯•æ•°æ®",
                "memory_level": "mid"
            }
        )

    @task(1)
    def store_large_content(self):
        """è¾¹ç•Œæµ‹è¯•ï¼šå¤§å†…å®¹å­˜å‚¨ï¼ˆæƒé‡1ï¼‰"""
        with self.client.post(
            "/api/v1/memory/store",
            json={
                "project_id": "proj_stress",
                "content": self.long_content,
                "memory_level": "mid"
            },
            catch_response=True
        ) as response:
            if response.status_code != 200:
                response.failure(f"å¤§å†…å®¹å­˜å‚¨å¤±è´¥: {response.text}")

    @task(1)
    def concurrent_same_project(self):
        """è¾¹ç•Œæµ‹è¯•ï¼šåŒä¸€é¡¹ç›®é«˜å¹¶å‘ï¼ˆæƒé‡1ï¼‰"""
        self.client.get(
            "/api/v1/memory/retrieve",
            params={"project_id": "proj_001", "query": f"å¹¶å‘æµ‹è¯•{random.randint(1, 100)}", "top_k": 10}
        )

    @task(1)
    def test_hallucination_detection(self):
        """å¹»è§‰æ£€æµ‹æ€§èƒ½æµ‹è¯•"""
        self.client.post(
            "/api/v1/validate/hallucination",
            json={
                "project_id": random.choice(self.project_ids),
                "output": "è¿™æ˜¯ä¸€æ®µæµ‹è¯•è¾“å‡ºå†…å®¹ï¼Œç”¨äºéªŒè¯å¹»è§‰æ£€æµ‹çš„æ€§èƒ½",
                "threshold": 0.65
            }
        )

# æ€§èƒ½æŒ‡æ ‡æ”¶é›†
@events.test_stop.add_listener
def on_test_stop(environment, **kwargs):
    """æµ‹è¯•ç»“æŸåç”ŸæˆæŠ¥å‘Š"""
    stats = environment.stats

    # è®¡ç®—P95å»¶è¿Ÿ
    for stat in stats.entries.values():
        p95_latency = stat.get_response_time_percentile(0.95)

        report = {
            "endpoint": stat.name,
            "total_requests": stat.num_requests,
            "failures": stat.num_failures,
            "avg_response_time": stat.avg_response_time,
            "p95_response_time": p95_latency,
            "requests_per_second": stat.total_rps,
            "meets_300ms_requirement": p95_latency <= 300
        }

        print(f"\n{'='*60}")
        print(f"ç«¯ç‚¹: {report['endpoint']}")
        print(f"æ€»è¯·æ±‚æ•°: {report['total_requests']}")
        print(f"å¤±è´¥æ•°: {report['failures']}")
        print(f"å¹³å‡å“åº”æ—¶é—´: {report['avg_response_time']:.2f}ms")
        print(f"P95å“åº”æ—¶é—´: {report['p95_response_time']:.2f}ms")
        print(f"QPS: {report['requests_per_second']:.2f}")
        print(f"æ˜¯å¦è¾¾æ ‡(<300ms): {'âœ“' if report['meets_300ms_requirement'] else 'âœ—'}")

        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        with open(f"perf_report_{stat.name.replace('/', '_')}.json", "w") as f:
            json.dump(report, f, indent=2)
```

### 3.2 èµ„æºç“¶é¢ˆè¯Šæ–­è„šæœ¬

```python
# scripts/diagnose_performance_bottleneck.py
"""æ€§èƒ½ç“¶é¢ˆè‡ªåŠ¨è¯Šæ–­"""
import psutil
import time
from pymilvus import connections, Collection

def diagnose_bottlenecks():
    """è¯Šæ–­ç³»ç»Ÿç“¶é¢ˆ"""
    report = {"timestamp": time.time(), "bottlenecks": []}

    # 1. æ•°æ®åº“è¿æ¥æ± æ£€æŸ¥
    from sqlalchemy import create_engine
    engine = create_engine("postgresql://...")
    pool_status = engine.pool.status()

    if "overflow" in pool_status:
        overflow_count = int(pool_status.split("overflow=")[1].split()[0])
        if overflow_count > 5:
            report["bottlenecks"].append({
                "type": "database_pool_overflow",
                "severity": "high",
                "detail": f"è¿æ¥æ± æº¢å‡º{overflow_count}æ¬¡",
                "solution": "å¢åŠ pool_sizeæˆ–max_overflowé…ç½®"
            })

    # 2. Redisè¿æ¥æ•°æ£€æŸ¥
    import redis
    r = redis.Redis()
    client_count = len(r.client_list())

    if client_count > 80:  # å‡è®¾max_connections=100
        report["bottlenecks"].append({
            "type": "redis_connection_high",
            "severity": "medium",
            "detail": f"Redisè¿æ¥æ•°: {client_count}/100",
            "solution": "æ£€æŸ¥è¿æ¥æ³„æ¼æˆ–å¢åŠ max_connections"
        })

    # 3. Milvusç´¢å¼•æ•ˆç‡æ£€æŸ¥
    connections.connect("default", host="localhost", port="19530")
    collection = Collection("mid_term_memories")

    # æ‰§è¡Œæµ‹è¯•æŸ¥è¯¢å¹¶è®¡æ—¶
    import numpy as np
    test_vector = np.random.rand(768).tolist()

    start = time.time()
    collection.search(
        data=[test_vector],
        anns_field="embedding",
        param={"metric_type": "COSINE", "params": {"ef": 64}},
        limit=10
    )
    query_time = (time.time() - start) * 1000

    if query_time > 100:  # å‘é‡æ£€ç´¢åº”<100ms
        report["bottlenecks"].append({
            "type": "milvus_slow_query",
            "severity": "high",
            "detail": f"å‘é‡æ£€ç´¢è€—æ—¶{query_time:.2f}ms",
            "solution": "æ£€æŸ¥ç´¢å¼•ç±»å‹(æ¨èHNSW)æˆ–è°ƒæ•´efå‚æ•°"
        })

    # 4. CPU/å†…å­˜æ£€æŸ¥
    cpu_percent = psutil.cpu_percent(interval=1)
    memory_percent = psutil.virtual_memory().percent

    if cpu_percent > 80:
        report["bottlenecks"].append({
            "type": "high_cpu_usage",
            "severity": "high",
            "detail": f"CPUä½¿ç”¨ç‡: {cpu_percent}%"
        })

    if memory_percent > 85:
        report["bottlenecks"].append({
            "type": "high_memory_usage",
            "severity": "critical",
            "detail": f"å†…å­˜ä½¿ç”¨ç‡: {memory_percent}%"
        })

    return report
```

---

## ğŸ” å››ã€å®‰å…¨å®¡è®¡å¢å¼ºæ–¹æ¡ˆ

### 4.1 ç»†ç²’åº¦æƒé™æ§åˆ¶å®ç°

```python
# src/mcp_core/security/permission.py
"""ç»†ç²’åº¦æƒé™æ§åˆ¶"""
from enum import Enum
from typing import List

class Permission(str, Enum):
    # è®°å¿†æ“ä½œæƒé™
    MEMORY_READ = "memory:read"
    MEMORY_WRITE = "memory:write"
    MEMORY_UPDATE = "memory:update"
    MEMORY_DELETE = "memory:delete"

    # é¡¹ç›®ç®¡ç†æƒé™
    PROJECT_CREATE = "project:create"
    PROJECT_UPDATE = "project:update"
    PROJECT_DELETE = "project:delete"

    # ç”¨æˆ·ç®¡ç†æƒé™
    USER_INVITE = "user:invite"
    USER_REMOVE = "user:remove"

    # é…ç½®ç®¡ç†æƒé™
    CONFIG_READ = "config:read"
    CONFIG_WRITE = "config:write"

class Role:
    """è§’è‰²æƒé™æ˜ å°„"""
    ADMIN = [p for p in Permission]  # å…¨éƒ¨æƒé™

    DEVELOPER = [
        Permission.MEMORY_READ,
        Permission.MEMORY_WRITE,
        Permission.MEMORY_UPDATE,
        Permission.CONFIG_READ,
    ]

    VIEWER = [
        Permission.MEMORY_READ,
        Permission.CONFIG_READ,
    ]

# æ•°æ®åº“Schemaæ‰©å±•
"""
-- ç»†ç²’åº¦æƒé™è¡¨
CREATE TABLE user_permissions_v2 (
    id SERIAL PRIMARY KEY,
    user_id VARCHAR(64) NOT NULL,
    project_id VARCHAR(64) NOT NULL,
    permission VARCHAR(50) NOT NULL,  -- å­˜å‚¨Permissionæšä¸¾å€¼
    granted_by VARCHAR(64),
    granted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP,  -- å¯é€‰çš„æƒé™è¿‡æœŸæ—¶é—´
    UNIQUE(user_id, project_id, permission),
    FOREIGN KEY (project_id) REFERENCES projects(project_id)
);

CREATE INDEX idx_user_perm ON user_permissions_v2(user_id, project_id);
"""

# æƒé™æ£€æŸ¥è£…é¥°å™¨
from functools import wraps
from fastapi import HTTPException

def require_permission(permission: Permission):
    """æƒé™æ£€æŸ¥è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # ä»è¯·æ±‚ä¸­æå–user_idå’Œproject_id
            user_id = kwargs.get("current_user_id")
            project_id = kwargs.get("project_id")

            # æŸ¥è¯¢æƒé™
            has_permission = check_user_permission(user_id, project_id, permission)

            if not has_permission:
                raise HTTPException(
                    status_code=403,
                    detail=f"ç¼ºå°‘æƒé™: {permission.value}"
                )

            return await func(*args, **kwargs)
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@app.delete("/api/v1/memory/{memory_id}")
@require_permission(Permission.MEMORY_DELETE)
async def delete_memory(memory_id: str, current_user_id: str, project_id: str):
    """åˆ é™¤è®°å¿†ï¼ˆéœ€è¦MEMORY_DELETEæƒé™ï¼‰"""
    pass
```

### 4.2 å®Œå–„çš„å®¡è®¡æ—¥å¿—ç³»ç»Ÿ

```python
# src/mcp_core/security/audit.py
"""å®¡è®¡æ—¥å¿—å¢å¼º"""
import json
from datetime import datetime
from typing import Dict, Any

class AuditLogger:
    """å®¡è®¡æ—¥å¿—è®°å½•å™¨"""

    # æ•æ„Ÿæ“ä½œå®šä¹‰
    SENSITIVE_OPERATIONS = {
        "permission_grant": "æˆäºˆæƒé™",
        "permission_revoke": "æ’¤é”€æƒé™",
        "memory_delete": "åˆ é™¤è®°å¿†",
        "project_delete": "åˆ é™¤é¡¹ç›®",
        "config_update": "æ›´æ–°é…ç½®"
    }

    def __init__(self, db_session):
        self.db_session = db_session

    def log(
        self,
        user_id: str,
        action: str,
        resource_type: str,
        resource_id: str,
        project_id: str = None,
        details: Dict[str, Any] = None,
        ip_address: str = None
    ):
        """è®°å½•å®¡è®¡æ—¥å¿—"""
        from .models import AuditLog

        # æ ‡è®°æ•æ„Ÿæ“ä½œ
        is_sensitive = action in self.SENSITIVE_OPERATIONS

        log_entry = AuditLog(
            user_id=user_id,
            project_id=project_id,
            action=action,
            resource_type=resource_type,
            resource_id=resource_id,
            details=json.dumps(details or {}, ensure_ascii=False),
            ip_address=ip_address,
            is_sensitive=is_sensitive,
            created_at=datetime.now()
        )

        self.db_session.add(log_entry)
        self.db_session.commit()

        # æ•æ„Ÿæ“ä½œé¢å¤–å‘Šè­¦
        if is_sensitive:
            self._alert_sensitive_operation(log_entry)

    def _alert_sensitive_operation(self, log_entry):
        """æ•æ„Ÿæ“ä½œå‘Šè­¦"""
        # å‘é€åˆ°ç›‘æ§ç³»ç»Ÿ
        alert_message = (
            f"ğŸš¨ æ•æ„Ÿæ“ä½œ: {self.SENSITIVE_OPERATIONS.get(log_entry.action)}\n"
            f"ç”¨æˆ·: {log_entry.user_id}\n"
            f"èµ„æº: {log_entry.resource_type}/{log_entry.resource_id}\n"
            f"æ—¶é—´: {log_entry.created_at}\n"
            f"IP: {log_entry.ip_address}"
        )

        # è¿™é‡Œå¯ä»¥é›†æˆSlack/é’‰é’‰/é‚®ä»¶é€šçŸ¥
        print(alert_message)

    def query_user_actions(
        self,
        user_id: str,
        start_time: datetime,
        end_time: datetime
    ) -> List[Dict]:
        """æŸ¥è¯¢ç”¨æˆ·æ“ä½œå†å²"""
        from .models import AuditLog

        logs = self.db_session.query(AuditLog).filter(
            AuditLog.user_id == user_id,
            AuditLog.created_at >= start_time,
            AuditLog.created_at <= end_time
        ).order_by(AuditLog.created_at.desc()).all()

        return [
            {
                "action": log.action,
                "resource": f"{log.resource_type}/{log.resource_id}",
                "timestamp": log.created_at.isoformat(),
                "details": json.loads(log.details)
            }
            for log in logs
        ]

# æ‰©å±•å®¡è®¡æ—¥å¿—è¡¨Schema
"""
-- å¢å¼ºå®¡è®¡æ—¥å¿—è¡¨
CREATE TABLE audit_logs_v2 (
    log_id BIGSERIAL PRIMARY KEY,
    user_id VARCHAR(64) NOT NULL,
    project_id VARCHAR(64),
    action VARCHAR(50) NOT NULL,
    resource_type VARCHAR(50) NOT NULL,
    resource_id VARCHAR(64) NOT NULL,
    details JSONB,
    ip_address INET,  -- è®°å½•IPåœ°å€
    user_agent TEXT,   -- è®°å½•User-Agent
    is_sensitive BOOLEAN DEFAULT FALSE,  -- æ ‡è®°æ•æ„Ÿæ“ä½œ
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_time (user_id, created_at),
    INDEX idx_sensitive (is_sensitive, created_at)
);

-- è‡ªåŠ¨æ¸…ç†ç­–ç•¥ï¼ˆä¿ç•™1å¹´ï¼‰
CREATE OR REPLACE FUNCTION cleanup_old_audit_logs()
RETURNS void AS $$
BEGIN
    DELETE FROM audit_logs_v2
    WHERE created_at < NOW() - INTERVAL '1 year'
    AND is_sensitive = FALSE;  -- æ•æ„Ÿæ—¥å¿—æ°¸ä¹…ä¿ç•™
END;
$$ LANGUAGE plpgsql;
"""
```

---

## ğŸ¯ äº”ã€æ ¸å¿ƒç®—æ³•ä¼˜åŒ–ä¸é…ç½®åŒ–

### 5.1 å¯é…ç½®çš„è®°å¿†æ£€ç´¢ç®—æ³•

```python
# src/mcp_core/memory/retrieval_strategies.py
"""è®°å¿†æ£€ç´¢ç­–ç•¥ï¼ˆå¯é…ç½®ï¼‰"""
from abc import ABC, abstractmethod
from typing import List, Dict
import numpy as np

class RetrievalStrategy(ABC):
    """æ£€ç´¢ç­–ç•¥åŸºç±»"""

    @abstractmethod
    def retrieve(
        self,
        query_embedding: np.ndarray,
        memory_pool: List[Dict],
        top_k: int
    ) -> List[Dict]:
        """æ£€ç´¢è®°å¿†"""
        pass

class HybridRetrievalStrategy(RetrievalStrategy):
    """æ··åˆæ£€ç´¢ç­–ç•¥ï¼ˆè¯­ä¹‰+å…³é”®è¯+æ—¶é—´è¡°å‡ï¼‰"""

    def __init__(self, config: Dict):
        self.semantic_weight = config.get("semantic_weight", 0.6)
        self.keyword_weight = config.get("keyword_weight", 0.3)
        self.time_decay_weight = config.get("time_decay_weight", 0.1)
        self.time_decay_factor = config.get("time_decay_factor", 0.95)  # æ¯å¤©è¡°å‡5%

    def retrieve(self, query_embedding, memory_pool, top_k):
        """æ··åˆæ£€ç´¢"""
        from sklearn.metrics.pairwise import cosine_similarity
        from datetime import datetime
        import re

        query_keywords = set(re.findall(r'\w+', query_text.lower()))
        current_time = datetime.now()

        scored_memories = []

        for memory in memory_pool:
            # 1. è¯­ä¹‰ç›¸ä¼¼åº¦
            mem_embedding = np.array(memory["embedding"])
            semantic_score = cosine_similarity(
                query_embedding.reshape(1, -1),
                mem_embedding.reshape(1, -1)
            )[0][0]

            # 2. å…³é”®è¯åŒ¹é…åº¦
            mem_keywords = set(re.findall(r'\w+', memory["content"].lower()))
            keyword_overlap = len(query_keywords & mem_keywords)
            keyword_score = keyword_overlap / max(len(query_keywords), 1)

            # 3. æ—¶é—´è¡°å‡
            days_old = (current_time - memory["created_at"]).days
            time_score = self.time_decay_factor ** days_old

            # ç»¼åˆè¯„åˆ†
            final_score = (
                self.semantic_weight * semantic_score +
                self.keyword_weight * keyword_score +
                self.time_decay_weight * time_score
            )

            scored_memories.append({
                **memory,
                "final_score": final_score,
                "score_breakdown": {
                    "semantic": semantic_score,
                    "keyword": keyword_score,
                    "time": time_score
                }
            })

        # æ’åºå¹¶è¿”å›Top-K
        scored_memories.sort(key=lambda x: x["final_score"], reverse=True)
        return scored_memories[:top_k]

# é…ç½®æ–‡ä»¶æ‰©å±•
"""
# config.yaml
memory:
  retrieval_strategy: "hybrid"  # hybrid/semantic_only/keyword_only
  hybrid_config:
    semantic_weight: 0.6
    keyword_weight: 0.3
    time_decay_weight: 0.1
    time_decay_factor: 0.95
"""
```

### 5.2 åŠ¨æ€é˜ˆå€¼è°ƒæ•´ç®—æ³•

```python
# src/mcp_core/anti_hallucination/adaptive_threshold.py
"""è‡ªé€‚åº”ç›¸ä¼¼åº¦é˜ˆå€¼"""

class AdaptiveThresholdCalculator:
    """æ ¹æ®æŸ¥è¯¢å¤æ‚åº¦åŠ¨æ€è°ƒæ•´é˜ˆå€¼"""

    def __init__(self, base_threshold: float = 0.65):
        self.base_threshold = base_threshold

    def calculate_threshold(self, query: str, context: Dict) -> float:
        """è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼"""
        adjustments = []

        # 1. æŸ¥è¯¢é•¿åº¦è°ƒæ•´
        if len(query) > 200:
            adjustments.append(-0.05)  # é•¿æŸ¥è¯¢é™ä½é˜ˆå€¼

        # 2. ä»£ç å—æ£€æµ‹
        if "```" in query or "def " in query or "class " in query:
            adjustments.append(-0.08)  # ä»£ç ç›¸å…³é™ä½é˜ˆå€¼

        # 3. æŠ€æœ¯æœ¯è¯­å¯†åº¦
        tech_terms = ["API", "æ•°æ®åº“", "æ¡†æ¶", "æ¥å£", "é…ç½®", "éƒ¨ç½²"]
        term_count = sum(1 for term in tech_terms if term in query)
        if term_count >= 3:
            adjustments.append(-0.05)

        # 4. é¡¹ç›®å†å²è®°å¿†æ•°é‡
        memory_count = context.get("memory_count", 0)
        if memory_count < 10:
            adjustments.append(0.05)  # è®°å¿†å°‘æ—¶æé«˜é˜ˆå€¼ï¼Œé¿å…è¯¯åˆ¤

        # 5. ç”¨æˆ·ç½®ä¿¡åº¦å†å²
        user_hallucination_rate = context.get("user_hallucination_rate", 0)
        if user_hallucination_rate > 0.1:
            adjustments.append(0.10)  # è¯¥ç”¨æˆ·å¹»è§‰ç‡é«˜ï¼Œæé«˜é˜ˆå€¼

        final_threshold = self.base_threshold + sum(adjustments)

        # é™åˆ¶èŒƒå›´[0.4, 0.85]
        return max(0.4, min(0.85, final_threshold))
```

---

## ğŸ“Š å…­ã€ç›‘æ§æŒ‡æ ‡æ‰©å±•

### 6.1 æ–°å¢ä¸šåŠ¡ç›‘æ§æŒ‡æ ‡

```python
# src/mcp_core/monitoring/metrics.py
"""PrometheusæŒ‡æ ‡å®šä¹‰"""
from prometheus_client import Counter, Histogram, Gauge

# è®°å¿†æ“ä½œæŒ‡æ ‡
memory_operations_total = Counter(
    'mcp_memory_operations_total',
    'Total memory operations',
    ['operation', 'memory_level', 'project_id']
)

memory_retrieval_latency = Histogram(
    'mcp_memory_retrieval_latency_seconds',
    'Memory retrieval latency',
    ['project_id'],
    buckets=[0.05, 0.1, 0.2, 0.3, 0.5, 1.0]
)

# Tokenä¼˜åŒ–æŒ‡æ ‡
token_saved_total = Counter(
    'mcp_token_saved_total',
    'Total tokens saved by optimization',
    ['project_id', 'content_type']
)

compression_rate = Histogram(
    'mcp_compression_rate',
    'Content compression rate',
    ['content_type'],
    buckets=[0.5, 0.6, 0.7, 0.8, 0.9, 0.95]
)

# å¹»è§‰æ£€æµ‹æŒ‡æ ‡
hallucination_detected_total = Counter(
    'mcp_hallucination_detected_total',
    'Total hallucinations detected',
    ['project_id', 'threshold_type']
)

hallucination_confidence = Histogram(
    'mcp_hallucination_confidence',
    'Hallucination detection confidence score',
    buckets=[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
)

# èµ„æºä½¿ç”¨æŒ‡æ ‡
vector_db_query_count = Counter(
    'mcp_vector_db_query_count',
    'Vector database query count',
    ['collection']
)

redis_cache_hit_rate = Gauge(
    'mcp_redis_cache_hit_rate',
    'Redis cache hit rate'
)

# ä½¿ç”¨ç¤ºä¾‹
def track_memory_operation(operation: str, memory_level: str, project_id: str):
    memory_operations_total.labels(
        operation=operation,
        memory_level=memory_level,
        project_id=project_id
    ).inc()
```

### 6.2 Grafanaä»ªè¡¨ç›˜é…ç½®

```json
{
  "dashboard": {
    "title": "MCP Core Metrics",
    "panels": [
      {
        "title": "è®°å¿†æ£€ç´¢å»¶è¿Ÿ (P95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(mcp_memory_retrieval_latency_seconds_bucket[5m]))",
            "legendFormat": "{{project_id}}"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {"type": "gt", "params": [0.3]},
              "operator": {"type": "and"},
              "query": {"params": ["A", "5m", "now"]},
              "type": "query"
            }
          ],
          "message": "è®°å¿†æ£€ç´¢P95å»¶è¿Ÿè¶…è¿‡300ms"
        }
      },
      {
        "title": "TokenèŠ‚çœç‡",
        "targets": [
          {
            "expr": "rate(mcp_token_saved_total[5m])",
            "legendFormat": "{{content_type}}"
          }
        ]
      },
      {
        "title": "å¹»è§‰æ£€æµ‹ç‡",
        "targets": [
          {
            "expr": "rate(mcp_hallucination_detected_total[5m]) / rate(mcp_memory_operations_total{operation='retrieve'}[5m])",
            "legendFormat": "å¹»è§‰ç‡"
          }
        ],
        "alert": {
          "conditions": [
            {
              "evaluator": {"type": "gt", "params": [0.05]},
              "message": "å¹»è§‰ç‡è¶…è¿‡5%"
            }
          ]
        }
      }
    ]
  }
}
```

---

## âœ… ä¸ƒã€ä¿®æ”¹éªŒæ”¶æ¸…å•ï¼ˆChecklistï¼‰

### 7.1 ä»£ç ä¿®æ”¹åçš„éªŒè¯æµç¨‹

```markdown
# MCPä»£ç ä¿®æ”¹éªŒæ”¶æ¸…å•

## A. æ ¸å¿ƒæŒ‡æ ‡éªŒè¯ âœ“
- [ ] è¿è¡ŒåŸºå‡†æµ‹è¯•: `pytest tests/benchmark/validate_memory_accuracy.py`
  - [ ] è®°å¿†å‡†ç¡®ç‡ â‰¥ 95%
  - [ ] Tokenä¼˜åŒ–ç‡ â‰¥ 90%
  - [ ] å¹»è§‰æ£€æµ‹å‡†ç¡®ç‡ â‰¥ 95%
- [ ] å¯¹æ¯”ä¿®æ”¹å‰åæŒ‡æ ‡ï¼Œç¡®è®¤æ— é€€åŒ–
- [ ] ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š: `python scripts/generate_comparison_report.py`

## B. ä¾èµ–å…¼å®¹æ€§éªŒè¯ âœ“
- [ ] è¿è¡Œä¾èµ–éªŒè¯: `python scripts/validate_dependencies.py`
- [ ] æ£€æŸ¥sentence-transformersåµŒå…¥ç»´åº¦: `pytest tests/test_embedding_dimension.py`
- [ ] éªŒè¯Pydantic v2å…¼å®¹æ€§: `pytest tests/test_pydantic_models.py`
- [ ] æ£€æŸ¥SQLAlchemy 2.0è¯­æ³•: `ruff check src/`

## C. æ€§èƒ½éªŒè¯ âœ“
- [ ] è¿è¡Œå‹æµ‹: `locust -f tests/performance/advanced_load_test.py --users 100 --spawn-rate 10`
  - [ ] 100 QPSä¸‹P95å»¶è¿Ÿ â‰¤ 500ms
  - [ ] é”™è¯¯ç‡ < 1%
  - [ ] CPUä½¿ç”¨ç‡ < 70%
  - [ ] å†…å­˜ä½¿ç”¨æ— æ³„æ¼
- [ ] è¿è¡Œç“¶é¢ˆè¯Šæ–­: `python scripts/diagnose_performance_bottleneck.py`

## D. å®‰å…¨å®¡è®¡ âœ“
- [ ] æƒé™æµ‹è¯•: `pytest tests/test_permissions.py`
  - [ ] éªŒè¯éæˆæƒç”¨æˆ·æ— æ³•è®¿é—®æ•æ„Ÿæ“ä½œ
  - [ ] æµ‹è¯•æƒé™è¿‡æœŸæœºåˆ¶
- [ ] å®¡è®¡æ—¥å¿—éªŒè¯:
  - [ ] æ•æ„Ÿæ“ä½œå·²è®°å½•
  - [ ] æ—¥å¿—åŒ…å«IPåœ°å€å’ŒUser-Agent
- [ ] SQLæ³¨å…¥æµ‹è¯•: `pytest tests/security/test_sql_injection.py`

## E. è¾¹ç¼˜æ¡ˆä¾‹æµ‹è¯• âœ“
- [ ] å¹»è§‰æ£€æµ‹è¾¹ç¼˜æ¡ˆä¾‹: `pytest tests/benchmark/hallucination_edge_cases.py`
  - [ ] æ¨¡ç³ŠæŸ¥è¯¢å‡†ç¡®ç‡ â‰¥ 90%
  - [ ] è·¨é¢†åŸŸæ··æ·†æ­£ç¡®æ‹’ç»ç‡ 100%
- [ ] å¹¶å‘åœºæ™¯æµ‹è¯•:
  - [ ] åŒä¸€é¡¹ç›®1000å¹¶å‘æ— æ•°æ®ç«äº‰
  - [ ] è·¨é¡¹ç›®éš”ç¦»æ­£å¸¸

## F. éƒ¨ç½²éªŒè¯ âœ“
- [ ] Dockeré•œåƒæ„å»ºæˆåŠŸ: `docker build -t mcp-api:test .`
- [ ] docker-composeå¯åŠ¨æ­£å¸¸: `docker-compose up -d`
- [ ] å¥åº·æ£€æŸ¥é€šè¿‡: `curl http://localhost:8000/health`
- [ ] PrometheusæŒ‡æ ‡é‡‡é›†æ­£å¸¸: `curl http://localhost:8000/metrics`

## G. æ–‡æ¡£æ›´æ–° âœ“
- [ ] æ›´æ–°APIæ–‡æ¡£ï¼ˆå¦‚æœ‰æ¥å£å˜æ›´ï¼‰
- [ ] æ›´æ–°CHANGELOG.md
- [ ] æ›´æ–°ä¾èµ–ç‰ˆæœ¬æ–‡æ¡£
```

---

## ğŸ“ å…«ã€æ€»ç»“ä¸å»ºè®®

### 8.1 å…³é”®æ”¹è¿›ç‚¹

| é—®é¢˜ç±»åˆ« | åŸæœ‰ç¼ºé™· | è¡¥å……æ–¹æ¡ˆ |
|---------|---------|---------|
| **æ ¸å¿ƒæŒ‡æ ‡éªŒè¯** | ä»…ç†è®ºç›®æ ‡ï¼Œæ— éªŒè¯æ–¹æ³• | âœ… æä¾›5ä¸ªçœŸå®åœºæ™¯çš„åŸºå‡†æ•°æ®é›†<br>âœ… è‡ªåŠ¨åŒ–éªŒè¯è„šæœ¬ï¼ˆ100ç»„å¯¹è¯ï¼‰ |
| **ä¾èµ–ç®¡ç†** | ç‰ˆæœ¬é”å®šä½†æ— å…¼å®¹æ€§æ£€æŸ¥ | âœ… ä¾èµ–å…¼å®¹æ€§çŸ©é˜µ<br>âœ… å‘é‡ç»´åº¦è¿ç§»æ–¹æ¡ˆ |
| **æ€§èƒ½æµ‹è¯•** | ä»…æ¦‚å¿µæ€§æŒ‡æ ‡ | âœ… Locustè¾¹ç•Œåœºæ™¯å‹æµ‹<br>âœ… ç“¶é¢ˆè‡ªåŠ¨è¯Šæ–­è„šæœ¬ |
| **å®‰å…¨å®¡è®¡** | åŸºç¡€æƒé™è®¾è®¡ | âœ… ç»†ç²’åº¦æƒé™ï¼ˆ9ç§æƒé™ï¼‰<br>âœ… æ•æ„Ÿæ“ä½œå‘Šè­¦ |
| **ç®—æ³•ä¼˜åŒ–** | ç®€åŒ–å®ç° | âœ… æ··åˆæ£€ç´¢ç­–ç•¥<br>âœ… è‡ªé€‚åº”é˜ˆå€¼ç®—æ³• |

### 8.2 ä½¿ç”¨å»ºè®®

**ä¿®æ”¹ä»£ç å‰**:
```bash
# 1. è¿è¡ŒåŸºå‡†æµ‹è¯•å¹¶ä¿å­˜ç»“æœ
pytest tests/benchmark/ --json-report --json-report-file=baseline.json

# 2. è®°å½•å½“å‰æ€§èƒ½æŒ‡æ ‡
python scripts/save_performance_baseline.py
```

**ä¿®æ”¹ä»£ç å**:
```bash
# 1. é‡æ–°è¿è¡ŒåŸºå‡†æµ‹è¯•
pytest tests/benchmark/ --json-report --json-report-file=current.json

# 2. ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
python scripts/generate_comparison_report.py baseline.json current.json

# 3. å¦‚æœæœ‰ä¾èµ–å‡çº§ï¼Œè¿è¡Œå…¼å®¹æ€§éªŒè¯
python scripts/validate_dependencies.py

# 4. è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶
pytest tests/ --cov --cov-report=html

# 5. æ€§èƒ½å‹æµ‹
locust -f tests/performance/advanced_load_test.py --headless \
       --users 100 --spawn-rate 10 --run-time 5m
```

### 8.3 æŒç»­æ”¹è¿›å»ºè®®

1. **å®šæœŸå›å½’æµ‹è¯•**: æ¯æ¬¡PRåˆå¹¶å‰è¿è¡Œå®Œæ•´éªŒæ”¶æ¸…å•
2. **åŸºå‡†æ•°æ®æ›´æ–°**: æ¯å­£åº¦æ›´æ–°åŸºå‡†æµ‹è¯•æ•°æ®é›†ï¼ˆåŠ å…¥æ–°åœºæ™¯ï¼‰
3. **æ€§èƒ½ç›‘æ§**: ç”Ÿäº§ç¯å¢ƒå¯ç”¨Grafanaå‘Šè­¦ï¼ŒP95å»¶è¿Ÿè¶…æ ‡è‡ªåŠ¨é€šçŸ¥
4. **å®‰å…¨å®¡è®¡**: æ¯æœˆç”Ÿæˆå®¡è®¡æ—¥å¿—æŠ¥å‘Šï¼Œæ£€æŸ¥å¼‚å¸¸æ“ä½œ

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0.0
**é€‚ç”¨äº**: MCPé¡¹ç›®éœ€æ±‚æ–‡æ¡£ xuqiu_enhanced.md
**ç»´æŠ¤è€…**: MCP Validation Team
